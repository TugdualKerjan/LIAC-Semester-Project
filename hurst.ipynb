{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rephrasing Chemistry  \n",
    "\n",
    "### Hurst could allow us to predict downstream performance.  \n",
    "\n",
    "To calculate it: Run an LM through it. Grab the prediction and calculate the amount of bits required to represent this prediction. Use this as your timeseries to calculate the Hurst Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, markdown_folder):\n",
    "        super().__init__()\n",
    "        self.paragraphs = self.load_paragraphs(markdown_folder)\n",
    "\n",
    "    def load_paragraphs(self, folder):\n",
    "        chunks = []\n",
    "        # Loop through each file in the markdown folder\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.mmd'):\n",
    "                file_path = os.path.join(folder, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        text = file.read()\n",
    "                        words = text.split()\n",
    "                        chunks.extend([\" \".join(words[i:i+1000]) for i in range(0, len(words), 1000)])\n",
    "                except IOError as e:\n",
    "                    print(f\"Failed to read {file_path}: {e}\")\n",
    "        return chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paragraphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.paragraphs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class InputDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.input_texts = self.load_paragraphs(filename)\n",
    "\n",
    "    def load_paragraphs(self, filename):\n",
    "        input_texts = []\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                for item in data['results']:\n",
    "                    input_texts.append(item['input_text'])  # Removed extra parentheses\n",
    "        except IOError as e:\n",
    "            print(f\"Failed to read {filename}: {e}\")\n",
    "        return input_texts\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each input_text has a corresponding output_text\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class OutputDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.output_texts = self.load_paragraphs(filename)\n",
    "\n",
    "    def load_paragraphs(self, filename):\n",
    "        output_texts = []\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                for item in data['results']:\n",
    "                    output_texts.append(item['output_text'])  # Removed extra parentheses\n",
    "        except IOError as e:\n",
    "            print(f\"Failed to read {filename}: {e}\")\n",
    "        return output_texts\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each input_text has a corresponding output_text\n",
    "        return len(self.output_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.output_texts[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pprint as pp\n",
    "\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "models_path = {\"qwen\": \"Qwen/Qwen1.5-7B-Chat\", \"mistral-inst\": \"mistralai/Mistral-7B-Instruct-v0.1\", \"zephyr\": \"HuggingFaceH4/zephyr-7b-alpha\"}\n",
    "MODELPATH = models_path[\"qwen\"]\n",
    "\n",
    "batch_size = 4\n",
    "dataset_input = InputDataset(\"./wikipedia/10-Papers-Mistral-7B-Instruct-v0.1.json\")\n",
    "dataset_output = OutputDataset(\"./wikipedia/10-Papers-Mistral-7B-Instruct-v0.1.json\")\n",
    "dataloader_out = DataLoader(dataset_output, batch_size=batch_size, shuffle=False)\n",
    "dataloader_in = DataLoader(dataset_input, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODELPATH)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELPATH)\n",
    "config = AutoConfig.from_pretrained(MODELPATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "results_in = []\n",
    "results_out = []\n",
    "\n",
    "def calculate_probability_and_perplexity(input_ids, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "        # print(tokenizer.decode(shift_labels[0][0]))\n",
    "        # print(tokenizer.decode(shift_labels[0][1]))\n",
    "        # print(tokenizer.decode(shift_labels[0][2]))\n",
    "\n",
    "        # print(shift_labels)   \n",
    "\n",
    "\n",
    "        # Assuming shift_logits is your input tensor of logits\n",
    "        log_probabilities_base_e = F.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "        # Convert to base 2\n",
    "        probabilities = log_probabilities_base_e / torch.log(torch.tensor(2.0)) * -1\n",
    "        print(probabilities)\n",
    "\n",
    "        # print(probabilities.shape)\n",
    "        # print(shift_labels.shape)\n",
    "        \n",
    "        # Gather the probabilities of the actual next tokens\n",
    "        actual_next_token_probs = torch.gather(probabilities, 2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        # pp.pprint(zip(shift_labels[0], probabilities))\n",
    "        \n",
    "        # Compute average negative log likelihood for perplexity\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        \n",
    "        return actual_next_token_probs, perplexity\n",
    "\n",
    "results_probs_next_token_original = []\n",
    "\n",
    "for batch in tqdm(dataloader_in):\n",
    "    for chunk in batch:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_position_embeddings)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "\n",
    "        actual_next_token_probs, perplexity = calculate_probability_and_perplexity(input_ids, model)\n",
    "\n",
    "        # actual_next_token_probs = \n",
    "\n",
    "        results_in.append({\"input_text\": chunk, \"actual_next_token_probs\": actual_next_token_probs, \"perplexity\": perplexity})\n",
    "\n",
    "for batch in tqdm(dataloader_out):\n",
    "    for chunk in batch:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_position_embeddings)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "\n",
    "        actual_next_token_probs, perplexity = calculate_probability_and_perplexity(input_ids, model)\n",
    "\n",
    "        results_out.append({\"input_text\": chunk, \"actual_next_token_probs\": actual_next_token_probs, \"perplexity\": perplexity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hurst import compute_Hc\n",
    "\n",
    "print(results_in)\n",
    "\n",
    "results_in_filtered = []\n",
    "results_out_filtered = []\n",
    "\n",
    "for i in range(len(results_in)):\n",
    "    if len(results_in[i][\"actual_next_token_probs\"][0]) > 100 and len(results_out[i][\"actual_next_token_probs\"][0]) > 100:\n",
    "        results_in_filtered.append(results_in[i])\n",
    "        results_out_filtered.append(results_out[i])\n",
    "\n",
    "# Assuming results_in and results_out are your datasets\n",
    "hurst_in = []\n",
    "hurst_out = []\n",
    "\n",
    "for i in range(len(results_in_filtered)):\n",
    "    # Calculate Hurst exponent and append it to the lists\n",
    "    H_in, _, _ = compute_Hc(results_in_filtered[i][\"actual_next_token_probs\"][0].cpu(), kind='change', simplified=True)\n",
    "    hurst_in.append(H_in)\n",
    "    H_out, _, _ = compute_Hc(results_out_filtered[i][\"actual_next_token_probs\"][0].cpu(), kind='change', simplified=True)\n",
    "    hurst_out.append(H_out)\n",
    "    # Calculate and append the ratio of Hurst exponents\n",
    "\n",
    "filtered_hurst_in=[]\n",
    "filtered_hurst_out= []\n",
    "\n",
    "hurst_ratio = []\n",
    "hurst_diff = []\n",
    "\n",
    "for i in range(0, len(hurst_in)):\n",
    "    if hurst_in[i] > 0.5 and hurst_out[i] > 0.5:\n",
    "        H_in = hurst_in[i]\n",
    "        H_out = hurst_out[i]\n",
    "\n",
    "        filtered_hurst_in.append(H_in)\n",
    "        filtered_hurst_out.append(H_out)\n",
    "        hurst_ratio.append(H_in / H_out)\n",
    "        hurst_diff.append(H_out - H_in)\n",
    "\n",
    "hurst_in = filtered_hurst_in\n",
    "hurst_out = filtered_hurst_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot for Hurst exponents\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(hurst_in, label='Input', color=\"purple\")\n",
    "ax.plot(hurst_out, label='Output', color=\"deepskyblue\")\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Hurst Exponent')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Hurst ratio\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(hurst_ratio, label='Hurst Ratio (Input/Output)', color=\"orange\")\n",
    "ax.set_xlabel('Sample Index')\n",
    "# ax.set_ylim([0, 2])\n",
    "ax.set_ylabel('Hurst Ratio')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot for Hurst ratio\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(hurst_ratio, label='Hurst Diff (Output - Input)', color=\"orange\")\n",
    "ax.set_xlabel('Sample Index')\n",
    "# ax.set_ylim([0, 2])\n",
    "ax.set_ylabel('Hurst Diff')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomprehensible_text_id = np.argmin(hurst_in)\n",
    "print(incomprehensible_text_id)\n",
    "print(results_in_filtered[incomprehensible_text_id][\"input_text\"])\n",
    "print(hurst_in[incomprehensible_text_id])\n",
    "print(results_out_filtered[incomprehensible_text_id][\"input_text\"])\n",
    "print(hurst_out[incomprehensible_text_id])\n",
    "\n",
    "bestter_text = np.argmin(hurst_ratio)\n",
    "print(results_in_filtered[bestter_text][\"input_text\"])\n",
    "print(hurst_in[bestter_text])\n",
    "print(results_out_filtered[bestter_text][\"input_text\"])\n",
    "print(hurst_out[bestter_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moderna1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
