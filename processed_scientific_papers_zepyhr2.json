[
    {
        "article": "The International Astronomical Union (IAU) recently reclassified the celestial object formerly known as Pluto as a \"dwarf planet,\" in a historic move that reflects broader conceptions of planetary status. Originally classified in 1930 as the ninth planet in our solar system, Pluto's distance from the sun and relatively small size have led some scientists to question its planetary status for decades. Following a 2006 definition of what constitutes a planet, a committee within the IAU undertook a thorough review of Pluto's characteristics and determined that it would no longer be classified as a planet. Instead, it joins a growing number of celestial bodies, including Ceres, Eris, and Haumea, that fit somewhere between the traditional categories of planet, moon, asteroid, and comet. While the new classification has generated some controversy, it reflects a growing scientific understanding of the vast and complex universe in which we live. The system under examination consists of a nematic liquid crystal confined between two parallel walls providing homogenous planar anchoring, but with perpendicular easy directions. The orientation of the nematic director is manipulated through the application of external electric or magnetic fields. The precise control of surface alignment over sizable regions is essential for the device's operation. While studies primarily examine liquid crystals in contact with uniform substrates, surface treatments like rubbing naturally generate substrate inhomogeneities. In many studies, nematic liquid crystal systems have been investigated in the presence of laterally uniform substrates. However, a more natural occurrence is the creation of substrate inhomogeneities through surface treatments, such as rubbing. This results in a non-uniform nematic texture near the surface, which, however, is smeared out beyond a decay length proportional to the periodicity of the surface pattern. In many cases, the thickness of the non-uniform surface layer is significantly smaller than both the wavelength of visible light and the thickness of the nematic cell, resulting in a system where the distance between the two confining parallel walls is insignificant compared to the light-absorbing properties of the liquid crystal. Despite an apparent lack of uniformity in certain liquid crystal systems, this non-uniformity tends to be masked by a decay length that is proportional to the pattern's periodicity. This is significant as the thickness of the non-uniform surface layer is often smaller than the wavelength of visible light and the thickness of the nematic cell. Notably, recent advancements have revealed that surfaces exhibiting large periodicities, such as those on a micrometer scale, hold significant technological value (as evidenced in Ref. @xcite and references therein).\n\nIn essence, the seemingly diverse properties of confined liquid crystals are not fundamentally altered by this patterned surface, as they ultimately behave in accordance with their interaction with effective, uniform substrates. This has implications for the technological applications of patterned surfaces, particularly those with periodicities on a micrometer scale. Recent advancements in technology have shown that surfaces patterned with a large periodicity of several micrometers have significant potential (see, for instance, ref. @xcite and references therein). A new generation of electro-optical devices relies on nematic liquid crystals with distinct orientation of the director over small areas, which can be achieved by chemically patterned surfaces. These devices, used in applications like flat-panel displays, feature sub-pixels that each contain a different orientation of the nematic director, induced by the surface structure and subsequently tuned by the electric field. Besides their technological relevance, nematic liquid crystals in contact with non-uniform substrates offer an opportunity to study fundamental phenomena, such as effective elastic forces between the substrates and phase transitions between various competing nematic textures (see, for instance, ref.). In order to manufacture flat-panel displays with wide viewing angles, one can utilize sub-pixels that are divided into distinct orientations of the nematic director. The director is induced by the surface structure and subsequently tuned by an electric field. This method not only has technological significance but also provides a chance to investigate basic phenomena, such as effective elastic forces between the substrates and the competition between various competing nematic textures. While the influence of homogenous confining substrates on nematic liquid crystals is well-understood, the phase behavior of nematic liquid crystals in contact with chemically or geometrically patterned substrates remains debated. Theoretical calculations based on continuum theories may resolve the properties of nematic liquid crystals on patterned substrates. References: [@xcite and references therein] Although theoretical calculations based on continuum theories have been attempted to understand the phase behavior of nematic liquid crystals in contact with patterned substrates, they remain a challenge. Such calculations require two- or three-dimensional grids due to the broken symmetry caused by the substrate pattern, making them numerically demanding. Moreover, determining metastable states and energy barriers becomes extremely challenging for understanding bistable nematic devices. In contrast, the present paper adopts a unique strategy by taking advantage of the finite decay length of the surface pattern's influence on the nematic liquid crystal in the direction perpendicular to the substrate. Furthermore, identifying metastable states and energy barriers between them can be a daunting task in the context of bistable nematic devices. This challenge is particularly pronounced in determining the metastable configurations. However, in this paper, an alternative approach is presented, which takes advantage of the finite decay length in the direction perpendicular to the substrate. By utilizing finite anchoring energy and average surface director orientation functions of patterned substrate, an effective free energy for the nematic liquid crystal cell is obtained. Notably, there is remarkable agreement between the phase diagrams calculated using the effective free energy functional and the original free energy functional. This development is an important breakthrough as it has its origins dating back to the work of Oseen and Zocher, pioneers in the field of liquid crystal theory. A remarkable level of congruence can be observed between the phase diagrams resulting from the implementation of the provided effective free energy function and those obtained using the original free energy functional. This continuum theory for liquid crystals has its origins in the work of Oseen and Zocher dating back to the early stages of the field's development. The Oseen-Zocher theory played a significant role in the evolution and refinement of the static theory and its subsequent formulation by Frank. Frank's theory is based on the nematic director, which describes the orientational behavior of the liquid crystal's molecules at the local level. The nematic director represents the direction of the preferred molecular alignment, and its potential distortions provide insights into the phase behavior of the material. The Frank theory is expressed in terms of the nematic director, specifically in its spatial distortions. The nematic director denotes the locally averaged molecular alignment in liquid crystals, where the centers of mass have no long-range order. Despite the local alignment, molecules are still free to move and can translate without constraints, with their directions and the nematic director being parallel. In uniform nematic liquid crystals, if the system is subjected to external perturbations, it relaxes back to its original state once the disturbing force is removed, indicating the uniform configuration represents a thermodynamically stable state. In a nematic liquid crystal system, the molecules can translate freely while being parallel to each other and the director, on average. If a uniform nematic liquid crystal is distorted by external forces and then the disturbing influence is removed, it relaxes back to a uniform state, signaling that it corresponds to a thermodynamically stable state. The deformation of the nematic director is associated with a free energy cost in the form of:\n\n\\begin{aligned}\n  \\mathcal{F} = \\frac{1}{2}\\int_v d^3 r\\,\\left[k_{11}\\left(\\nabla\\cdot\\hat{{\\bf n}}\\right)^2\\right.\\nonumber \\\\\n  + \\left.\\!\\!\\!k_{22}(\\hat{{\\bf n}}\\cdot(\\nabla \\times\\hat{{\\bf n}}))^2 + k_{33}(\\hat{{\\bf n}}\\times(\\nabla \\times \\hat{{\\bf n}}))^2\\right]\\!,\n  \\label{eq1}\n\\end{aligned}\n\nwhere the volume, @xmath3, is accessible to the nematic liquid crystal, and the elastic constants, @xmath4 , @xmath5 , and @xmath6, are associated with splay, twist, and bend distortions, respectively. These elastic constants depend on temperature, and they are commonly on the order of @xmath7 to @x math7. Sometimes, such as when the relative values of the elastic constants are unknown or the resulting Euler-Lagrange equations are complicated, the one-constant approximation, @xmath8, is made. In the realm of nematic liquid crystal, Equation (1) depicts the volume accessible to the medium, denoted as @xmath3, and is associated with the elastic constants @xmath5, @xmath6, and @xmath4, corresponding to splay, twist, and bend distortions, respectively. These elastic constants are temperature-dependent and generally range from @xmath7 to @xmath8. In situations where the values of elastic constants are unknown or the resulting Euler-Lagrange equations are complicated, the one-constant approximation @xmath8 is implemented. This reduces the elastic free energy functional to Equation (9), where the integral is calculated over the volume. However, in the presence of surfaces, the bulk free energy @xmath10 must be supplemented with the surface free energy @xmath11, with the total free energy being given by Equation (12). This leads to appropriate boundary conditions in the corresponding equilibrium Euler-Lagrange equations @xmath13, with anchoring being a crucial aspect for the description of nematic director alignment at surfaces. There are three types of anchoring: free, weak, and strong. When there is no anchoring condition imposed on the director @xmath14 at the boundary, the bulk free energy @xmath15 is minimized using standard techniques of calculus of variations. In strong anchoring conditions, the director orientations are fixed to particular values by the surface anchoring forces. The description of the nematic director alignment at the surfaces forming the boundaries of a material is known as anchoring. There exists a weaker form called weak anchoring, which requires minimization of the total free energy consisting of the bulk free energy and surface free energy. On the other hand, strong anchoring results in prescribed values for the director field at the boundary by minimizing the bulk free energy while satisfying specific anchoring conditions. The Rapini-Papoular model commonly used to calculate surface free energy is denoted by:\n\n@xmath17=\\frac{1}{2}\\int_{s } d^2 r\\,w({\\bf r } ) ( \\hat{{\\bf n}}\\cdot{\\nu})^2.\n\nIn this formula, \ud835\udee5 is the corresponding anchoring strength, \ufffd\ufffd\ufffd is the normal unit vector, and \ufffd\ufffd\ufffd\u03bd is the unit vector normal to the boundary. The integral runs over the boundary of the material. To minimize the total free energy, which comprises the surface free energy, the expression proposed by Rapini and Papoular is commonly used, which is given by @xmath17=\\frac{1}{2}\\int_{s } d^2 r\\,w(\\mathbf{r }) ( \\hat{\\mathbf{n}}\\cdot\\mathbf{\\nu})^2. This integral is taken over the boundary, and @xmath18 denotes the corresponding anchoring strength that characterizes the surface. The local unit vector perpendicular to the surface is denoted as @xmath14. For negative @xmath19, this contribution favors an orientation of the molecules perpendicular to the surface, while positive @xmath19 favor degenerate planar orientations at the surface. The absolute value of the anchoring strength is typically of the order @xmath20 to . In this contribution, the molecules align perpendicularly to the surface, while positive @xmath19 favor degenerate planar orientations at the surface. The absolute value of the anchoring strength is typically of the order @xmath20 to . A nematic liquid crystal is confined between a patterned substrate, characterized by geometrical and/or chemical patterns of periodicity @xmath24 along the @xmath25 axis, and a flat substrate at @xmath22 where the @xmath23 axis is normal to the flat substrate. The system is translationally invariant in the @xmath26 direction. Within the one-constant approximation, the total free energy functional is:\n\n\\begin{aligned}\nF =& \\frac{kl}{2} \\int\\limits_0^p dx \\int\\limits_{z_0(x)}^d dz (\\nabla\\theta(x,z))^2 + f_s[\\theta(x,z_0(x))]\\ ,\\label{eq4}\n\\end{aligned}\n\nwhere $p$ is the extent of the system in the @xmath26 direction, $z_0(x)$ is the surface profile of the patterned substrate, and $d$ is the total system depth. At the upper surface, strong anchoring @xmath31 is imposed. [ Figure 1 ] displays patterns of periodicity in the x-axis, which are either geometrical or chemical, for the lower substrate. Translation invariance exists in the y-direction. Using the one-constant approximation [Equation ([Eq. 2])], the total free energy functional is given by:\n\n\\begin{aligned}\nF_{total} & = \\frac{kl}{2}\\int\\limits_0^p \\text{d}x \\int\\limits_{z_0(x)}^d \\text{d}z \\bigg\\{\\frac{1}{2} \\left[ \\nabla \\theta(x,z) \\right]^2 + f_s \\left[ \\theta(x,z_0(x)) \\right] \\bigg\\}, \\label{eq4}\n\\end{aligned}\n\nwith k being the Frank elastic constant, l being the system's length in the y-direction, and p representing the system's length in the x-direction. z0(x) is the surface profile of the patterned substrate, d is the thickness of the substrate, and f_s is the anchoring energy. Strong anchoring is imposed at the upper surface, splay is assumed to be absent (i.e., twist is not considered by setting b=0), and the analysis can be extended to consider different splay and bend constants a and c, respectively, if the results are to be quantitatively specific to a particular nematic liquid crystal. The surface term $f_s(x)$ employs the Rapini and Papoular form for the anchoring energy, as represented in Equation ([Eq. 3]) below:\n\n\\begin{aligned}\nf_s = \\frac{1}{2} W_s \\left[ \\cos^2 \\left( \\alpha_s \\cdot \\theta \\right) + q_s \\sin^2 \\left( \\alpha_s \\cdot \\theta \\right) \\right],\n\\end{aligned}\n\nwhere W_s is the surface energy constant, $\\alpha_s$ represents the alignment angle for the substrate, and q_s is a factor measuring the strength of the asymmetric anchoring. The analysis can also be extended to account for different splay and bend constants at a given location, specifically denoted by @xmath4 and @xmath6, respectively, although this aspect is crucial only when quantitative results are desired for a particular nematic liquid crystal. The surface contribution, expressed as @xmath33, incorporates the anchoring energy, which is given by the Rapini-Papoular formulation [eq.(3)]. The upper flat substrate induces strong anchoring, i.e., @xmath31, while the lower substrate presents a periodic grating structure with a period of @xmath24 in @xmath25 direction. The system exhibits translational invariance in @xmath26 direction, which is perpendicular to the plane of the figure. In case (a), the lower sinusoidally grating surface, with a groove depth of @xmath34, is endowed with an alternating stripe pattern, where locally homeotropic anchoring (white bars) and homogeneous planar anchoring (black bars) exist. In both (a) and (d), the system possesses translational invariance in the direction perpendicular to the figure, referred to as the @xmath26 direction, which is perpendicular to the plane of the figures. The lower substrate in (a) features a sinusoidally grated surface with groove depth @xmath34, and an alternating stripe pattern of locally homeotropic anchoring (white bars) and homogeneous planar anchoring (black bars) that has a period of half the period of the surface grating at @xmath24. Meanwhile, in (c) and (d), both the lower substrate and the chemical pattern are presented as pure geometric or chemically patterned configurations, respectively, where the anchoring direction is represented by black rods at the lower substrates. (d) depicts two distinct substrate arrangements: pure geometrically structured lower substrate and pure chemically patterned lower substrate. The anchoring direction at these substrates is illustrated by black rods. To predict the reliable phase behavior of a nematic liquid crystal confined between these two substrates at an arbitrary mean distance, the effective free energy function [equations ([eq6])-([eq8])] in the main text must be analyzed. To do so, the free energy functional of a nematic liquid crystal confined between two substrates at a single and relatively small mean distance has to be minimized, which is discussed in the text. To achieve this goal, we integrate from position zero (p) to position l to obtain:\n\n@xmath39 \n\nwhere @xmath38.   \n\n<|user|>\nCan you provide a visual representation of what the pure geometrically structured and pure chemically patterned substrates might look like? It would be helpful to have a better understanding of the physical differences between the two. The analysis of the difference between equations ([ eq6 ] ) and ([ eq8 ] ) is performed by minimizing the free energy functional of the nematic liquid crystal confined between two substrates at a small mean distance, as discussed in the main text. This involves calculating the integral from @xmath37=}\\nonumber \\\\&, \\frac{l}{2}\\int^p_0 dx, w(x) \\frac{[-\\sin(\\theta_0(x))z'_0(x) + \\cos(\\theta_0(x))]^2}{\\sqrt{1 + (z'_0(x))^2}}.\\] \n\nEquations ([ eq4 ] ) and ([ eq5 ] ) combined with the boundary condition for the nematic director at the upper surface provide the free energy functional for the system under consideration, and the Euler-Lagrange equations obtained from its stationary conditions can be solved numerically using an iterative method on a fine two-dimensional grid. However, due to the pattern on the lower surface, determining the director field and obtaining stable and metastable configurations can be a challenging numerical problem, particularly for large cell widths. The Euler-Langrange equations arising from the stationary conditions of the total free energy, derived by varying the nematic director field, can be numerically solved using an iterative method on a fine two-dimensional grid. However, obtaining both stable and metastable configurations can be a challenging numerical problem, particularly for larger cell widths. The pattern of the lower surface plays a crucial role in determining the director field and phase diagram. The free energy functional of an arbitrary nematic liquid crystal cell is:\n\nF = \u222bdxdy [ f_c(x,y) + f_el(x,y) ]           (eq. 1)\nwhere f_c(x,y) is a confinement potential and f_el(x,y) is the elastic energy functional. Determining the energy barrier between two metastable states requires additional techniques to be implemented beyond this approach. In order to calculate the average surface director orientation and effective surface free energy function at a patterned surface within a nematic liquid crystal cell with arbitrary width and anchoring angle at the upper surface, we map the free energy functional at this surface onto an effective free energy function using the following method. First, we determine numerically the minimum of the free energy for a single, relatively small value of the anchoring angle at the upper surface. The mapping allows us to explicitly calculate both the average surface director orientation and the effective surface free energy function characterizing the anchoring energy at the patterned surface. The free energy functional is written as\n\n@xmath39 $ ] [ eq . ( [ eq4 ] ) ] $\n\nwhere [xmath35] denotes the arbitrary width of the cell and [xmath40] refers to the anchoring angle at the upper surface (see Fig. [xref fig1] [a])). We then obtain the effective free energy function by writing\n\n@xmath41\n\nwhere @xmath42 represents the average surface director orientation, which is given by\n\n@xmath43\n\nFinally, the effective surface free energy function is characterized and written as\n\n@xmath44\n\nIn order to calculate these quantities, we numerically determine the minimum of the free energy for a single, relatively small value of the anchoring angle at the upper surface, as shown in Fig. [xref fig1] [b]. [Fig 1] illustrates the nematic liquid crystal cell where the average surface director orientation at the lower patterned surface is represented by @xmath43. The effective surface free energy function characterizing the anchoring energy at the patterned surface can be written as @xmath44 for calculating @xmath42 and @xmath44 explicitly. To do so, we first determine numerically the minimum of the free energy @xmath46 for a single and rather small value @xmath47 at the upper surface for an arbitrary anchoring angle @xmath48. This allows us to obtain the phase behavior, energy barriers between metastable states, and effective anchoring angles for arbitrary values of @xmath35 and @xmath40 from @xmath49 as a function of the single variable @xmath42. The calculation is less challenging than minimizing the original free energy functional @xmath39 on a two-dimensional @xmath51 grid.\n\n[Fig 1] (a)\n\n[Fig 1] (b) [Figure 1] depicts the (a) portion of an arbitrary value of variables x35 and x40 that can be computed through equation 49 as a function of the sole variable x42. Calculating this is substantially less challenging than minimizing the initial free energy functional, equation 39, with respect to variable x50 on a two-dimensional grid. However, the effective free energy method can only be applied when equation ([Eq7]) can be inverted to obtain x52, which is necessary as input for equation ([Eq8]). To obtain the necessary input parameter @xmath52 for Eq. ( [ eq8 ] ), the inversion of Eq. ( [ eq7 ] ) can be performed. This leads to Eq. ( [ eq53 ] ), which expresses the necessary condition for the inversion. Additionally, it has been determined that @xmath54 is practically independent of the cell width @xmath36 when the aforementioned condition (Eq. ( [ eq55 ] )) is satisfied, indicating that the interfacial region above the lower substrate does not extend to the upper one. \n\nBefore exploring the nematic liquid crystal in contact with the patterned substrates depicted in Fig. [ fig1 ] (to be discussed), we first examine the nematic liquid crystal confined between two homogeneous, flat substrates at a distance of approximately @xmath35. ( [ eq7 ] ) further highlights a significant aspect, that the cell width @xmath36 has minimal influence on the variable @xmath54, which suggests that the interfacial area located above the lower substrate does not connect to the upper substrate. Prior to examining the nematic liquid crystal in contact with the substrates shown in Fig. [ fig1 ], it is necessary to scrutinize the liquid crystal confined between two flat, homogenous substrates separated by a distance @xmath35. The upper surface induces substantial anchoring, meaning that the free energy functional, defined in Eq. ( [ eq4 ] ), follows as:\n\n@xmath56 = \\dfrac{klp}{2}\\int\\limits_0^d dz ,\\left(\\dfrac{d\\theta(z)}{dz}\\right)^2 + f_s(\\theta_0)\n\nThe solution obtained via the Euler-Lagrange equation, subject to the boundary condition at the upper surface (Eq. [ eq5 ]), interpolates linearly between the top and bottom surfaces:\n\n@xmath59 = \\theta_0(1-\\dfrac{z}{d}) + \\theta_s(\\dfrac{z}{d})\n\nwhere Eq. ( [ eq6 ] ) represents the solution at the bottom surface. Using the given equations, the free energy functional defined in Eq. (4) is given as:\n\n[Eq. 56]\n\n$$\n\\frac{klp}{2}\\int\\limits_0^d dz\\ \\left(\\frac{d \\theta(z)}{d z}\\right)^2+f_s(\\theta_0)\n$$\n\nThe solution to the Euler-Lagrange equation, subject to the boundary condition at the upper surface in Eq. (58), is linear and interpolates between the top and bottom surfaces:\n\n@xmath59\n\n$$\n\\frac{d \\theta(z)}{d z} = (\\theta(0) - \\theta(d)) \\frac{z}{d} + \\theta(0)\n$$\n\nSubstituting this solution into the original free energy functional of Eq. (4) and using the specified surface conditions in Eqs. (58) and (60), the minimized free energy function can be obtained as shown in Eq. (61):\n\n[Eq. 61]\n\n$$\n\\int\\limits_0^d dz\\ \\frac{1}{2}klp \\left( (\\theta(0) - \\theta(d)) \\frac{z}{d} + \\theta(0) \\right)^2 + f_s(\\theta_0)\n$$\n\nSubstituting this expression into Eq. (62), Eq. (63), and Eq. (64), it can be shown that the following simplifications hold for a homogeneous substrate:\n\n[Eq. 62]\n\n$$\n\\frac{d \\theta(z)}{d z} = 0\n$$\n\n[Eq. 63]\n\n$$\n\\theta(d) = 0\n$$\n\n[Eq. 64]\n\n$$\n\\theta(0) = 0\n$$\n\nIn this case, the effective free energy function agrees exactly with the minimized free energy function of the original system. [Eq. 13] In a scenario where nematic liquid crystals are confined between homogeneous substrates, we can accurately express their effective free energy function, as explained in a previous section, through the equation [ eq . ( [ eq6 ] ) ]. In this specific subsection, we choose to apply this approach to the case of a liquid crystal embedded within a sinusoidal substrate and a flat substrate with strong homeotropic anchoring, as shown in Fig.. \n\nTo describe this situation, we can adopt the effective free energy function [ eq . ( [ eq6 ] ) ] that was introduced in the preceding section. Its agreement with the minimized free energy function of the original system established in equation [ eq . ( [ eq12 ] ) ] is exact in the scenario of homogeneous confining substrates. By utilizing this method, we can examine and model the behaviour of nematic liquid crystals in varying confined geometries. In the previous section, we demonstrated that we can represent nematic liquid crystals trapped between two homogenous substrates with an effective free energy function as seen in [Eq. ([Eq. 6])]. In this subsection, we apply this theory to elucidate the case of a nematic liquid crystal adhering to a chemically patterned sinusoidal surface with a strong homeotropic anchoring, juxtaposed with a flat substrate as shown in Figure [Fig. 1(a)]. The sinusoidal profile of the grating surface has a period of @xmath66 and a groove depth of @xmath34, as illustrated in the depiction. The surface anchors are locally homeotropic and homogeneously planar, as illustrated in the diagram. The formula for the groove depth, denoted by @xmath34, and the period, denoted by @xmath66, together create a pattern of stripes with alternating homeotropic and homogeneous planar anchoring, as depicted in Figure [ fig1 ] (a). The widths of the stripes projected onto the @xmath25 axis are denoted by @xmath67, and the anchoring strength is represented by a periodic step function: @xmath68 for the homeotropic stripes and @xmath69 for the planar stripes. Figure [ fig2 ] (a) displays @xmath70 (dashed line) and @xmath71 (solid line) for @xmath72, @xmath73, and @xmath74. The shapes of @xmath70 and @xmath71 as functions of @xmath48 and @xmath42, respectively, are rather similar due to the value of @xmath75 (see eq. [ eq8 ]) for these set of model parameters. As shown in Figure [ fig2 ] (a), both curves @xmath70 (dashed line) and @xmath71 (solid line) exhibit similar shapes for variables @xmath72, @xmath73, and @xmath74, due to the similarity of parameter @xmath75, as dictated by equation ([ eq8 ]) for this specific set of model parameters. Figure [ fig2 ] (b) displays the phase diagram, illustrating the existence of two distinct nematic director configurations. The first configuration is the homeotropic (h) phase, where the director field is almost uniform and parallel to the surface anchoring direction imposed at the upper substrate, as stated by @xmath76. The second configuration, known as the hybrid aligned nematic (HAN) phase, involves a variation in director field orientation from @xmath77 at the upper surface to a near-planar orientation within the cell. Of note, there are two distinct HAN textures: HAN@xmath78 and HAN@xmath79, associated with positive and negative average surface angles at the lower surface, respectively. The calculations show the existence of two stable or metastable nematic director configurations: the homeotropic (H) phase and the hybrid aligned nematic (HAN) phase. In the H phase, the director field is almost uniform and parallel to the anchoring direction at the upper surface, represented by the mathematical expression @xmath76. On the other hand, in the HAN phase, the director field varies from its planar orientation at the upper surface, as represented by the mathematical expression @xmath77, to nearly planar orientation through the cell. There are two HAN textures: HAN@xmath78 and HAN@xmath79 corresponding to positive and negative average surface angles at the lower surface, respectively (for a detailed diagram, see Fig. [Fig] below). For small anchoring angles @xmath40, the HAN phases are stable for cell widths larger than @xmath80 (more precisely, the HAN@xmath78 texture is stable for @xmath81, while the HAN@xmath79 texture is stable for @xmath82, and they coexist at @xmath83). For smaller distances between the substrates @xmath84, the HAN phases are no longer stable due to the high cost of distortions in the presence of the dominating strong anchoring at the upper surface. The comparison of the phase boundary of thermal equilibrium, obtained from the effective free energy method [eqs. For cell widths exceeding @xmath80, stable han@xmath78 or han@xmath79 phases are observed depending on the distance between substrates, with the han@xmath78 texture being stable for @xmath81 and the han@xmath79 texture stable for @xmath82. However, for smaller distances, these configurations are no longer stable as significant distortions of the director field become costly due to strong anchoring at the upper surface. The reliability of the effective free energy method was demonstrated through comparison with the results obtained from direct minimization of the underlying free energy functional for determining the phase boundary of thermal equilibrium. This can be seen in Figure 2(b), where the solid line represents the effective free energy method and the diamonds represent direct minimization. This highlights the validity and usefulness of the effective free energy method. The implementation of conditions [eq4] and [eq5], as illustrated in Fig. [fig2][(b)], provides evidence for the trustworthiness of the effective free energy technique (sometimes called relaxation theory) in this context (see Fig. [fig1][(b)], in which the cell's size along the invariant direction @xmath26 is denoted by @xmath85, and the homeotropic and planar anchoring strengths are indicated by the white and black bars, respectively). \n\nNote that here, \"@xmath85\" refers to the isotropic elastic constant, and \"@xmath24\" and \"@xmath26\" are specific spatial directions in the system. In this context, the extension along the invariant direction, designated as @xmath26, is referred to as @xmath28. Furthermore, the isotropic elastic constant is represented as @xmath85. The anchoring strength varies on the homeotropic stripes, represented by @xmath72, and planar anchoring stripes, denoted as @xmath73. Figure [fig1] (b) visualizes the phase diagram as a function of the anchoring angle at the upper flat substrate, labeled as @xmath40, and cell width, designated as @xmath35. A solid line indicates in-depth first-order phase transitions between a homeotropic (h) state and hybrid aligned nematic (han@xmath78 and han@xmath79) phases. At @xmath86 and @xmath87, the han@xmath78, han@xmath79, and h phases coexist in a triple point. In Figure [fig1], Panel A depicts the phase diagram of a given system as a function of anchoring angle at the upper flat substrate at xm40 and cell width at xm35. The solid line indicates first-order phase transitions between the homeotropic (h) and hybrid-aligned nematic (HAN@xm78 and HAN@xm79) phases. At angles xm86 and xm87, there is a triple point where HAN@xm78, HAN@xm79, and h states coexist. A critical point exists at angles xm88 and xm89, represented by a solid circle. The dot-dashed lines denote the metastability limits of HAN@xm78 (1) and the h (2) state. For clarity, the limit of metastability for h at xm82 and HAN@xm79 is not shown in the figure. The horizontal dashed lines in this graph denote the limits of metastability for the han@xmath78 (1) and h (2) state, as marked by the dots. The limit for the h state for @xmath82 and the han@xmath79 state is not shown for clarity. The solid circle and the lines represent the results obtained from analyzing the effective free energy function [eqs. ([eq6])-([eq8])], while the diamonds illustrate the phase boundary of thermal equilibrium obtained from direct minimization of the underlying free energy functional [eqs. ([eq4])] and ([eq5])]. The anchoring strengths on the homeotropic stripes and planar anchoring stripes are \u03b8_H = @xmath90 and \u03b8_p = @xmath91, respectively.\n\n[Figure 1] (b). During the minimization of the free energy functional described by equations ([eq4]) and ([eq5]), the phase boundary of thermal equilibrium is discovered, which is depicted by diamonds in our analysis. This information is presented in Fig. [fig1] (b), where the phase diagram shows the first-order phase transition between a homogeneous nematic phase (h) and a hybrid aligned nematic phase (han@xmath78), as a function of the anchoring angle at the upper flat substrate @xmath40 and the cell width @xmath35. While the anchoring strength on homeotropic stripes is represented by @xmath90, the anchoring strength on planar anchoring stripes is represented by @xmath91. [Figure 1]: A visual representation of the first-order phase transition between homogeneous (H) and hybrid aligned nematic (HAN) phases (a) with the critical point denoted by a solid circle at xmath92 and xmath93. The dotted-dashed lines indicate the limits of metastability for HAN(1) and H(2) phases, respectively. These lines are derived by analyzing the effective free energy function as described by the equations in discussion. The boundaries of metastability for the han@xmath78 (1) and the h (2) state are indicated by the dot-dashed lines, which were derived from analyzing the effective free energy function [equations ([eq6])-([eq8])]. The diamonds represent the phase boundary, and the solid circle represents a critical point as obtained from a direct minimization of the underlying free energy functional [equations ([eq4]) and ([eq5])]. However, for @xmath94, the phase transition and limits of metastability cannot be determined using the effective free energy function due to the unknown @xmath95 value in the region close to its maximum. For clarity, the figure demonstrates this. In the context of the effective free energy function, for @xmath94 and @xmath95, it is not possible to determine the phase transition or the limits of metastability, as the latter is not known in the region near its maximum. To clarify, the phase diagram for positive @xmath40 is shown, as depicted in Figure 2. Contrary to expectations, the phase transition between the h and han textures, despite the preference for planar anchoring, is still first order. This effect has been predicted for the specific scenario of @xmath98, where monostable anchoring exists on a homogeneous lower substrate, as reported in references [1, 2]. The phase transition from the h texture to the han texture is a first-order transition despite the favouring of monostable planar anchoring, as observed in the empirical surface free energy of a nematic liquid crystal device. This is demonstrated for the special case of @xmath98 as presented in refs . @xcite by using the expression @xmath99 as input into eq . ( [ eq13 ] ). For @xmath100, the surface free energy has two minima at @xmath101 and @xmath102 in the interval @xmath103 $] $, making it bistable, while for @xmath104, only the minimum @xmath101 exists, i.e., the surface is monostable. Thus, this study aims to investigate the stability limit of the h phase. In the case of $x\\in(x_{101},x_{102})$ for the given surface free energy, denoted as $\\phi(x)$, there are two local minima, namely, $x_{101}$ and $x_{102}$. This results in a bistable system, whereas for $x\\in(x_{104},x_{103})$, only the minimum $x_{101}$ is present, thereby indicating a monostable surface. In order to understand the stability limit of the $h$ phase, we perform a sixth-order expansion of $\\phi(x)$ around $x_{101}$ and refer it as $\\tilde{\\phi}(x)$:\n\n$\\tilde{\\phi}(x) = \\phi(x_{101}) + \\frac{1}{2!} \\phi''(x_{101})(x-x_{101})^2 + \\frac{1}{3!} \\phi'''(x_{101})(x-x_{101})^3 + \\frac{1}{4!} \\phi''''(x_{101})(x-x_{101})^4$\n\n$+ \\frac{1}{5!} \\phi'''''(x_{101})(x-x_{101})^5 + \\frac{1}{6!} \\phi''''''(x_{101})(x-x_{101})^6$\n\nThe $h$ phase of this system corresponds to a local minimum in $\\tilde{\\phi}(x)$, where its maximum stability requirement is $a_{8}\\equiv\\phi''''''(x_{101})>0$.\n\nA standard bifurcation analysis can be carried out to investigate the transition from the $h$ phase to the $han$ phase, which can be either a first-order or a continuous transition depending on whether the condition $a_6 \\equiv (-1^5/2^46!)\\phi''''''''(x_{101})<0$ is met. In regards to Eq. 13, values of @xmath101 ranging from approximately up to the sixth order correspond to the appearance of a local minimum, known as the h phase, in Eq. 13's energy function. If a particular @xmath107 value is reached, standard bifurcation analysis reveals that the transition from the h phase to the Han phase can potentially be either first order or continuous depending on the value of @xmath108. This transition is continuous if @xmath109 holds true, but it is first order if @xmath110 is met. The @xmath111 point is a tricritical point, where the two phase boundaries intersect. However, the order of the phase transition is based solely on the surface free energy in the vicinity of @xmath101 for @xmath98, regardless of whether the surface is characterized by a monostable or monotonous surface free energy. For instance, in the case where a monostable surface with a monotonic surface free energy, equivalent to the one depicted in Fig., it is possible to still observe a first-order phase transition. The boundary indicating thermodynamic equilibrium is defined by the equation @xmath112, where the order of phase transition solely relies on the surface free energy @xmath113. Notably, when approaching @xmath101, there can be a first-order phase transition that emerges even for monostable surfaces with monotonically decreasing surface free energies, as shown in Fig. [fig2]. These conditions, such as for a surface characterized as having a monostable surface and a monotonic surface free energy, can be represented by Eq. [eq14] with a certain set of parameters @xmath116 and @xmath117. First-order phase transitions between h and han textures are particularly crucial for bistable liquid crystal displays, allowing for the existence of locally stable molecular configurations corresponding to light and dark states, without requiring power to maintain either state when the applied voltage is removed @xcite. By contrast, monostable liquid crystal displays necessitate both power to switch and to sustain the light and dark states. In particular, first-order phase transitions between the h and han texture in bistable liquid crystal displays have drawn increased attention. In these displays, the molecular configurations corresponding to the light and dark states are locally stable when the applied voltage is removed. As a result, switching from one stable state to another is the only power that is required. In contrast, monostable liquid crystal displays require ongoing power to maintain the light and dark states. We now focus on a scenario where @xmath118, critical for evaluating @xmath 119 [Eq. [9]], cannot be calculated due to the inability to meet the requisite conditions [Eq. [11]]. Using the parameters @xmath 90, @xmath 91, and @xmath 74 for the system presented in Fig. [1] (a), we will investigate this case in more detail. In the system illustrated in figure [fig1] (a), we have selected parameters @xmath90, @xmath91, and @xmath74. A depiction of the displacement and equilibrium position is showcased in figure [fig3] (a), and figure [fig3] (b) presents the corresponding phase diagram. However, it is evident from the solid line in figure [fig3] (a) that determining the effective surface free energy function for all values of @xmath42 is not feasible because the upper flattened substrate @xmath36 is positioned too far from the lower patterned substrate to facilitate the realization of all possible average anchoring orientations @xmath42. Essentially, the anchoring energy on the patterned substrate is too great to be counterbalanced by the elastic energy at the chosen mean distance @xmath36 between the substrates (refer to figure [fig1] for clarity). In Figure 3 (b), it is evident that the solid line indicates the inability to determine the effective surface free energy function for all values of @xmath42 due to the upper flat substrate, situated too far from the lower patterned substrate (as shown in Figure 3 (a)). In simpler terms, the anchoring energy at the patterned substrate is considerably high, which cannot be balanced by the elastic energy, resulting in partial information about the effective surface free energy function. However, as demonstrated in Figure 3 (b), this restricted information can still be used to calculate the phase diagram for cell widths significantly larger than the width at the critical point @xmath120. The previous subsection highlighted that by adopting an appropriate chemical and geometric surface morphology on one of the interior cell surfaces, two stable nematic director configurations can be sustained. Figure 3 (b) shows that even with only partial information about the effective surface free energy function, one can calculate the phase diagram for cell widths significantly greater than the width at the critical point at xmath120. In a previous section, we demonstrated that utilizing a chemically and geometrically tailored surface morphology on one of the interior surfaces of a liquid crystal cell supports two stable nematic director configurations. The zenithally bistable nematic devices that have been studied recently consist of a nematic liquid crystal confined between a chemically homogeneous grating surface described by Eq. (121), where xmath34 is the grating depth, xmath66 is the period, and xmath122 represents the degree of asymmetry in the grating profile. Figure 1 depicts an asymmetric surface grating consisting of a profile given by the equation @xmath121, where @xmath34 denotes the groove depth, @xmath66 represents the period, and @xmath122 is the blazing parameter defining the asymmetry of the profile. Brown et al. studied this grating substrate and observed a first-order transition from the homogeneous nanopillar state (Han state) possessing a low pretilt angle (\u03b8Han \u2248 @xmath123) to the nematic homeotropic state (H state) characterized by a high pretilt angle (\u03b8H \u2248 @xmath124) [1]. Contrary to common misconception, the H state does not necessarily indicate a homeotropic texture on this grating surface, as described in subsequent discussions. \n\n[ fig1 ]  ( c ) ) and a flat substrate with strong homeotropic anchoring .\n\nThe grating substrate depicted in Figure 1 consists of a substrate with a strong homeotropic anchoring characteristic, a particular attribute that allows the molecules to align along the substrate normal. The profile of the asymmetric surface grating is determined by the expression @xmath121, where @xmath34 is the depth of the groove, @xmath66 represents the period, and @xmath122 denotes the asymmetric profile parameter. This grating surface has been studied extensively by Brown et al. to better understand the behavior of nematic liquid crystals confined between glass plates with aligned homeotropic anchoring [1]. \n\nIn their studies, Brown et al. discovered a transition between the homogeneous nanopillar state and the nematic homeotropic state, a phenomenon that occurs when the pretilt angle (\u03b8) changes. Specifically, they observed two distinct pretilt angles: a low pretilt angle (\u03b8Han \u2248 @xmath123) for the nanopillar state (or Han state), and a high pretilt angle for the nematic homeotropic state (or H state) characterized by a high pretilt angle (\u03b8H \u2248 @xmath124). \n\nIt should be emphasized that while a high angle of orientation commonly characterizes the nematic homeotropic state (or H In previous research, @xcite discovered a first-order transition between the low pretilt angle Han state, denoted by $\\theta_H$, and the high pretilt angle H state, denoted by $\\theta_H'$. In this context, the H state does not directly correspond to the homeotropic texture, which is shown in Figure $[ \\text{fig\n5} ]$ (b) below, but rather employs the same notation for consistency. In this analysis, we delve further into the study of phase transitions of a nematic liquid crystal in contact with a blazed surface using the effective free energy method introduced in Section II c. Therefore, if we call the homeotropic texture $H$ for brevity, the $\\theta_H$ and $\\theta_H'$ states in question are associated with deviations from this ideal texture. To better elucidate the phase transitions of a nematic liquid crystal in contact with a blazed surface, we employed the effective free energy method described in Section II, c, to analyze the system in greater detail. The minimized free energy and calculated effective surface free energy at various anchoring strengths, groove depths, and blazing parameters are depicted in Fig. [Fig. : Blazed] (a). The asymmetric effective surface free energy is a consequence of the asymmetry of the grating surface, which causes an asymmetric phase diagram when plotted as a function of the anchoring angle on the upper surface and the distance. Such an analysis affords greater insight into the behavior of nematic liquid crystals under specific geometric constraints. The effective surface free energy displays an inherent asymmetry due to the grating surface's asymmetry. This asymmetry results in a non-symmetric phase diagram that is plotted as a function of the anchoring angle on the upper surface and the distance. The topology of the phase diagram is similar to that seen in the case of a symmetric substrate as illustrated in figures [fig2] (b) and [fig3] (b). For a detailed discussion, please see section [section]. However, it is noteworthy that the phase diagram's asymmetry originates from the grating's asymmetry. In the phase diagrams, the topology is identical to that of a symmetric substrate, as illustrated in Figs. [Fig. 2(b)], [Fig. 3(b)], and for a more comprehensive analysis, please refer to Section III.C below. Furthermore, the expression [Eq. ([Eq. Blazed])] remains valid regardless of the substrate's symmetry. The locally homeotropic anchoring strength on the blazed surface, denoted as c, follows equation ( [ eq : blazed ] ), where the groove depth is labelled as xmath131 and the blazing parameter is xmath127. The extension of the cell in the invariant xmath26 direction is xmath28, and the isotropic elastic constant is represented as xmath85. The effective surface free energy, similarly to , exhibits periodicity with a period of xmath132 but is asymmetric with respect to xmath133. The extension along the invariant direction @xmath28 is referred to as extension @xmath26, while the isotropic elastic constant is denoted as @xmath85. The effective surface free energy, similarly, is periodic with a period of @xmath132 but displays asymmetry with respect to @xmath133. In (b), the phase diagram for the same system is presented as a function of the anchoring angle at the upper flat substrate @xmath40 and cell width @xmath35. The triple point where the h, han@xmath78, and han@xmath79 phases coexist is located at @xmath134 and @xmath135, and critical points are found at @xmath136, @xmath137, and @xmath138, @xmath139. [fig2] and [fig3] illustrate these concepts. In Figures 2 and 3, the triple point, where the han@xmath78, han@xmath79, and h phases coexist, is situated at @xmath134 and @xmath135. The critical points, denoted by the solid circles, occur at @xmath136, @xmath137, and @xmath138, @xmath139. The figure depicts the limits of metastability for the han@xmath79 (1) and h (2) phases for negative @xmath40. For clarity, only these limits are shown.\n\nFigure (c) and Eq. ([eq:blazed]) present how the director orientation on a blazed surface with strong homeotropic anchoring and local homeotropic anchoring changes as a function of the groove depth, @xmath140, and the cell width, @xmath129. The anchoring strength on the blazed surface is @xmath130. \n\n<|user|>\nCould you please provide a brief explanation of Figures 2 and 3 and their relevance to the topic being discussed? With local homeotropic anchoring and a strongly anchoring flat surface as a function of groove depth and cell width, the behavior of the system is investigated. The anchoring strength on the blazed surface is specified as \u03c6. When considering the groove depth and cell width as variables, the lines in the figure denotes the first order transitions from homeotropic (H) to hybrid aligned (HAN) phases, represented by different values of the variable \u03b5 (not illustrated). For small groodepths, these lines extend to a first-order transition between planar and homeotropic phases at \u03b4, which is not shown in the figure. As the groove depth is increased, the first-order transitions lines converge at critical points that are not represented in the figure. Energy barriers at the first-order transitions are also presented with the same line code as in Figure A. In Figure [fig4], the first order transition lines terminate at critical points which are not depicted on the graph. While the lines are obtained from the total effective free energy, as shown in Equation [eq6], the diamonds indicate energy barriers encountered between the planar and homeotropic effective anchoring in the hybrid aligned (han@xmath79) phase illustrated in Figure (a) and the homeotropic (h) phase illustrated in Figure (b) for a nematic liquid crystal confined between a blazed surface with local homeotropic anchoring, characterized in Figure [fig1] and Equation [eq:blazed], and a flat surface with strong homeotropic anchoring (@xmath98). In this scenario, wherein a nematic liquid crystal is confined between a blazed surface, as depicted in Figure 1, with local homeotropic anchoring, and a flat surface at the top with strong homeotropic anchoring, specific parameters are utilised. The diamonds displayed in Figures a and b correspond to the two minima of the effective surface energy function. The model parameters and line code are consistent with those featured in Figures . However, to provide a more practical perspective, in this article, we shall focus on the most intriguing case, which employs strong homeotropic anchoring at the upper homogeneous surface. In figure [fig4], the phase diagram is shown for a few values of the blazing parameter at a constant local homeotropic anchoring strength on the upper homogeneous surface for a fixed cell width. Asymmetry causes a decrease in the groove depth required for a first-order transition from the Han to Ha phases compared to the nematic liquid crystal cell with a symmetric surface grating. Upon increasing groove depth, the transition line ends at a critical point (not shown in the figure) where it diverges as X increases. The groove depth represents an _anchoring_ (or surface) transition between low tilt and high tilt surfaces. As the transition line approaches a critical point, it diverges in accordance with @xmath145. Meanwhile, the groove depth @xmath146 corresponds to a significant transition between a low tilt and high tilt surface states, considered as the homeotropic and planar effective anchoring states, respectively, in the context of @xmath144. The effective free energy method proves to be advantageous in this regard, as it enables the calculation of an energy barrier @xmath147 that is inaccessible through direct numerical minimization of the free energy functional. This implies that the effective free energy method provides insights that would have been unattainable using traditional methods. The effective free energy method facilitates calculating the energy barrier between two bistable states when direct numerical minimization of the free energy function proves infeasible. In this method, the energy barrier can be determined at a particular value of x, denoted as $\\Delta G$. The calculated energy barrier, which can be seen in Figure 4 (b), decreases as the groove depth increases and ultimately disappears when reaching the critical point. This asymmetry in the surface grating contributes to the decreased energy barrier between the bistable states. In Figure 4(b), a decline in the energy barrier is observed due to the asymmetry of the surface grating. A decrease in the groove depth @xmath140 leads to a reduction of the energy barrier, which eventually vanishes near the critical point. The diamond-shaped symbols denote the energy barriers for the anchoring transitions of a nematic liquid crystal in contact with a single grating surface. This quantity is crucial for designing a zenithally bistable nematic device, where two bistable states exist. The energy barriers for the nematic liquid crystal's anchoring transitions are denoted as \"b\" in Fig. 4(b), and provide crucial data for designing zenithally bistable nematic devices. Inadequately small energy barriers compared to K148 will result in unwanted switching between the two stable states due to thermal fluctuations. Conversely, enlarging the energy barrier leads to increased power consumption. By employing the calculated values from Fig. 4(b), using a cell of size A149 x W150 and typical values D151 and e152, it results in E153 for the energy barrier at a twisting angle of \u03b8154, which is an acceptable value. In a real nematic liquid crystal cell with an area of @xmath149 and a width of @xmath150, the energy barrier can be estimated using the typical values of @xmath151 and @xmath152. This yields @xmath153 for @xmath154 and @xmath155, which seem to be an acceptable value. Another crucial factor in grating-assisted nematic devices is the average director orientation at the grating surface in the two degenerate states, as demonstrated in Figs. [Fig. 4] and [Fig. 5], both illustrating results for the same model parameters. Specifically, Fig. [Fig. 4(b)] depicts this angle, referred to as the average surface director, for the Han ( @xmath156 ) and h ( @xmath157 ) states. The average surface director angles \u03b8 and \u03b8\u2081 in the Han system, represented by @xmath156 and @xmath158, respectively, are depicted in Figure 5 for the same model parameters as in Figure 4. These angles are derived from the orientation of the director in the nanogratings. The figure illustrates the asymmetry of the surface grating effect, which leads to a decrease in @xmath156 and an increase in @xmath158. Specifically, when the depth of the groove @xmath140 increases, the difference between the two angles diminishes, eventually vanishing as the critical point is approached (not shown in the figure). The imbalance in the depth of grooves present in the grating surface results in a decline in @xmath156 and an increase in @xmath158 values. Fixing a specific value of @xmath122, the difference between the angle values decreases with an increase in groove depth, and vanishes at the critical point. This asymmetry in surface grating leads to a drop in groove depth necessary for bistability and a lowered energy barrier, which ameliorates optical properties and lowers power consumption of a zenithally bistable nematic device. Still, the difference between the two bistable states is also decreased, potentially affecting the optical properties of the device. Phase diagrams in the @xmath159 plane are illustrated in Section [ iiia ], which can be explained by the surface free energy provided by the given equation. Furthermore, as we discussed in section [iiia], the phase diagrams in this system can be described using the surface free energy presented in Eq. [1]. However, for a more encompassing perspective, it may be beneficial to consider a more general scenario where the surface free energy follows from the truncation of Fourier expansion, as depicted in Eq. [2]. This allows us to investigate not only symmetric, but also asymmetric surfaces by introducing a generalized version of Eq. [1], denoted by Eq. [3]. In the case of a symmetric surface characterized by Eq. [4], Eq. [3] reduces to Eq. [1]. To investigate both symmetric and asymmetric surfaces, we adopt a natural generalization of Eq. ., which in the case of a symmetrical surface characterized by Eq. . reduces to Eq. . In this scenario, the angle that minimizes Eq. is a function of angles @xmath40 and @xmath35. The derivative of Eq. is a measure of susceptibility that diverges at the critical thickness @xmath165, and remains finite and positive for angles @xmath107. By applying Eq. , the conditions for the critical angle @xmath166 can be obtained, where they both depend only on the form of the surface free energy given by Eq. . . The derivative, denoted by ^x with respect to x, diverges at the critical thickness, denoted by t_c, and remains finite and positive for thicker substrates in the system. This is evident from Eq. . By satisfying the conditions for the critical angle, denoted by \u03b8_c, as Eq. implies, the values of \u03b1_s and \u03b8_c only depend on the form of the surface free energy, given by Eq. . The notation of @xmath168 and @xmath169 refers to different non-uniform textures, where the thick lines signify first-order transitions, and black circles mark critical points. The phase diagram can be either type (a) or (b) when a minima and a maximum exist for @xmath11 defined on the unit circle @xmath170. If two minima and two maxima exist, the phase diagram is of type (b) unless the minima are of equal depth, in which case it is of type (c). When the function @xmath11, defined on the unit circle at @xmath170, has a single minimum and maximum point, the resulting phase diagram can either be of type (a) or type (b). However, if the function has two local minima and two local maxima, the phase diagram will always be of type (b), unless the minima are of equal depth, in which instance it falls into the category of type (c). To create schematic phase diagrams, we view @xmath11 as a function that varies with the parameters @xmath116 and @xmath117, defined on the unit circle. Regardless of the location of critical points or transition lines, the focus here is on exploring the possible topologies that can emerge in the @xmath159 plane based on the values of those parameters. When considering phase diagrams, the specific location of critical points is not the primary focus in our research. Rather, we aim to understand the potential topologies that may arise in the @xmath159 plane based on equations @xmath119 and @xmath120. When defining the function @xmath113 on the unit circle, which has either one minimum and one maximum or two minima and two maxima, similar conclusions apply to @xmath121 as well. The presence of either one or two critical points in the phase diagram can be determined based on these equations. \n\nIn the limit of large @xmath35, there is always a first-order phase transition between non-uniform textures with opposite orientations of the director at @xmath21 due to the fact that @xmath122 is not a periodic function of @xmath175 at fixed @xmath40. At the transition point, we can approximate the free energy using @xmath123, where the extrapolation length @xmath178 is used to account for fluctuations near the minimum. These insights provide essential information for understanding potential phase diagrams and transitions in various systems. In the absence of uniform textures in the system, a first-order phase transition occurs between two non-uniform textures with opposite director orientations, when transition length scale \ufffd\ufffd\ufffd reaches a critical point where the periodic system function \ufffd\ufffd\ufffd174 is no longer a periodic function of \ufffd\ufffd\ufffd175. To locate this critical \ufffd\ufffd\ufffd value, we expand \ufffd\ufffd\ufffd11 around its deepest minimum (denoted \ufffd\ufffd\ufffd176) and approximate the free energy to be:\ufffd\ufffd\ufffd\ufffd\ufffd177 = \ufffd\ufffd\ufffd11(\ufffd\ufffd\ufffd176) + {{1\u2009\u2009\u00a0\u2009 \u2009 \u2009/\\ 2}\\big{<}{\\big{(}{\\Delta\\delta}^{2}\\big{)}}_{+}}} (\ufffd\ufffd\ufffd178-\ufffd\ufffd\ufffd176)^2,\n\nwhere \ufffd\ufffd\ufffd178 is the extrapolation length and \u27e8(\u0394\u03b4^2)\u27e9_ are the quadratic fluctuations around the minima \ufffd\ufffd\ufffd176 and \ufffd\ufffd\ufffd179. Since both \ufffd\ufffd\ufffd176 and \ufffd\ufffd\ufffd179 are equivalent minima of \ufffd\ufffd\ufffd11 within the interval [\ufffd\ufffd\ufffd180, \ufffd\ufffd\ufffd181] of the periodic function \ufffd\ufffd\ufffd174, the phase transition occurs at \ufffd\ufffd\ufffd181 if \ufffd\ufffd\ufffd182 or at \ufffd\ufffd\ufffd183 if \ufffd\ufffd\ufffd184. This information enables us to draw the phase diagram, where, away from the transition lines, there exists a smooth evolution from one texture to another. These phase transition lines identify vertical lines within Figure [fig]: diagram. Due to the vertical lines at @xmath171 being equivalent, the phase diagram can be viewed on a cylindrical surface. Far from the transition lines, there is a seamless transition from one texture to another. Essentially, by maintaining a fixed value of @xmath35, it's possible to convert the @xmath185 texture to the @xmath186 texture without crossing into the transition line, even for @xmath187. Notably, within a certain range of parameters, the phase diagram for @xmath11 with a single minimum is identical to that for @xmath11 with two minima, possessing two critical points and a triple point in both cases. Fig. [Fig.: Diagram] (b) illustrates this similarity. In certain parameter ranges, the phase diagram for @xmath11 with one minimum shares similar topology with that of @xmath11 containing two minima as illustrated in Figure [Figure]. [Figure] (b) highlights the presence of two critical points and a triple point in both scenarios. When the two minima are equal, as in the example of a symmetric surface, the triple point vanishes, and the first-order transition lines extend to a higher value, as shown in Figure [Figure] (c). This investigation focuses on the phase behavior of a nematic liquid crystal confined between a flat substrate and a patterned substrate, as depicted in Figure [Figure]. In our recent study, we investigated the phase behavior of a nematic liquid crystal sandwiched between a flat and a patterned substrate, depicted in Figure 1. Using the Frank-Oseen model [Eq. (4)] and the Rapini-Papoular surface free energy [Eq.(5)], we derived an expression for the effective free energy function of the system [Eq.]. The study presented an effective free energy function for a nematic liquid crystal confined between a flat surface with strong anchoring and a chemically patterned sinusoidal surface. To derive this expression, the surface free energy based on the Landau-de Gennes theory was considered, including both interfacial and anchoring contributions. The anchor points for the nematic director were assumed to deviate from the substrate geometry by a fixed value [eq4], while the sinusoidal pattern was captured using [eq5]. By determining an effective surface free energy that characterizes the anchoring energy at the patterned surface [eq6], the phase behavior of the system, featuring first order transitions between homeotropic (H) and hybrid aligned nematic (HAN) textures for different sinusoidal wavelengths and amplitudes [figs not reproduced here], could be obtained. In this study, we have investigated the phase behavior of nematic liquid crystals trapped between a flat surface with strong anchoring and a chemically patterned sinusoidal surface. Contrary to the conventional belief that a monostable surface should not lead to a first-order phase transition, we have found first-order transitions between a homeotropic texture (H) and hybrid aligned nematic (HAN) textures. This surprising discovery is evidenced in Figure 1(a), showing the sinusoidal surface. Figures 2(b) and 3(b) illustrate the H and HAN textures, respectively. The surface free energy function @math114 has a monotonic characteristic, as shown in Figure 2(a). The unexpected finding highlights a potential application of using chemically patterned surfaces to manipulate liquid crystal phases and may offer insights into the complex physics of liquid crystal systems. \n<|>\nIn this examination, the phase behavior of nematic liquid crystals confined between a flat surface with strong pinning and a chemically patterned sinusoidal surface has been determined. In contrast to prior assumptions that a monostable surface would not cause a first-order phase transition, a first-order transition was observed between a homeotropic texture (H) and a hybrid aligned nematic (HAN) texture. Figures 1(a), 2(b), and 3(b) demonstrate these textures, respectively. As shown in Figure 2(a), the surface free energy function has a monotonic character. The discovery suggests a novel application for the use of chemically patterned surfaces to manipulate liquid crystal phases and provides insight into the intricate physics underlying these systems. \n<|>\nIn our research study, the phase behavior of liquid crystals trapped between a flat surface with strong pinning and a chemically patterned sinusoidal surface has been investigated. Contrary to prior expectations that a monostable surface would not lead to a first-order phase transition, we found first-order phase transitions between a homeotropic texture (H) and a hybrid aligned nematic (HAN) texture. These textures are evident in Figures 1(a), 2(b), and 3(b), respectively. While the surface free energy function has a monotonic characteristic, as shown in Figure 2(a), the surprising discovery suggests It is noteworthy that despite a monostable surface with a monotonous surface free energy function specified in Fig. [fig2] (a), wherein @xmath114 $ ] , a first order phase transition can still occur. This is evident from our direct minimization of the original free energy functional, which resulted in remarkably accurate phase boundaries compared to the phase boundaries determined by analysis of the effective energy function (Figs. [fig2] (b) and [fig3] (b)). The effective energy function analysis was used to investigate the phase behavior of equations [ eq4 ] and [ eq5 ] on a two-dimensional grid. The results obtained through this method exhibited a remarkable agreement with the phase boundaries found through the analysis. Therefore, this method is quantitatively reliable for predicting the phase behavior. By utilizing this technique, we have also studied the phase behavior of a nematic liquid crystal confined between a chemically uniform, asymmetrically grooved substrate (as depicted in figures [ fig : blazed ] (b))). In our research, we have investigated the phase behavior of a nematic liquid crystal confined between a chemically uniform and asymmetrically grooved substrate, which is typically used for a zenithally bistable nematic device @xcite. The locally homeotropic anchoring and strong homeotropic anchoring on the flat substrate caused a decrease in groove depth where a first-order transition between the hybrid (h) and homeotropic-anchor nematic (han) phases occurs (fig. [fig1]). According to fig. [fig], we observed the blazed phases for increasing grating angle. These findings provide insights into the design of asymmetrically grooved substrates for zenithally bistable nematic devices. In this experiment, a configuration with locally homeotropic anchoring and a strong homeotropic substrate (commonly employed for zenithally bistable nematic devices) is established. The asymmetry in the substrate's grating leads to a drop in groove depth for the first-order transition between the homogeneous (H) and homeotropic anchoring with nematic (HAN) phases (as illustrated in Fig. [ fig4 ] (a)). Furthermore, we have measured the energy barrier for these two coexisting states (Fig. [ fig4 ] (b)). Furthermore, we have identified the energy barrier between the coexisting states, illustrated in Figure 4(b), through our calculations. Notably, the energy barrier decreases with an increase in the grating surface asymmetry, yet remains well above 190 degrees for standard nematic liquid crystal devices. Additionally, we have calculated the average orientation of the director at the grating surface in both bistable states, as shown in Figure 5. Moreover, the average director orientation at the grating surface in two bistable states has been determined (Figure 5). This difference between the two bistable orientations diminishes with increasing substrate asymmetry, thus negatively affecting the optical properties of a zenithally bistable nematic device. The parry-jones model for the effective surface free energy, previously studied for isotropic substrates, has been extended to cover asymmetric structured substrates. The resultant phase diagram in the plane spanning the orientation of the director at the homogeneous surface and the thickness of the nematic cell includes three possible types. In previous research, Parry and Jones proposed a model for the effective surface free energy that is applicable for symmetric substrates. Recently, this model has been generalized for asymmetric substrates, resulting in three distinct types of phase diagrams constructed in the plane that encompasses the orientation of the director on the homogeneous surface and the thickness of the nematic cell. Unlike in the symmetrical case, the asymmetry of the substrate leads to the mere shifting of transition lines and critical points, without significantly altering the topology of the phase diagram. This generalization has been verified in the context of a nematic liquid crystal confined between a planar homogeneous substrate and a substrate with grooves, allowing for the reproduction of the qualitative phase diagram (Figure 1). The figure represents two examples of the phase diagram, presented under different orientation angles between the grooves on the substrate and the director. Upon verification, it has been demonstrated that the proposed model is capable of qualitatively recreating the phase diagram of a nematic liquid crystal in a scenario where it is situated between a homogeneous planar substrate and an asymmetrically grooved surface. The phase diagram can be visualized through diagrams such as figures [Fig: Blazed] (b) and [Fig: Diagram] (b) [Fig]. The qualitative resemblance to the actual phase diagram signifies that the model adequately depicts the behavior and characteristics of nematic liquid crystals in such confined environments.",
        "abstract": " we study the phase behavior of a nematic liquid crystal confined between a flat substrate with strong anchoring and a patterned substrate whose structure and local anchoring strength we vary . by first evaluating an effective surface free energy function characterizing the patterned substrate we derive an expression for the effective free energy of the confined nematic liquid crystal . \n then we determine phase diagrams involving a homogeneous state in which the nematic director is almost uniform and a hybrid aligned nematic state in which the orientation of the director varies through the cell . \n direct minimization of the free energy functional were performed in order to test the predictions of the effective free energy method . \n we find remarkably good agreement between the phase boundaries calculated from the two approaches . \n in addition the effective energy method allows one to determine the energy barriers between two states in a bistable nematic device . ",
        "section_names": "introduction\neffective free energy function\napplications\nsummary"
    },
    {
        "article": "The International Atomic Energy Agency (IAEA), established in 1957, is the world's central intergovernmental organization charged with promoting the peaceful use of nuclear energy and the peaceful application of nuclear technology. Its mandate encompasses various aspects of nuclear technology, including nuclear power generation, basic science, and radiological applications in fields such as medical care, agriculture, and industry. The IAEA also plays a crucial role in safeguards and verification regimes, working to ensure that nuclear materials are not being diverted to military purposes. The agency's headquarter is located in Vienna, Austria, and it has a presence in over 100 countries around the world. The framework of structure formation theory, which is realized through large-scale n-body simulations, has robust predictions for cluster space density and intracluster dynamics in various cosmological models. Clusters are the most observationally accessible components of large-scale structure, providing a crucial opportunity to impose stringent constraints on both cosmological parameters and the evolution of structure. However, there remains a significant challenge in correlating the theoretically predicted dark matter halos with the baryonic structures we observe. Through the joint efforts of theory and observations, substantial progress is being made in bridging this gap. Simulations at higher complexity and resolution have provided new insights into the evolution of baryons within clusters, and efforts are being made to identify the connection between cluster galaxies and dark matter substructure. Concurrently, large sample catalogs of clusters detected and observed through diverse methods are being amassed, contributing to a wealth of information for structure cosmology research @xcite . Theory-based computer simulations, with growing levels of intricacy and resolution, continue to offer insights and a deeper understanding into the development of baryonic material within clusters of galaxies. On the observational front, an extensive effort is being undertaken to gather large samples of clusters detected and analyzed using a diverse array of methodologies. The use of this wide variety of approaches is enabled by the sheer abundance of observable traits for galaxy clusters, which include optical light produced by individual galaxies and intracluster stars, as well as X-rays emitted by the hot intracluster medium and active galactic nuclei within cluster galaxies. In addition to optical light, individual cluster galaxies and intracluster stars also generate optical radiation. X-rays, on the other hand, are emitted by both the hot intracluster medium (ICM) and active galactic nuclei (AGN) within the cluster galaxies. This same ICM alters the spectrum of microwave background radiation passing through the cluster. Furthermore, the total projected mass distribution of the cluster provides a weak or strong gravitational lensing effect on the images of background galaxies. These five observables offer a unique opportunity to detect clusters and quantify their properties. By combining and cross-referencing these observation techniques, our understanding of cluster physics can be validated and confirmed in various ways. Throughout the centuries, clusters have been detected in the sky as anomalous groupings of similarly bright galaxies. Optical surveys have been integral in identifying and cataloging clusters, as they have the capability to detect low-mass objects at a reasonable cost. However, early optical detection methods were often plagued by projection effects. With the advancement of CCD photometry, precise measurements of galaxies clustered in space, brightness, and color have become possible, significantly reducing the problem of projection. In combination with weak and sometimes strong lensing distortions produced by the total projected mass distribution, these observables provide numerous opportunities to cross-check our understanding of cluster physics in various ways. These early detections of clusters, dating back to the 18th century, have shown evidence for the existence of dark matter, which was further supported by more recent surveys. A comprehensive review of optical selection can be found in * ? ? * . For many years, optical surveys have been the most cost-effective method to detect clusters, largely due to their ability to detect low-mass objects inexpensively. Early optical detection provided evidence for dark matter, yet was limited by the presence of projected galaxies along the line of sight. However, precise CCD photometry enabled more accurate detection methods for galaxies clustered in space, brightness, and color. This significantly reduced projection effects and provided photometric redshifts. In contrast, X-ray detection of thermal emission from hot intercluster medium through the use of X-ray satellites provides a significantly higher contrast, largely immune to projection effects since X-ray emission is dependent on the square of the density. (For a review of optical selection, see * ? ? ? *.) The introduction of X-ray satellites marked a significant development in the field of astronomy, as it enabled the detection of thermal emission from the highly ionized gas that inhabits cluster interiors @xcite. X-ray emission shows an inverse square dependence on the gas density, making it a sensitive and efficient diagnostic tool that can provide a high contrast signal largely immune to projection effects. However, when the spatial resolution is low, the presence of non-thermal X-ray sources can pose a significant challenge. These include AGN, the non-thermal emission from cooling cores and ongoing merger activity that has disturbed the thermal equilibrium of the cluster. Over the past several decades, X-ray surveys have been utilized to assemble a sizable number of cluster catalogs @xcite. For several decades, X-ray surveys have amassed a considerable number of cluster catalogs that have significantly advanced our comprehension of cluster physics. These catalogs primarily involve point sources, including active galactic nuclei (AGN) and non-thermal emission from cooling cores. Moreover, ongoing merger activity, which disrupts the thermal equilibrium of clusters, is also an essential contributor to these catalogs. However, incomplete and unbalanced catalogs have been a major obstacle in comparing optical and X-ray properties of clusters. In this paper, we aim to provide a comprehensive comparison of the optical and X-ray properties of clusters by utilizing the largest and most uniformly observed samples of clusters across both passbands. Extensive comparisons between optically and X-ray selected catalogs have been hindered by the lack of uniformly observed samples. This paper aims to remedy this, presenting measurements of the X-ray properties of the largest publicly available optically selected cluster sample, the MAXBCG catalog, spanning the redshift range from z = 0.1 to 2. Uniform optical photometry and precise photometric redshifts for all the clusters are available from the Sloan Digital Sky Survey (SDSS) database. The MAXBCG catalog provides a significant and volume-limited set of optically selected clusters, addressing limitations inherent to previous optically selected analyses. The limited cluster catalog, covering a redshift range from @xmath2, was produced through uniform optical photometry and precise photometric redshifts from the Sloan Digital Sky Survey (SDSS: * ? ? ? *) data used to select the clusters. Although x-ray observations from the ROSAT All-Sky Survey (RASS: * ? ? ? ) lack individual detections due to shallow exposures, they offer precise measurements of mean x-ray luminosity, in relation to optical richness and redshift, for all the clusters in the catalog. Wikipedia-style rephrasing:\n\nX-ray observations for all MAXBCG clusters are accessible from the ROSAT All-Sky Survey (RASS), and although shallow exposures prevent the detection of every individual cluster, precise measurements for the mean X-ray luminosity (@xmath0) as a function of optical richness and redshift can be obtained through stacking. Furthermore, low signal-to-noise measurements of X-ray emission from individual clusters are useful for substantiating the mean X-ray emission and estimating the scatter in the optical richness-X-ray luminosity relation. The MAXBCG catalog has been examined in complementary ways through studies using dynamical and weak lensing analysis. In a multitude of methods, the MaxBCG catalog has been comprehensively analyzed. For example, the dynamical properties of these clusters have been investigated through the utilization of BOSS data, which is designated as BOSS07. Additionally, weak lensing techniques have also been applied to the SDSS data to better understand the cluster's structure, which is labeled as S07 and J07. By combining these earlier observations with the present measurements of the mean @xmath0, additional insights into cluster physics can be acquired. The data used for the present study involves extraction of * s07 and * j07 observations from the SDSS database, which can be combined with the mean *xmath0 measurements presented in this paper to offer further insights into the physics of cluster formation. The study highlights the *xmath0*xmath12 and *xmath0*xmath13 relations inferred for MAXBCG clusters, as discussed in our previous work (see companion letter) or the corresponding relation obtained from weak lensing. An analysis similar to this has also been performed for the nIR-selected 2MASS cluster catalog, previously presented by DKM07. In a similar analysis to that described in this paper, DKM07 examined the NIR selected 2MASS cluster catalog. This dataset contains approximately 14 nearby ( @xmath15 ) groups and clusters, with masses spanning from @xmath16 , which were identified through a matched-filter algorithm. Notably, DKM07 were the first to measure the mean (stacked) X-ray properties using the RASS technique for a large optically-selected cluster catalog, rather than simply cross-correlating optically- and X-ray-selected clusters. Their analysis revealed that the X-ray luminosity of 2MASS clusters correlates with optical richness ( @xmath17 ), which represents the number of galaxies brighter than a particular threshold within @xmath19 . In this study, dkm07 utilized a sizeable cluster catalog selected based on optical criteria and measured their mean X-ray properties using the RASS method instead of cross-correlating between X-ray and optically-selected clusters. The results revealed a correlation between the X-ray luminosity and optical richness, quantified using the number of galaxies brighter than a particular magnitude within a specific radius. This study employed the Sloan Digital Sky Survey (SDSS) data, the MaxBCG catalog, and the RASS data to analyze the X-ray luminosity of individual clusters using specific methods to obtain the mean X-ray luminosity for a group of clusters with similar richness, as described in Section [sec: X-ray analysis]. This investigation briefly reviews the SDSS data, MAXBCG catalog, and RASS data that serve as our basis for study. Section [Sec : X-Ray Analysis], which outlines our measurements of X-ray luminosities for individual clusters, as well as our methods for determining the mean X-ray luminosity for a set of clusters with similar richness, can be found in this section. This study presents the mean relation of X-ray luminosity as a function of richness, the scatter in this relation, and the underlying median vs. X-ray mass relationship in section [Sec : Mean Relation]. Section [Sec : Biases] addresses several possible sources of systematic bias in these results. In section [Sec : Lsig], we combine these results with dynamical measurements of MAXBCG clusters to produce a measurement of the X-ray luminosity versus halo mass relationship. Our study concludes and discusses future work in section [Sec : Summary], where we use a @xmath22CDM cosmology with @xmath23 and @xmath24. In this piece, we explore potential sources of systemic bias in the obtained results. Our investigation is detailed in Section [sec : biases], where we examine several possible factors that may influence these results. Later in Section [sec : lsig], we combine our results with dynamical measurements of MaxBCG clusters to derive an @xmath21 relationship measurement. Conclusions and future work insights are presented in Section [sec : summary]. Throughout our work, we employ a @xmath22cdm cosmology with @xmath23 and @xmath24. Our measurements are based on data obtained from two extensive imaging surveys, namely, SDSS and RASS. To select galaxy clusters, we apply a red sequence selection method on SDSS's five-band imaging data. Additionally, with the help of SDSS, we measure cluster redshifts and richnesses. Galaxy clusters are selected from the Sloan Digital Sky Survey (SDSS) five-band imaging data via a red sequence selection approach. SDSS data permits measurement of cluster redshifts and richnesses as well. The X-ray emission from these clusters is measured with the Roentgen Astronomical Satellite (RXS) photon maps. This section delves briefly into SDSS imaging data, cluster selection and calibration, and RASS input data. Notably, the optical data for this study stems from the Sloan Digital Sky Survey, which constitutes a combined imaging and spectroscopic survey spanning 10,500 square degrees in the Northern Galactic Cap and a smaller region in the Southern Galactic Cap. In this study, we utilize Sloan Digital Sky Survey (SDSS) imaging and spectroscopic data, covering a combined 10,000 square degrees in the northern galactic cap and a smaller area in the southern hemisphere. The optical survey operates in drift-scan mode for each of its five filters (u, g, r, i, and z) with a 5\u03c3 limiting magnitude of ~22.5 at 95% confidence. Photometric errors are typically limited by systematics at the 3% level at bright magnitudes. The spectroscopic survey selects galaxies with an SDSS magnitude of <14.5 (main sample) and a median redshift of z ~ 0.1 (luminous red galaxies) out to z ~ 0.38. For further details regarding the SDSS, readers may consult the survey's official website. Photometric errors in the bright magnitudes are generally limited to 3% uncertainty due to systematic factors. The spectroscopic survey caters to a primary sample of galaxies that have a redshift median of 0.34 and an absolute magnitude value @xmath33, while the \"luminous red galaxy\" sample is volume-limited up to z=0.38. Further details about the Sloan Digital Sky Survey (SDSS) can be found in @xcite and @xcite. MaxBCG cluster catalog selection is performed by leveraging imaging data present in the SDSS's Data Release 5. The MaxBCG algorithm enables the selection of galaxy clusters from this imaging data. The MaxBCG cluster catalog is derived from observational data presented in the Data Release 5 (DR5) of the Sloan Digital Sky Survey (SDSS). The selection of galaxy clusters from this imaging data involves employing the \"MaxBCG\" algorithm, details of which are provided in @xcite. Essentially, the algorithm utilizes two well-known properties of rich galaxy clusters, namely: (1) The bright end of the cluster luminosity function is dominated by red sequence galaxies whose color-magnitude space follows a tight correspondence (the E/S0 ridgeline) with a small dispersion, and (2) Cluster galaxies exhibit strong spatial clustering, and the brightest cluster galaxies (BCGs) are typically located near the center of the galaxy density. Further details and insights into the MaxBCG catalog and its properties may be found in the aforementioned publication. The red sequence galaxies are dominant in the bright end of the cluster luminosity function, displaying a small dispersion in color-magnitude space, particularly along the early-type/late-type (E/S0) ridge line. Furthermore, every cluster possesses at least one distinct brightest cluster galaxy (BCG) located near the center of the galaxy distribution. While not all clusters have an obvious central, dominant galaxy, every cluster still has a red sequence galaxy that is brighter than any other. The algorithm independently measures the probability that a galaxy is situated in an overdensity of E/S0 ridgeline galaxies with comparable color and magnitude properties, as well as the probability that it displays typical BGC characteristics for every galaxy's redshift range in the Sloan Digital Sky Survey (SDSS). The algorithm separately assesses the possibility that a given galaxy lies within a dense cluster of e/S0 ridgeline galaxies possessing comparable @xmath35 and @xmath36 colors, while also holding typical brightest cluster galaxy characteristics. These probabilities are calculated at each redshift in a grid, and the maximum likelihood redshift is determined for every SDSS galaxy. The product of these likelihoods provides an optimal estimate for the cluster redshift of e/S0 galaxies. These potential centers are ranked by decreasing maximum likelihood, and the top-rated contender serves as the initial seed for the first identified cluster's brightest cluster galaxy. For E/S0 galaxies, the maximum likelihood redshift associated with each cluster provides a good estimation of the center redshift. This list of likely cluster center redshifts is assembled, and the potential center with the highest maximum likelihood is seeded as the first cluster's brightest cluster galaxies (BCG). Galaxies within a scaled radius of @xmath37, based on the radius within which the mean density is 200 times the critical density as determined from SDSS galaxy populations, and within @xmath38 in @xmath31 are removed from the list of potential centers to avoid overlapping. This process continues recursively for the next most likely BCG on the list, only selecting it if it has not been eliminated by a higher likelihood BCG. This process continues for all potential centers, labeling them as cluster BCGs or designating them as subsumed under higher likelihood centers. The radius inside which the mean density is 200 times the critical density, as determined from the galaxy populations of the Sloan Digital Sky Survey (SDSS), is determined through a specific process. Starting from the most likely Brightest Cluster Galaxy (BCG) candidate, potential centers are labeled as cluster BCGs or subsumed by higher likelihood centers. This is repeated for every candidate on the list until all potential centers have been labeled. The cluster defined via this method has a center, estimated redshift, and richness, as measured by the number of E/S0 ridge-line members that are brighter than 0.4 magnitude and located within the radius determined via aforementioned process. This catalog is public and contains a total of 13,823 clusters spanning approximately 7,500 square degrees of sky, with a median redshift of 0.13. The final MaxBCG cluster catalog features an array of recorded properties, such as photometric redshifts, richnesses, optical luminosities, and positions, pertaining to a total of 13,823 clusters selected from approximately 7500 square degrees of sky with redshifts ranging from 0.1 to 0.3. The median redshift for the clusters is 0.15. The center of each cluster is determined as the position of the brightest cluster galaxy (BCG) determined by the algorithm. The richness of the clusters varies between 10 and 188 in the public catalog, with less certain values below a richness of 43. For this study, we incorporate clusters with lower richness values, which increases the number of clusters to 17,355. The richness of the cluster catalog, spanning from 10 to 188, is analyzed in this study, which incorporates a slightly extended catalog with an additional 3532 clusters that have a lower richness according to the maxbcg selection function. This enables us to use consistent richness bins studied in previously published works on galaxy dynamics and gravitational lensing. The accuracy of the redshift estimates, produced during the cluster identification process, is demonstrated by comparing them to spectroscopic redshifts with an \u223c1% error rate. Completeness and purity of the catalog are also high as established in previous studies. According to recent findings, the redshift estimates for the clusters in this catalog generated during the cluster identification process have been demonstrated to be quite accurate, showing comparability to spectroscopic redshifts with @xmath11 %xcite. The completeness and purity of the catalog have also been extensively studied in @xcite ; both are quite high. The completeness estimate for clusters whose masses exceed @xmath45 is @xmath44, and purity is approximately @xmath44 for clusters with richnesses above @xmath46. This cluster population has been utilized to formulate constraints for cosmology, as discussed in @xcite. The relationship between the maxbcg richness, denoted as @xmath6 , and mass has also been investigated through both galaxy dynamics ( b07 ) and weak lensing ( s07 , j07 ). Further analysis on the galaxy population, mass-to-light ratios, and improved richness estimates for these clusters is currently underway. The relationship between the maximum BCG richness at xmath6 and mass has been studied through both galaxy dynamics (b07) and weak lensing (s07, j07). Notably, ongoing research encompasses further investigation of galaxy populations, the relationship between mass and luminosity ratios, and the development of refined richness estimations. The ROSAT All-Sky Survey, a six-month study primarily conducted during the years 1990-1991, surveyed the entire sky at soft X-rays (0.1-2.4 keV) with the ROSAT Position Sensitive Proportional Counter. The survey, which covered large sectors on the celestial sphere, acquired the highest exposure time (47) near the ecliptic poles. In the period between 1990 and 1991, a six-month campaign aimed at imaging the entire sky in soft X-ray wavelengths (0.1-2.4 keV) was undertaken using the ROSAT Position Sensitive Proportional Counter (PSPC). The survey scanned the sky in great circles, with the maximal net exposure time in regions close to the ecliptic poles. Specifically, the typical field that overlaps with the MaxBCG survey region but does not intersect with the Northern Ecliptic Pole has an effective exposure time of approximately @xmath48. The Point Spread Function (PSF) of these RASS scans is notably broad, with a Full-Width-Half-Maximum (FWHM) of approximately @xmath49, which is primarily caused by far-off-axis photons that occur due to the survey's strategy. An updated release by @xcite includes reprocessed photon maps and exposure maps of the full RASS survey region. In these RASS scans, the point spread function (PSF) is extensive, with a half-maximum width of @xmath49. This is due to the survey's strategy, which captures far-off axis photons. Reprocessed photon maps and exposure maps have been released by @xcite for the entire RASS survey region. These photon maps serve as the input for analysis, as described in this paper. RASS data has been utilized to compile several catalogs of pure X-ray objects. The ROSTAT Bright Source Catalog (BSC) contains 18,811 sources with a typical signal-to-noise ratio @xmath50. RASS data, the R\u00f6ntgen Satellite All-Sky Survey, has been utilized to create two catalogs consisting of x-ray selected sources. The ROSAT Bright Source Catalog (BSC) contains 18,811 objects with a median signal-to-noise ratio of 50 at a 15 arcsecond position resolution, which is superior to the point spread function full width at half maximum (FWHM) with 68% (90%) of the sources falling within it. Furthermore, a companion catalog, the ROSAT Faint Source Catalog (FSC), comprises 105,924 sources, predominantly unresolved, with a typical signal-to-noise ratio of 2.5. Beyond the Bachelor of Science, a companion catalog known as the Faint Source Catalog (FSC), containing 105,924 sources, has also been produced. These sources have a typical signal-to-noise ratio of 53. Many of the sources in the catalog are unresolved. Combining these catalogs provides a list of soft X-ray detected sources that can be compared to the MaxBCG clusters. The photon data from the ROSAT All-Sky Survey has also been utilized in the creation of X-ray flux-limited cluster catalogs, such as the Brightest Cluster Sample (BCS), which contains the 201 brightest clusters in the northern hemisphere. The Northern ROSAT All-Sky Galaxy Cluster Survey (NORAS) is an all-sky catalog of galaxy clusters that utilizes data from ROSAT, which is a X-ray telescope. The NORAS (Northern ROsat All-Sky Galaxy Cluster Survey) is a compilation of 378 extensively observed but confirmed X-ray sources that have been identified as clusters of galaxies. However, the catalog's completeness is limited to brighter flux levels at a state of flux at xmath55 and a luminosity at a median redshift of xmath41. The Northern ROsat All-Sky Galaxy Cluster Survey's resolution is primarily restricted by its broad Point Spread Function (PSF), which is around xmath56. A study was undertaken by @xcite to compare the clusters identified by NORAS against the MaxBCG clusters and found that approximately xmath58 of NORAS objects were detected by MaxBCG. The current catalog available, with a completion rate of '@xmath55' at their declared flux restriction '@xmath56', corresponds to a luminosity of '@xmath57' at the median redshift of '@xmath41'. Researchers from @xcite carried out an initial comparison between NORAS clusters and MAXBCG clusters and found that MAXBCG identifies '@xmath58' of NORAS objects, coinciding with completeness estimates from simulations  @xcite. The REFLEX: \"_ROSAT-ESO Flux-limited X-ray Galaxy Cluster Survey\" (reflex: * ? ? ? * ) in the southern sky, shares comparable characteristics with NORAS, featuring the same flux limit. The REFLEX: _ROSAT_-ESO Flux-Limited X-ray Galaxy Cluster Survey, commonly abbreviated as REFLEX, is analogous to the NORAS survey situated in the southern sky. Likewise, REFLEX shares the same flux limit. With 447 cataloged clusters, the REFLEX survey exhibits over 90% completion due to enhancements in RASS analysis. Although only a small fraction of REFLEX clusters align with the MAXBCG survey area, the extensive number of MAXBCG clusters (over 1,000) enables us to rectify the deficiency of prolonged exposure times (400 seconds) for individual clusters. In contrast to the standard RASS exposure time of 400 seconds per cluster, the sheer number of 7,986 MAXBCG clusters having a richness of 9 provides an opportunity to compensate for this constraint. The massive total RASS exposure time of 60,083 seconds for these objects enables us to accurately measure the mean X-ray emission for these clusters collectively. The comprehensive total exposure time for RASS is 60 seconds, which provides an opportunity to precisely determine the average x-ray emission of these clusters. A detailed explanation of the stacking approach is explained later in this section and further outlined here. Initially, the clusters are categorized into nine richness bins based on RASS data, employing bins used in previous analyses of mean velocity dispersions (B\u00f6hringer et al. 2007). Our initial step includes dividing the clusters into nine richness ( @xmath6 ) bins for comparison to other MAXBCG analyses, using the same richness bins presented in B07. The number of clusters in each richness bin is listed in Table [tab: ncluster]. The specific bin values are as follows: cccccc with @xmath61 = 240, @xmath62 = 225, @xmath63 = 201, @xmath64 = 172, @xmath65 = 163, @xmath66 = 148, @xmath67 = 137, @xmath68 = 121, and @xmath69 = 106.\n\nFor simplicity, we treat the brightest cluster galaxy (BCG) selected by the MAXBCG algorithm as the center of each cluster and stack our analysis on these centers since we do not have X-ray centers for individual clusters. In a stacked analysis of galaxy clusters selected by the MaxBCG algorithm, photons from each source and background are scaled and weighted to the median redshift of the clusters in the catalog, based on the center of the cluster, which is determined using MaxBCG. This method was shown to provide good alignment with X-ray selected cluster centers in a previous study called @xcite, and other optically-selected catalogs have also shown similar results. The potential biases introduced by this method of determining cluster centers are discussed in section [sec : centering]. Possible biases associated with assuming a central cluster redshift are addressed in Section [ sec : centering ]. Every source and background photon in the stacked analysis is scaled and weighted based on their median redshift from the MAXBCG cluster catalog. This methodology is discussed in Section [ sec : images ]. By constructing stacked images in this way, along with radial profiles and spectra in Section [ sec : profiles  ] and Section [ sec : spectra  ], we obtain RASS photon data and merged exposure maps from the High Energy Astrophysics Science Archive Research Center (HEASARC) archives, which are available in a set of arbitrarily constructed 0.5\u00b0 fields with significant overlap. Nevertheless, since a significant fraction ( @xmath74 ) of the MAXBCG clusters fall across a field boundary due to the large background annulus we use extending to @xmath73 degrees from the central background cluster galaxies (BCGs), this poses a potential source of bias that needs to be considered. To obtain our rass photon data and merged exposure maps, we accessed the high energy astrophysics science archive research center (HEASARC) archives. Rass data and exposure maps are distributed in 72-degree angular fields with significant overlap. Even though a large background annulus extending to 73 arc minutes is used, a significant fraction of MAXBCG clusters fall across a field boundary. To address this, we have created a tool that accesses all RASS photons within a specified aperture, extracting them from multiple fields if necessary. The tool also prevents the counting of any photon that appears more than once in overlap regions. After this step, merged exposure maps (mex file) are utilized to calculate the total effective exposure time at the coordinates of each detected photon. However, before proceeding with stacking, we eliminate a subset of MAXBCG clusters that may introduce unrealistic x-ray measurements under certain conditions. Following photon retrieval, we generate merged exposure maps using mex files to calculate the effective exposure time at the position of each detected photon (@xmath75). Prior to stacking, we exclude a subset of MaxBCG clusters from the list that may skew our x-ray measurements due to unphysical reasons. No cut on galactic absorption is necessary, as the SDSS observations are confined to high galactic latitude, resulting in a very low equivalent galactic hydrogen column density of @xmath76 @xcite for the MaxBCG clusters. However, the maximum column density is @xmath77. Although the typical RASS exposure time is @xmath78, there are some fields that have significantly less exposure time. The typical equivalent galactic hydrogen column density for MAXBCG clusters is exceedingly low, with a maximum of ~1021 cm^-2. While the standard RASS exposure time is 3600 seconds, some fields have less time dedicated to their observation. This creates a challenge for source and background estimation but does not drastically enhance the signal. Consequently, we exclude every cluster with less than 200 seconds of mean exposure time when conducting our analysis. This eliminates only 4% of all clusters but less than 1% of the total net exposure time. To ensure the accuracy of our stacking measurements, we discard clusters with a mean exposure time of less than 200 seconds, eliminating 4% of the total number of clusters. This, however, accounts for less than 1% of the net exposure time. While rejecting regions contaminated by extremely bright foreground sources is integral to our analysis, it poses a significant challenge at higher redshifts, where RASS images of MaxBCG clusters are often unresolved. To address this obstacle, we employ a method that does not rely solely on extent information but instead proceeds as follows. In order to avoid the inclusion of contaminated regions in our analysis, we reject those areas impacted by bright point sources. Specifically, we refer to the RASS (Roentgen Astrophysics and Spectroscopy) images of MaxBCG (Maximum Redshift Appendix to the X-ray Cluster Catalogue: A Comparison with Optical Catalogs for z>0.1) cluster surveys, which often become blurred, particularly at higher redshifts. Given this challenge of removing point sources using extent criteria, we adopt a different method. To begin, we first identify those sources in the survey area listed in the ROSAT Bright Source Catalog (BSC) that exceed a count rate higher than the expected emission from any cluster beyond redshift 0.1. This translates to a soft-band (0.1-0.5 keV) or hard-band (0.5-2.1 keV) count rate of 79 counts per second. There are 179 BSC sources located in the MAXBCG survey area that meet this threshold. Out of these 179 sources, we screen them for further filtering during our data analysis. In the MaxBCG survey, 179 background sources in the soft (0.1-0.5 keV) and hard (0.5-2.1 keV) bands of ROSAT have count rates above the threshold of @xmath79. Out of these 179 sources, only 13 were found to be clusters as identified in the Brightest Cluster Sample (BCS). Among these 13 clusters, only one is associated with a MaxBCG cluster, which is Abell 2142, the richest and nearest cluster in the catalog. Among the 13 clusters under investigation, only one, Abell 2142, which is the richest and closest cluster identified in the catalog at a redshift of approximately 0.05, is affiliated with a MaxBCG cluster. The other bright clusters are found at redshifts of around 0.8. Furthermore, visual examination has corroborated that Abell 2142 is the sole MaxBCG cluster that is concurrently linked to one of the extremely intense sources. For the stacking analysis, we rejected all MaxBCG clusters (except for Abell 2142) that fall within a radius of @xmath81 around any of those bright sources due to a potential contamination from non-cluster photons. Additional in-depth examination has solidified the fact that Abell 2142 is the sole MAXBCG cluster associated with one of the blindingly bright sources encountered. For the analysis by stacking, we reject all MAXBCG clusters apart from Abell 2142, within 1 arcminute of any of these radiant sources, to avoid contamination from stray radiation. This rejection criterion affects less than 0.6% of the MAXBCG catalog. The Virgo Cluster is a close, X-ray luminous collection, located nearby, that casts intricate shadows over a significant region of sky, thereby making it challenging to gauge the X-ray emission from background clusters there. The Virgo Cluster, a nearby cluster with bright X-ray emission, also acts as a significant foreground source. This X-ray bright cluster, which includes galaxies M86 and M87, dominates the emission over a substantial region of the sky and thus makes it challenging to estimate the background clusters' X-ray emission. Therefore, we reject all clusters that fall within 82 degrees of M86 or M87 to minimize this effect, which eliminates only 0.1% of the MAXBCG clusters. The number of clusters in each richness bin, as well as the mean richness for each bin, is provided in Table [tab:ncluster]. To combine the X-ray measurements of the clusters, RASS photons from each cluster in our sample are employed. In this small effect scenario, removing merely 0.1% of the maxBCG clusters, Table [tab : ncluster] displays the number of clusters present in each richness bin before and after undergoing the rejections mentioned in this section, as well as the mean richness in each bin. To stack X-ray measurements, RASS photons from each cluster are appropriately scaled and weighted. For each photon, the projected physical distance to the brightest cluster galaxy (BCG) is calculated, and this scaled distance is used in image construction and radial profile calculations. Each source and background photon is weighted by a factor of @xmath83, where \u039bCDM's Hubble constant @xmath84 is the luminosity distance to the cluster, and @xmath85 is the luminosity distance to the median cluster redshift @xmath71. The weighted count rate in a given bin for a cluster at redshift z is calculated based on a procedure similar to @xcite. Each source and background photon is scaled by a distance that is proportional to their respective luminosity distances, with the factor being @xmath83 , where @xmath84 and @xmath85 represent the luminosity distances to the cluster and median cluster redshift, respectively. For background estimation, we use a fixed annulus with inner ( outer ) radii of @xmath86 and @xmath87, which corresponds to a physical distance of @xmath88 at our minimum redshift of @xmath89 and a distance of @xmath90 at our maximum redshift of @xmath91. We confirm that our results are not sensitive to the choice of background annulus. The count rate in a given bin for a cluster is calculated by multiplying the weight factor with the count rate in that bin and summing it up over all the photons detected within that bin's area. To confirm our stacking procedure's validity, we performed a simple test. The background annulus in our analysis spans a physical distance of @xmath88 at our minimum redshift of @xmath89 and @xmath90 at our maximum redshift of @xmath91. We confirmed that our results are independent of the exact selection of this annulus. When calculating the weighted count rate for a specific bin, we follow a similar procedure to @xcite. In each bin, we sum the exposure time @xmath93 for each detected photon @xmath75, which is also referred to as photon-weighted exposure time. As a simple test, we also extract RASS photons from a set of random positions covering the same area covered by the MaxBCG catalog. Each random position is associated with a redshift drawn from the catalog within the specific @xmath69 bin. We treat these random points identical to the real cluster positions when analyzing them. Each randomly chosen position in a given redshift bin, drawn from the catalog, is associated with a real cluster's redshift value of @xmath69. These random points are analyzed identically to the actual cluster positions. The resulting images after background subtraction are shown in Figure [fig:imageplots], with a projected radius of @xmath94. The photons used for these images are from the _ROSAT_ hard band (channels 51-201, 0.5-2.0 keV), which provides the highest signal-to-noise ratio for cluster emission. In each image, projected radii are @xmath94 . These images contain photons from channel 51 to channel 201 of the hard band (0.5 to 2.0 keV) in the ROSAT data, which has the highest signal-to-noise ratio for cluster emission. After background subtraction, the counts were put in @xmath95 bins. Each image has been scaled using histogram equalization to show similar background noise levels. The two lowest bins contain clusters two orders of magnitude fewer in number than the most abundant bin. Contours are drawn above background level at @xmath96 and @xmath97. Highly significant X-ray emission is observed in each stack of clusters. In contrast to a stack of 7,566 random points, highly significant X-ray emission is evident in each cluster stack. Specifically, the contours at xmath96 and xmath97 are drawn above background level in these stacks. Notably, there is no significant excess in the stack of random points, and no indication of centers for the emission. When considering the average count rate in the xmath69 bin, which is only @xmath98, the resulting flux equates to approximately @xmath99 counts. In total, there are an excess of @xmath100 sources within the @xmath101 clusters, with an average of just 1 source count per cluster. It is significant to note that the average count rate in the @xmath69 bin is only @xmath98, which corresponds to a flux of @xmath99. There are a total of @xmath100 counts in excess of background from @xmath101 clusters, representing an average count per cluster of only one. This demonstrates the robustness of the stacking method in uncovering very low count rates, making it possible to measure low @xmath5 cluster emission for a moderate redshift of @xmath102. Figure [Fig: Profiles] presents the background-subtracted surface brightness profiles for the stacked x-ray images in each richness bin, where the radial profiles have been summed in @xmath103 bins inside and @xmath104 bins outside @xmath105. For reference, [Fig: Profiles] is included.\n*Note: @xmath refers to Unicode characters for mathematical expressions. Figure [Fig: Profiles] depicts the background-subtracted surface-brightness profiles for the stacked X-ray images in each richness bin. The radial profiles have been summed in @xmath103 bins within (@xmath104 bins outside) the @xmath105 radius for comparison. The dotted lines represent the baseline level of @xmath106. Significant cluster emission is detected in all richness bins, extending beyond @xmath105 for the richest groups. Significant cluster emission is detected in each richness bin, spreading out to beyond @xmath 105 arcminutes for the richest clusters. The x-ray surface brightness profiles for every bin exhibit striking resemblance, except for the normalization increasing dramatically with richness and a gradual decrease in signal-to-noise with rising richness. These surface brightness profiles are fitted out to @xmath 107 arcminutes with the standard @xmath 108 model described by @xmath 109. The best-fit parameters are listed in Table [tab:betafit], and are shown as dashed lines in Figure [fig:profiles], indicating a close match to the observed profiles. We conducted a surface brightness analysis up to a radius of 107 arcseconds using a standard \u03b2-model fit for all clusters (see Table [tab:betafits] for the best-fit parameters). The \u03b2-model results in a good fit for all cases, and the overplotted fits are shown in Figure [fig:profiles]. Surprisingly large core radii, ranging from 126 to 127 arcseconds, were obtained for these fits. This is significantly larger than what is typically observed in X-ray clusters (* et al. ). The calculated values for the core radii using the specified @xmath61, @xmath62, @xmath63, @xmath64, @xmath65, @xmath66, @xmath67, and @xmath68 model fits for a cluster system are surprisingly large, ranging from @xmath126 to @xmath127. This significantly exceeds the typical core radii observed for X-ray clusters, whose values typically fall within the range of * ? ? ? and * ? ? ? (Hudson et al., 2010). Interestingly, a weak trend in the @xmath108 parameter is observed with the richness of the clusters, with richer clusters tending to have slightly larger values of @xmath108 (Burenin et al., 2013). The parameters used in these model fits for @xmath108 are generally characteristic of X-ray clusters, for instance * ? ? ? * ; * ? ? ? (Sanderson et al., 2003). The core radii themselves are calculated using the given @xmath110, @xmath112, @xmath113, @xmath114, @xmath115, @xmath116, @xmath117, @xmath118, and @xmath119, @xmath120, @xmath121, and @xmath122 values. The specific @xmath108 model fits used are @xmath61, @xmath62, @xmath63, @xmath64, @xmath65, @xmath66, @xmath67, and @xmath68. A slight correlation between richness and @xmath108 in cluster measurements has been observed, consistent with previous findings linking @xmath108 and cluster mass. Similar results have been reported in DKM07, where the contribution of richness to @xmath108 was not deemed significant. However, the outcomes have proven inconclusive due to two factors: the broad point spread function for RASS objects and the offset distribution between the brightest cluster galaxy (BCG) and the X-ray cluster emission. A test was conducted on bright ROSAT point sources to identify the influence of PSF on stacked profiles in this regard. According to DKM07's measurements, the @xmath108-model parameters are comparable to those obtained. However, unlike DKM07's results, a strong correlation between @xmath108 and richness was not observed. This can stem from two effects: the broad point spread function (PSF) in RASS objects and the offset distribution between the brightest cluster galaxy (BCG) and the X-ray cluster emission. To investigate the impact of RASS's PSF, this study tests the analysis on bright ROSAT point sources selected from the WGA catalog. The study uses 130 moderately bright (131) point sources that are both nearby galactic sources and distant AGN and quasars at moderate to high redshifts. These sources were specifically chosen as they also fall within the Sloan Digital Sky Survey (SDSS) DR5 mask and are detectable in RASS. In our analysis, we select moderately bright sources, notably at a brightness of @xmath130, from the SDSS DR5 mask, covering sources that are nearby galactic sources and AGN and quasars at moderate and high redshifts. These sources are also detected in RASS. We randomly select redshifts for each point source from the MAXBCG cluster catalog's redshift distribution. It is important to note that a nearby, unresolved point source can be mistaken for a distant, unresolved point source, making it necessary to follow a similar stacking process as those described for clusters previously. By selecting point sources based on their brightness and redshift, we can perform a rigorous analysis to better understand the properties of point sources with various redshifts and brightnesses. In analyzing nearby and distant unresolved point sources, it becomes challenging to differentiate between them. Therefore, in order to compare the characteristics of point sources, they are stacked precisely using the same method described for clusters. Figure [Fig. ptsrc] reveals the radial profile of the accumulated point sources, which essentially represents the point spread function (PSF) of the RASS survey with the scaling factor computed for the MAXBCG cluster redshifts. While this profile appears more sharply peaked than that of the accumulated clusters, the broad X-ray PSF spreads significant emission to distances of 132 (133 at 134) arcseconds from the center. This smearing effect brings about a minimum constraint to the core radius and may introduce a potential source of error in calculating the corresponding parameter. The photon-source function (PSF) smearing in our study results in an effective minimum on the core radius for the @xmath108-model. Consequently, the smearing effect creates a possible bias in determining the @xmath108 parameter. Notably, out of the selected Wide-Field Grazing Incidence Wide-Bridge X-Ray Telescope (WGA) point sources, 135 of them are within 136 arcseconds of the centers of MaxBCG clusters. Eliminating the neighboring point sources that are potential extended X-ray sources does not significantly alter this result. Furthermore, the stacked and projected surface brightness profile can be influenced by the choices of cluster centers. Removing the immediate surrounding point sources from the evaluation does not significantly impact the result. The stacked and projected surface brightness profile may also be influenced by our choice of cluster centers. In some cases, the chosen center through the MAXBCG algorithm may be offset from the X-ray emission. Dynamically active clusters and those in the process of merging can have noticeable offsets between the X-ray centroid and the brightest cluster galaxy (BCG). Additionally, the accuracy of the MAXBCG algorithm in selecting the center is a concern and is addressed in a later section. In dynamically active clusters and merging systems, there is a noticeable difference between the location of the X-ray centroid and the brightest cluster galaxy (BCG). Additionally, a potential error in the center chosen by the maxBCG algorithm is addressed in Section [sec: centering]. Furthermore, additional X-ray point sources, such as active galactic nuclei (AGN), associated with the cluster may also affect the offset distribution. To account for these effects, we model the optical-X-ray offset distribution by matching the maxBCG catalog to known X-ray sources. The resultant effects lead to an accurate radial profile that may be biased due to the optical/X-ray offset distribution. To address this, we model the offset distribution by matching the MAXBCG catalog to known X-ray sources. Specifically, we utilize the fact that many MAXBCG clusters are in correspondence with individual detections in the ROSAT Bright Source Catalog and the Faint Source Catalog. However, these X-ray sources have not been previously recognized as associated with clusters due to their insignificant extension in the RASS. By matching the MAXBCG clusters with the BSC and FSC, and allowing multiple X-ray sources to match to each cluster, we can model the offset distribution and avoid potential discrepancies. The majority of BSC and FSC sources have not been previously attributed to clusters primarily due to their low luminosity or vast distance, making them appear as unextended points in RASS observations. Here, we match the MACSJ0744-1223 clusters to both the BSC and FSC catalogs, allowing for possible overlapping X-ray sources. This approach enables us to identify all X-ray sources linked to each cluster and any potential false associations. We carry out an equivalent analysis for randomly selected clusterless areas from the same region as the MAXBCG survey. The projected distance between the clusters and the BSC and FSC catalogs is illustrated in Figure [Fig: ptmatchhist]. In a comparable manner, we also extract the same number of random locations from the MAXBCG survey region and match them to the BSC and FSC catalogs. Figure [ fig: ptmatchhist ] presents the projected offset distribution from the cluster centers identified by MAXBCG to the BSC and FSC catalogs. The solid line illustrates a histogram of the offsets with respect to the cluster centers in 103 bins, while the dashed line represents the histogram of random offsets. The dotted line in the same figure shows the residual distribution obtained by subtracting the random matches from the cluster matches. This subtracted histogram exhibits a significant excess of X-ray sources closely associated with the optical cluster centers. The dotted line exhibits the residual distribution following the subtraction of random matches from the cluster matches. The resulting subtracted histogram depicted in Figure [fig: ptmatchhist] shows a significant excess of X-ray sources closely associated with the optical cluster centers. A tight core is visible, with the brightest cluster galaxy (BCG) located within @xmath138 of an X-ray source, along with a long tail out to @xmath139. The observed excess of X-ray sources at larger radii suggests a connection to the MAXBCG clusters, which include merging clusters, poorly identified cluster centers, and associated point sources such as AGNs. \n\nNote: While the original content used number signs to denote variables, I preferred to use LaTeX math mode markup for improved ease of readability and clarity. The X-ray source excess beyond a specific radius can likely be linked to the MaxBCG clusters. The sources in question consist of merging clusters, clusters with poorly defined centers, and associated AGN. The subtracted histogram serves as a primary, empirical radial distribution of X-ray sources closely connected to the centers of MaxBCG clusters. To account for an additional effect on the radial profile, every WGA point source detailed in Section [ sec : rasspsf ] is randomly altered, ensuring that the resulting radial offset distribution is identical to the empirical one. We now estimate the additional effect on radial profiles due to the radial offset distribution of x-ray sources associated with the maxBCG clusters. From the point sources presented in Section [sec: rasspsf], we randomly alter each position to match the empirical radial distribution of offsets between the brightest x-ray emission and BCGs of @xmath140 maxBCG clusters. We assume that this measurement is a good proxy for the offset distribution of all maxBCG clusters. Offset point sources resulting from this alteration are then run through a stacking analysis and Figure [fig: ptsrc] displays the resulting radial profile (circles). The offset point sources undergo stacking analysis, yielding a radial profile shown in Fig. [fig:ptsrc]. The stacked points demonstrate reasonable agreement with a @xmath108 model up to a radius of @xmath141, but within this range, the fit is poor. At large radii, the radial profile is well-fitted by @xmath142, indicating that it becomes challenging to differentiate between true _ extended _ cluster emission and point-source contribution at these distances. The radial profile obtained using @xmath142 is in accordance with the data. At larger radii, it becomes challenging to distinguish between extended cluster emission and X-ray point sources convolved with the observed MILCA X-ray source offset distribution. This casts doubts on the usefulness of the @xmath108 model parameterization for this particular exercise. The utility of using the @xmath108 model parameterization in this exercise is questionable. This is because the stacked X-ray profile, which is created by convoluting the ROSAT PSF, the centering distribution, and the true extended X-ray emission, cannot practically separate the contributions from these three components. This means that the @xmath108 parameters obtained through stacking exercises, particularly when most individual X-ray clusters are not detected, should be used with caution. Further examination of the stacked ROSTAT data yields complexities during analysis using XSPEC version 11.3.2. These complications are highlighted in DKM07, due to the operational limitation of segregating the contributions from the three components. This stacking exercise involves the analysis of the spectra for individual x-ray clusters, which in most cases are not detected, resulting in caution when using the @xmath108 parameters that result from this process. Therefore, in this instance, x-ray luminosities are calculated by performing spectral fitting of the photons in each stack. The inherent intricacies in analyzing stacked ROSTAT data are discussed in depth in DKM07. In the interpretation of X-ray spectra obtained using XSPEC version 11.3.2, a variety of challenges arise when conducting spectral analysis on stacked ROSAT data. DKM07 addresses some of these complexities. As all RASS observations combine exposure times over the entire ROsat field of view, we use the spectral response file PSPCC_GAIN1_256.RMF from HEASARC, which is designed for off-axis observations. The RASS (R\u00f6ntgensatellit zur Kartierung des Erebo-Schaumes) observations integrate exposure time across its Field-of-View. As the aim is to correct for vignetting, we rely on the HEASARC spectral response file pspcc_gain1_256.rmf, suitable for off-axis observations. In combination with the ARF (Ancillary Response File), we obtain a vignetting-corrected spectral response for the off-axis photons which make up our spectra. Whilst the merged exposure maps for RASS provide exposure times already corrected for vignetting using the mean spectrum of the X-ray background in the PSPC band, in practice, the use of the standard RMF+ARF combination leads to spectra that are already corrected for vignetting. Through merging exposure maps for RASS, the extracted exposure times are already corrected for vignetting using the mean spectrum of the X-ray background in the PSPC band. This method of correction means that, with fits using the standard RMF+ARF combination, any spectra obtained are corrected for vignetting twice. Firstly, using the exposure times from the merged exposure map, and secondly in the calculation of the ARF file. To approximate the extra vignetting correction, we compared the on-axis response file RSP from HEASARC with the combination of the RMF+ARF to calculate the correction factor as a function of energy. The correction factor is weakly dependent on the spectrum of the observed source, and by simulating cluster spectra of various temperatures, we calculated the average correction factor for cluster flux in the ROSTAT hard band (0.5 to 2.0 keV) which is dominated by cluster flux. In terms of energy correction, this function is significant and slightly influenced by the spectrum of the observed source. By simulating cluster spectra with different temperatures and averaging the typical correction factor in the ROSAT hard band (0.5-2.0 keV), which primarily captures the cluster flux, we obtain a value around @xmath143. Utilizing this correction factor results in excellent agreement between our spectral analysis and the conversion tables from @xcite regarding count rates, rates to flux. Nonetheless, incorporating this correction factor in analyzing all stacked spectra' flux and luminosity values adds an additional systematic error of @xmath144. For this study, we employ the RMF file previously detailed in combination with an ARF output generated using the FTOOL PCARF. In our analysis, we systematically add an estimation of 144% to all flux and luminosity values obtained from stacked spectra due to a correction factor. For this purpose, we utilize the response matrices (RMFs) from a previous study (DKM07) and combine them with an array response file (ARF) that covers the Rosat field of view. Each stacked cluster samples photons from the entire field of view, according to DKM07. To ensure valid statistical results, we group the spectral files with a minimum of 50 counts per bin after subtracting the background. XSPEC's spectral fits are limited to the 0.1 - 2.1 keV range. In order to obtain valid results, spectral files undergo background subtraction and each bin receives a minimum of 50 counts. Statistical analyses are performed using @xmath145, and fits using XSPEC are restricted to the 0.1-2.1 keV range. The uncertainties in spectral fit parameters are 90% confidence errors, obtained by allowing all fit parameters to vary simultaneously. Cluster spectra are obtained by summing all weighted cluster photons in both fixed physical apertures and scaled apertures according to the optically determined size @xmath37, which is chosen as a reasonable fiducial value because it provides good signal-to-noise ratio and does not appear to change significantly with richness. We acquire cluster spectra by summarizing all weighted photons within both fixed physical apertures and scaled apertures that are based on the optically determined maximum distance @xmath37 (see section [ sec : maxbcg ]). For a reasonable fiducial value, we chose a fixed aperture of @xmath146, which provides good signal-to-noise and insignificant radial profile changes with richness. Each bin's @xmath37 value was taken as the median value for all clusters in the corresponding richness bin, and these values are enumerated in table [ tab : lxr200 ] (see previous paragraph). The background spectra are calculated with the same set of weights as the source spectra, using the annuli defined in section [ sec : extraction ]. According to our previously mentioned procedure, each photon is weighted according to the median redshift @xmath147. The background spectra are accumulated with the same weight as the source spectra, using the defined annuli in Section [ sec : extraction ], as mentioned previously. Each photon is assigned a weight based on the median redshift of @xmath147. However, due to the predominant spectral response of the ROSETTA instrument in the detection channel, correcting individual photons is not feasible. Therefore, the stacking procedure will result in a smearing out of the incident cluster spectra.\n\nSimulations using XSPEC were conducted to determine the impact of this approach on the best-fit spectral values. These simulations revealed that there is minimal effect on the results for our redshift range of @xmath149, which is not particularly broad. The stacking procedure tends to 'smear out' the incident cluster spectra, but in our redshift range of z = 0.89 to z = 1.52, it does not create a statistically significant effect on our best-fit spectral values. Simulations using XSPEC confirm that the impact of stacking is negligible. Our analysis does not incorporate any corrections for possible redshift evolution in X-ray luminosity, which we will examine further in section [sec: redshift]. The spectra are fitted with an absorbed thermal plasma model. Specifically, the following component measurements demonstrate this:\n\n@math{\n     ccccccc        \\ \\ \\          z         \\ \\ \\ \\ \\ N_H       \\ \\ \\     T \\ \\ \\       \\ \\ \\   l_{[2-10]}\\ \\ \\    \\ \\ \\   norm\\ \\ \\    \\ \\ \\     flux\\\\\n\\hline\n   Cluster A       &  @math{0.89}   &  @math{6.1} x 10^23 cm^-2  &   @math{92.85} &   @math{1.73}   &  @math{47.2}   &   @math{76.0}   &   @math{58.22}   &  @math{1.47}   &  @math{44.67}   &   @math{0.99}   &   @math{62.9}   &  @math{95.0}   &   @math{29.57}   &  @math{0.94}   &   @math{77.3}   &  @math{88.0}   &   @math{9.80}   &  @math{0.73}   &  @math{127.8}   &  @math{112.0}   &   @math{0.22 ph~cm~s}^{-1}  @math{}\\\\\n   Cluster B       &  @math{0.89}   &  @math{6.1} x 10^23 cm^-2  &   @math{92.85} &   @math{1. This section presents the analyzed spectra, which are fitted using an absorbed thermal plasma model. The metallicity is fixed at 0.3 solar (as commonly used in X-ray spectroscopy). The redshift is set to the median scaled redshift of 0.2296. The luminosities are calculated in the 0.1-2.4 keV rest-frame band at the median redshift. The following data is presented: ccccccc @xmath61 & 92.85 & 1.73 & @xmath176 & @xmath177 & @xmath178 & 40.5/61 + @xmath62 & 58.22 & 1.47 & @xmath179 & @xmath180 & @xmath181 & 40.1/59 + @xmath63 & 44.67 & 1.32 & @xmath182 & @xmath183 & @xmath184 & 85.6/61 + @xmath64 & 35.74 & 1.20 & @xmath185 & @xmath186 & @xmath187 & 92.0/81 + @xmath65 & 28.57 & 1.10 & @xmath188 & @xmath189 & @xmath190 & 78.1/84 + @xmath66 & 22.70 & 0.99 & @xmath191 & @xmath192 & @xmath193 & 86.0/94 + @xmath67 & 18.91 & 0.94 & @xmath194 & @xmath195 & @xmath196 & 73.6/76 + @xmath68 & 13.88 & 0.82 & @xmath197 & @xmath171 & @xmath198 & 182.3/134 + @xmath69 & 9.80 & 0.73 & @x In order to accurately depict the variations in cluster luminosity within each richness bin, we employ bootstrap resampling with 2000 trials in each bin. For each resampling, we sample the same number of clusters with replacement from the given richness bin. We opt to calculate the 0.1-2.4 keV luminosity because extrapolating spectral temperatures for a bolometric luminosity leads to considerable extrapolations and variability in determined values. The best-fit spectral parameters for the scaled apertures, detailed in Table [tab: lxr200], and fixed apertures, presented in Table [tab: lx750], are shown accordingly. To accurately account for the variations in luminosity for each richness bin, we use bootstrap resampling to estimate the luminosity errors. For this purpose, we run 2000 trials by sampling the same number of clusters in each bin with replacement. To save time with processing, we do not recreate the entire stacking procedure for each resampling; instead, we use the individual cluster count rates, scaled to the median redshift, as a reliable proxy for luminosity in the 0.1-2.4 keV band, which we confirmed to have a strong correlation (see section [ sec : scatter ] for further details). The computed 68% confidence interval (approximately 1\u03c3) obtained from the bootstrap is added in quadrature with the previously obtained errors obtained from the spectral fits. The results are presented in tables [ tab : lxr200 ] and [ tab : lx750 ]. The average scaled count rate stands as an effective proxy for luminosity in the 0.1 - 2.4 keV band, as presented in section [ sec : scatter ]. The 68% ( @xmath106 ) confidence interval generated through bootstrap analysis is added in quadrature with @xmath5 errors obtained from the spectral fits. Only the richest ( @xmath61 ) bin predominantly features bootstrap error calculation due to the significant range in richness and @xmath5 in the selected bin. The mean X-ray luminosity with respect to temperature, depicted in Figure [ fig : ltplot ], illustrates the observed relationship for our stacked clusters ( within a scaled @xmath37 aperture ) when compared to the @xcite @xmath5@xmath204 ( 0.1 - 2.4 keV ) relation. It indicates that the cluster's temperature increases as the luminosity grows, but notably, our observed @xmath0@xmath203 relationship is steeper than that presented by @xcite . Figure [fig:ltplot] displays the mean X-ray luminosity-temperature relation for our stacked clusters within a scaled aperture of 37, when compared to the 0.1-2.4 keV relation of [xcite]. The relationship shows an increase in cluster temperature with luminosity. However, our observed luminosity-temperature relation is noticeably steeper than that of [xcite]. The stacked X-ray temperatures appear to underestimate the expected temperature, particularly at higher luminosities. This discrepancy emphasizes the challenges of measuring cluster temperatures with ROSAT, including the difficulty of measuring stacked cluster temperatures, given its sensitivity only to soft X-rays. However, this differential in measurement is not confined to the use of the ROSAT instrument alone as measuring stacked cluster temperatures also poses significant challenges. Specifically, the sensitivity of ROSAT is restricted to soft X-rays, which means that when the break in the bremsstrahlung spectrum at 2 keV, determined by cluster temperature, is exceeded, it becomes challenging to accurately estimate the X-ray temperature. This is exemplified in the case of Abell 1689, where the temperatures detected by ROSAT are consistently lower than expected. The discrepancy highlights the difficulty of measuring hot cluster temperatures using either ROSAT or stacked cluster measurements. The _XMM-Newton_ observations of the bright cluster Abell 1689 reveal a nearly isothermal profile, with a temperature of @xmath207 consistent with previous _ASCA_ and _Chandra_ measurements. However, the _ROSAT_/_PSPC_ result yields a lower temperature estimate of @xmath208. This lower temperature may be attributed to the stacking of multiple non-isothermal clusters with varying temperatures, which make an isothermal Raymond-Smith spectrum no longer applicable in such cases. XMM/Newton and other observations with a temperature of 10.7 keV, consistent with those reported by ASCA and Chandra, yield a best-fit ROSAT/PSPC temperature of 9.5 keV (for instance, as demonstrated by the \"second\" paper, *? ?*). However, when stacking multiple non-isothermal clusters with different temperatures, using an isothermal Raymond-Smith spectrum may not be appropriate. Even for individual clusters, the spectroscopically-weighted temperature may differ significantly from the emission-weighted temperature. Therefore, interpreting the average best-fit spectral temperature from a stack of hundreds of clusters is not trivial. Noteworthy, in the 0.1-2.4 keV band, the ROSAT luminosity is virtually insensitive to temperature, making it a valuable tool for measuring the mean cosmic X-ray background radiation. When combined, the interpretation of the average best-fit spectral temperature from a stack of hundreds of clusters is not a straightforward task. However, for our goal of measuring the mean luminosity in the 0.1-2.4 keV band, it is of utmost significance that it is largely unaffected by the cluster temperature. When we fix the spectral temperature to the value predicted by the @xcite @xmath5@xmath204 relation, changes in @xmath0 only occur within \u22120.008 in each bin, without any systemic bias associated with richness. Another method to achieve our aim involves following the prescriptions outlined by @xcite to convert the mean 0.5-2.0 keV RASS count rate to flux and luminosity. Nevertheless, this approach requires iterative computations that assume the @xcite @xmath5@xmath204 correlation and convert count rate to flux and luminosity. To convert the mean 0.5-2.0 keV RASS count rate to flux and luminosity, we can utilize the prescription proposed by @xcite, which assumes the relationship between flux and temperature based on the @xmath5 and @xmath204 models. Through iteratively calculating the flux and temperature, @xmath0 values are obtained that are consistent with those obtained from spectral fitting, with a difference of up to @xmath210. These findings suggest that calculating 0.1-2.4 keV flux using the _ROSAT_ which relies predominantly on photon counting, is more reliable than extrapolating @xmath204 to @xmath202. The comparison between computing 0.1 - 2.4 keV via photon counting with the _ROSAT_ data and extrapolating @xmath204 to @xmath202 suggests that the former method is more trustworthy. While a small @xmath148-correction of  @xmath144 is necessary to convert from the observer frame to the rest frame luminosity, this correction is not highly dependent on the spectral temperature as the clusters' moderate redshift minimizes its impact. The luminosity in the 0.1 - 2.4 keV range is calculated using stacking and incorporating the methods described in the previous section, as shown in Figure [Fig: LXN200plot] for both fixed and scaled cluster apertures. The clusters' x-ray luminosity in the 0.1-2.4 keV range is measured by stacking and following the procedure described in the prior section. Figure [fig: lxn200plot] reveals the strong correlation between the mean @math0@math1 correlation, using both the fixed @math146 aperture and the scaled @math37 aperture. In low richness bins of @math211, the fixed and scaled apertures are almost equivalent in size. However, in high richness bins, the scaled aperture @math37 is much bigger than the fixed aperture @math146, resulting in a slightly larger @math0 for scaled apertures, causing the @math0@math1 relation to be steeper when using scaled apertures. The aperture @xmath37's scaled size is significantly greater than that of @xmath146, leading to a larger X-ray luminosity for scaled apertures. The power law fits used for this analysis pivot around @xmath214 to minimize errors in slope and normalization, with the best-fit mean relationships being:\n\n@xmath212\n@xmath213\n\nThe DKM07 study found that the mean X-ray luminosity of 2MASS clusters scales with optical richness. To compare this to our relation, first, the DKM07 bolometric luminosities from Table 2 in their paper were converted to 0.1-2.4 keV luminosities using Table 5 of Xu et al. (2004). However, because of the different luminosity and richness definitions employed, this comparison is challenging and is limited to a first-order conversion. Additionally, this may result in some differences between the two relations. Dkm07 established that the mean X-ray luminosity of 2MASS clusters correlates strongly with optical richness, using a table presented in @xcite. However, a direct comparison between Dkm07's relation and our findings is challenging because of the differences in luminosity and richness definitions used. Specifically, we converted Dkm07's bolometric luminosities, listed in their Table 2, to 0.1-2.4 keV luminosities using Table 5 in @xcite. When we reformulated Dkm07's @xmath215@xmath17 relation for the current study, we found that the power-law fit consistently corresponded to our own @xmath216@xmath17 relation, despite some deviation in the lower richness bins. However, we were unable to make a direct comparison of the results for individual clusters because the 2MASS catalog was not yet publicly available. The majority of the clusters in the 2MASS catalog reside at redshifts below 0.217, whereas the MAXBCG catalog covers more distant clusters. At present, further analyses will be required to better understand these discrepancies. In contrast to comparing individual clusters using the 2mass catalog, which is yet to be made public, our analysis provides insights into the @xmath0richness relation between cluster abundances and their environment. At present, there are some discrepancies between the coverage of the 2mass catalog and the maxBCG catalog. While the former primarily focuses on clusters at redshifts of approximately @xmath217, the latter extends to a higher range of @xmath218. Nevertheless, our findings indicate that the slope of the @xmath0richness relation is relatively similar between our work and the DKM07 study. Comparisons with X-ray selected catalogs, as discussed in section [ sec: xrayselect ], as well as prior research (* ? ? *; * ? ? ? ), enable us to contextualize our results and better understand cosmic structures and evolution. Comparisons to X-ray selected catalogs, such as those detailed in Section [sec : xrayselect ], as well as prior investigations such as * ? ? ? * and * ? ? ? * (cited in text), suggest significant scatter in the X-ray luminosity-richness relationship for individual clusters. With a high degree of scatter, the mean statistic used to devise the stacked luminosity-richness relation may be significantly out of alignment with the median relation due to this substantial variability. To comprehend the associated bias, we make assumptions. We presume that the conditional probability for the X-ray luminosity of a cluster at a particular richness follows a log-normal distribution, with a constant intrinsic scatter and a mean log luminosity that conforms to:\n\n@ xmath222\n\nwhere @xmath223 is the log normalization of @xmath5 obtained at @xmath214 and @xmath224 is the slope of both the @xmath5 and @xmath6 relationship. In our analysis, the mean statistic used to calculate the stacked @xmath0-@xmath1 relation may result in a significantly biased outcome compared to the median relation, as was discussed previously. To understand this, let's assume there is a log-normal conditional probability @xmath219 for the x-ray luminosity of a cluster at richness @xmath6, given by: @xmath220, \\[ @xmath221\\] with a constant intrinsic scatter @xmath222, and a log-normalized mean log-luminosity that follows: @xmath222,\n\nwhere @xmath223 is the log-normalization of @xmath5 at @xmath214, and @xmath224 is the slope of the @xmath5-@xmath6 relation. It is crucial to note that the median (@xmath225 for a log-normal distribution) of this distribution is equivalent to the mode and geometric mean. \n\nAt a specific richness, we aim to measure the median x-ray luminosity @xmath227, which is the peak of the distribution. By calculating the mean, we may introduce a significant bias when working with clusters that follow a log-normal distribution. The mean can be skewed by the presence of outliers or the tail of the log-normal distribution, leading to an overestimated average luminosity. However, by measuring the median, we eliminate the skewness and avoid being affected by either extreme values or the distribution's tail. We employ the notation @xmath226 and, at a certain richness, aim to measure the median x-ray luminosity @xmath227, which is the peak of the underlying log-normal distribution. However, the stacking exercise we have undertaken is essentially a calculation of the arithmetic mean ( @xmath0 ) at the same richness. As for a log-normal distribution with median @xmath227 and intrinsic scatter @xmath221, its arithmetic mean is @xmath228. Therefore, the stacked normalization results in an overestimate of the median of the underlying distribution by a factor of @xmath229. The arithmetic mean, represented as \u03bc, is calculated to be @xmath228. This implies that the stacked normalization is an overestimate of the median of the underlying distribution by a factor of @xmath229. If the scatter is large, the stacked (mean) normalization is significantly affected by the most luminous clusters and is biased high. For example, an @xmath230 scatter would imply a @xmath231 factor of bias. To alleviate this, we employ measurements of x-ray flux (and hence @xmath5) at the specific locations of each cluster with @xmath232. To constrain the scatter between the x-ray flux and cluster locations, we employ measurements of x-ray flux (corresponding to x^5) at those locations using the IDL Astronomy Library tool linmix_err, which utilizes a bayesian approach to linear regression with errors in both x and y. This tool allows for well-behaved results even when measurement errors dominate. We use this tool to fit x^5 as a function of x^6, which allows us to determine the intrinsic scatter (denoted by x^221) present in this relationship. The IDL Astronomy Library tool linmix_err is employed to fit the function of @xmath5 against @xmath6 using a Bayesian approach to linear regression with errors in both variables. The tool is capable of handling errors in measurement, non-detections, and upper limits in y. The chosen @xmath6 cut enables a significant richness range to constrain the slope and scatter in this approach due to Monte Carlo simulations. The fit is not sensitive to the precise richness cutoff selected. To calculate @xmath5 for the detected clusters, a procedure based on the count-rate to flux conversion method from the REFLEX survey is implemented. Out of the 955 clusters, over 83% are detected at least at the @xmath106 level. Furthermore, the fits are not sensitive to the precise richness cutoff selected for calculating the fluxes of the clusters. To obtain the fluxes for each of the marginally detected clusters, we use a procedure based on the count-rate to flux conversion method from the REFLEX Survey of @xcite. Firstly, we calculate the 0.5-2.0 keV (ROSAT hard channel) count rate within a fixed aperture of 146\" to improve the signal-to-noise ratio. The local background is calculated in an annulus, using the sector-rejection method as described in @xcite. In REFLEX, the temperature is estimated in an iterative fashion from the luminosity, using a scaling factor of 5\u00d710^5 (@xmath5\u00d710^5) at 2 keV, as proposed by @xcite. In order to calculate the local background in a 234 annulus and estimate the temperature in a reflex manner, the sector-rejection method developed by Xcite is employed. However, since many MaxBCG clusters do not have significant flux or luminosity measurements, individual cluster temperatures cannot be determined using these methods. Therefore, in our analysis, each cluster temperature is approximated by the stacked spectral temperature from the appropriate richness bin, shown in Table [tab: lx750]. This ensures that our stacked cluster luminosities and individual luminosity estimates are on a comparable footing. For each cluster's temperature, an estimate is obtained by stacking the spectral temperature from the corresponding richness bin based on Table [tab: lx750]. This procedure ensures that the stacked cluster luminosities and individual luminosity estimates are comparable. This is crucial as altering the cluster temperature does not substantially change the 0.1-2.4 keV luminosities (e.g. * * * for clusters with >=235 detections, as demonstrated by the reflex iterative recipe used to convert count rate to flux if we vary the temperatures, the individual flux values change by approximately  * * %). This illustrates the challenging nature of measuring cluster temperatures using ROSAT. However, when applying the reflex iterative recipe with count rates and temperatures in the clusters with @xmath235 detections, instead of fixing the temperatures at the stacked values, the individual conversion to @xmath5 values changes by approximately @xmath236. This demonstrates the challenging nature of measuring temperatures using ROSAT. The 0.5 to 2.0 keV count rate is then converted to a 0.1 to 2.4 keV luminosity using Table 2 from @xcite with the equivalent hydrogen column density at the cluster's position. A @xmath148-correction is then applied using Equation 4 from @xcite, which approximates Table 3 in @xcite with $z\\textrm{\\ }{}^{1/2}$. The corrections, at most 10%, are not significant. To refine the calculations for X-ray fluxes obtained from individual clusters, a @xmath148-correction is implemented based on equation 4 from @xcite. This approximation is a good fit for table 3 in the same publication. The resulting corrections, which do not exceed 10%, were applied where necessary. Our method for calculating X-ray fluxes for matched clusters is compared to the values obtained from the NORAS catalog. Consistency within errors was found to be consistent with @xmath237 scatter, though there is a systematic offset of @xmath238. The differences between the two methods lie in the way that the fluxes are calculated, with the NORAS fluxes evaluated using an aperture calculated by growth curve analysis to enhance signal-to-noise, while our approach involves using a fixed physical aperture. (For details on the X-ray catalog matching, see section [sec: xrayselect]). In contrast to our approach in calculating fluxes for matched clusters, the Noras fluxes were obtained using a growth curve analysis (GCA), which is designed to obtain the best signal-to-noise ratio for each cluster. In our method, we use a fixed physical aperture and calculate the fluxes without any prior knowledge of extended cluster emission, providing an unbiased estimate of cluster flux and luminosity. The primary difference between our approach and the Noras method is that our aperture is fixed, and we obtain the best signal-to-noise ratio for each cluster without considering extended emission. The individual cluster statistics for @xmath5 and @xmath6 are shown in Figure [fig: linmix], and we use linmix_err to estimate the slope, normalization, and intrinsic scatter for the 955 richest clusters with @xmath239 using the @xmath5@xmath6 relationship. The individual cluster values for @xmath5 and @xmath6 are displayed in Figure [ fig : linmix ]. Employing linmix_err, we estimate the power-law slope, normalization, and intrinsic scatter of the @xmath5@xmath6 relationship for the 955 richest clusters with @xmath239. The best-fit relation is @xmath240 with an intrinsic scatter of @xmath241. Calculating the individual @xmath5 values within a fixed @xmath146 aperture, this result is to be compared to Equation [ eqn : lxn_750 ]. To test the robustness of our finding, we divide the input data into multiple independently fit subsamples and slightly adjust the richness threshold. The calculation of the individual @xmath5 values is performed within a fixed aperture of @xmath146, which is compared to equation  [ eqn : lxn_750 ]. The results' robustness is tested by splitting the input data into independent subsamples and changing the richness threshold slightly. The analysis confirms that the constraint on the scatter is robust and accurate, as demonstrated in section  [ sec : coolcore ]. Potential systematic biases in the constraint on @xmath221 are addressed in the same section. Figure  [ fig : linmix ] displays the corrected @xmath227@xmath1 relation. Systematic biases in the restriction on @xmath221 are examined in Section [ sec : coolcore ], while the scatter-corrected @xmath227@xmath1 relation is depicted in Figure [ fig : linmix ]. Solid circles indicate the luminosities for the individual @xmath106 detections, and empty circles indicate the @xmath106 upper limits. The lower-right corner exhibits a typical error bar for detections. Dark gray contours illustrate the @xmath242 contours on the best-fit median relation, with dashed (dotted) lines signifying the @xmath243 (@xmath244) scatter constraints. In the lower-right corner, the contours depicting the best-fit median relation for @xmath242 are presented in dark gray. The dashed and dotted lines indicate the @xmath243 and @xmath244 scatter constraints, respectively. The median relation, as dictated by linmix_err, has been converted into an equivalent mean relation by multiplying the normalization by @xmath229. The light gray line shows the mean relation, which closely aligns with the squares representing the stacked bins, which have brighter individual values than the underlying median values. It should be noted that we are assessing the observed scatter in the @xmath5@xmath6 relation in accordance with our catalog and stacking method. The average trend aligns well with the stacked bins, which are approximately 231 times brighter than the underlying median values. It is crucial to note that we are measuring the observed scatter in the @xmath5@xmath6 relation based on our catalog and stacking method, as opposed to measuring the intrinsic variation between the clusters. This scatter is influenced by not only variations among clusters that differ in morphology and merging, but also by point sources, cooling flows, and the projection of multiple clusters along the line of sight. For the 245 richest clusters used in this exercise (246), the likelihood of projection is exceedingly low. Unfortunately, we do not have a method to efficiently remove point sources or bright cool cores from individual clusters in an unbiased manner without influencing our measurement. Operatively, whether the observed scatter derives from intrinsic variations or point source and cooling core contamination is insignificant; we focus on constraining the observed scatter. Lacking the means to remove point sources or cool cores on an unbiased basis, it is irrelevant for our measurement whether these intrinsic variations or contamination contribute to the observed scatter. To constrain the scatter, however, it is only necessary to assume that the underlying distribution is approximately log-normal, a consistent observation with our findings. In particular, we measure the observed scatter in the relationship between intracluster light and richness, where richness is determined from the amplitude of the galaxy-cluster correlation function. We use an approach based on Bayesian maximum-likelihood fitting, with 290 optically-selected clusters and 40 of these groups showing significant detections in _ROSAT_/PSPC observations. Xcite's study measures the observed scatter of a power-law correlation between the bolometric luminosity and the richness measure, derived from the amplitude of the galaxy-cluster correlation function. The data came from 290 optically-selected clusters, with 40 having significant detections in ROSAT/PSPC observations. Through a Bayesian maximum-likelihood fitting technique, Xcite found a significant correlation between the two quantities, with a slope of 0.77 and an intrinsic scatter of 0.12 dex. The study introduces a prior weighting of 0.05 to give stronger preference to models with lower scatter. When comparing their findings with other measurements, it should be noted that different richness quantities and selection functions were utilized. However, it should be acknowledged that the study by @xmath250 incorporates a prior weighting that gives higher importance to models that have lower scatter. Such an approach is beneficial in this context. Nonetheless, when comparing our measurement of the slope and scatter to the results of @xcite, there are challenges that arise due to differences in the richness measures and selection functions used. This is evident from the fact that optically-selected clusters sometimes appear to be less luminous in X-rays compared to their optical richness. This can be explained by a large intrinsic scatter in the @xmath5@xmath6 relation that is often observed. The pattern formed by punctuation marks is \" * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? \" In particular, the @xcite study introduced a group of Abell X-ray underluminous (AXU) clusters, which have masses determined through velocity dispersion measurements and are significantly dimmer in X-rays than expected. Figure [Fig: Linmix] depicts the open squares that denote the seven AXU clusters that match to the MaxBCG catalog (Maximum Likelihood Cluster Identification) presented by @xmath2 and @xmath233. Several \"Abell X-ray underluminous\" (AXU) clusters identified by @xcite exhibit significantly lower x-ray luminosities compared to their expected levels. The seven axu clusters that align with the MaxBCG catalog (@xmath2, @xmath233) are depicted with open squares in Figure [fig:linmix]. These clusters are substantially dimmer than the average @xmath5@xmath6 relation, but still within the observed scatter. Velocity dispersion measurements of MaxBCG clusters (@xmath2) and similar studies based on simulations (@xcite) have revealed considerable mass-mixing in a given @xmath6 bin, indicating that lower-mass halos may be selected as moderate-richness clusters. The velocity dispersion measurements of MAXBCG clusters (B07) have revealed significant mixing within a given \\chi^2_6 bin. Low-luminosity clusters, which could be from lower-mass halos picked up as moderate richness in the cluster finder, have also been observed. However, according to Xu et al., the AXU clusters display lower-velocity anisotropies relative to their mass when compared to simulations. One possible explanation is a potential mass calculation bias in the velocity dispersion method, especially with small sample sizes, or a high intrinsic scatter in the \\chi^2_{5} - \\chi^2_{13} relationship. The apparent discrepancies in galaxy velocity dispersion masses calculated from small samples and the @xmath5@xmath13 relation may be attributed to inherent biases or significant intrinsic scatter in this correlation. These possibilities are aligned with the observations and are further explored in a companion letter @xcite . It is rational to speculate that the scatter in the @xmath5@xmath13 relation may not be fixed with respect to richness, which could offer insights into the structure and evolution of galaxy clusters. In our latest research letter, we investigate the relationship between the @xmath5@xmath13 quantities in @xmath5@xmath13 clusters, as detailed in the companion letter  @xcite . However, we found that the scatter in this relationship, represented by @xmath241, is not fixed and may change with cluster richness. Specifically, B07 recently identified a significantly larger scatter in the velocity dispersion - richness relation at lower richness compared to higher richness levels. Unfortunately, the quality of our data for this analysis is not sufficient to accurately measure the variation of scatter with richness. Therefore, for the remainder of the paper, we adopt the nominal value of @xmath241 to correct and obtain the underlying median @xmath227 values by averaging the stacked @xmath0 data. Despite having insufficient data to determine changes in scatter as a function of richness, we adopt a nominal value of k for correcting stacked (mean) x0 to obtain the underlying median values. Essentially, this requires multiplying all x0 values by a factor of k. We compare the median x227x1 relationship for optically-selected MAXBCG clusters to that from X-ray selected clusters from literature. For this exercise, we compare to the NORAS * and the 400 square degree (400d) catalogs, both constructed from ROSAT data and measured in the same energy range as used in this work. In direct comparison with the x-ray selected cluster catalogs, we specifically chose to compare the @xmath227@xmath1 relation for optically - selected maxbcg clusters to the results presented in the NORAS *400d catalog, both constructed from ROSAT data, and primarily measured in the same energy range used in our study. The NORAS catalog is an X-ray flux-limited cluster catalog that serves as a large sample of bright X-ray clusters, constructed from the RASS photon maps in the northern sky, while the 400d catalog is a serendipitous X-ray flux-limited cluster catalog compiled through pointed ROSAT PSPC observations. Although the NORAS catalog is recognized to be only 55% complete, it provides a crucial dataset for cross-comparison with the maxbcg survey region. The 400d catalog, a serendipitous X-ray flux-limited cluster catalog assembled from pointed ROSAT PSPC observations, provides a significant number of bright X-ray clusters that overlap with the MAXBCG survey region. However, the 400d catalog only covers fields sampled from the whole sky, resulting in only about 249 of the catalog overlapping the MAXBCG survey area. This exercise aims to establish a baseline comparison of the X-ray and optical richness properties of X-ray selected clusters with optically selected clusters. To obtain a list of clean matches between NORAS or 400d clusters and MAXBCG clusters, the X-ray position and BGC position must be matched within 253 arcseconds and the redshift difference should be less than 0.05. The primary objective of this analysis is to determine a baseline comparison of richness properties, obtained from studying X-ray-selected and optically-selected galaxy clusters. A slightly different comparison was completed in reference @xcite, focusing on assessing the completeness of the MaxBCG catalog. To ensure accurate and genuine matches, we require the X-ray and BCG positions to be within @xmath253 and the redshift difference to be less than 0.05. This ensures that the probability of a false match is minimal and we are not incorporating overlapping or merged clusters. Out of 89 NORAS clusters and 53 400d clusters that match the MaxBCG catalog, their median redshifts are @xmath255 and @xmath256, respectively. Furthermore, to maintain consistency, we convert all X-ray luminosities to 0.1-2.4 keV in the background frame, as per our predetermined cosmology. There are 89 clusters in the NORAS dataset that match the MAXBCG cluster catalog, with a median redshift of 0.34, and 53400 clusters that match MAXBCG in the 400d catalog, with a median redshift of 0.35. We transform all X-ray luminosities between 0.1 and 2.4 keV in the rest frame for our adopted cosmology. It is essential to note that the X-ray luminosities in NORAS and 400d have been corrected for aperture effects, and the corrections for the aforementioned catalogs using the specified parameter values (@X_{108} being 9.2 for NORAS and 8.0 for 400d) produce a maximum offset of 0.02 dex in the estimated luminosities obtained in our scaled 37 arcmin aperture. Figure [ fig : catmatches ] shows the correlation between the estimated X-ray luminosities and the projected cluster mass for clusters that meet the necessary matching criteria. Nevertheless, utilizing the adopted parameters of xmath108 ( xmath257 for Noras and xmath258 for 400d) yields at most a \u0394xmath5 of approximately xmath237 with respect to the luminosities obtained in scaled xmath37 apertures. Figure [fig : catmatches] demonstrates the correlation between xmath5 and xmath6 for the x-ray selected clusters that meet our matching criteria. The solid circles represent Noras clusters, while the empty squares represent 400d clusters. Noteworthy, the luminosities of 400d clusters are typically lower than those for Noras due to the more profound flux limit in the pointed ROSETTA observations in contrast to the RASS survey. The dashed line denotes the median xmath227-xmath1 correlation, and the dotted lines indicate the xmath243 scatter constraints. The luminosities of the 400d clusters are generally less than that of NORAS clusters due to the deeper flux limit of the pointed ROSAT observations relative to the RASS survey. The dashed line represents the median X-ray luminosity-richness relation, while the dotted lines indicate the @xmath243 scatter constraints. It should not be surprising that X-ray selection primarily picks out the X-ray brightest clusters at the same richness, with the majority of clusters from NORAS lying above the median MAXBCG relation. However, the deeper 400d survey selects a sample that is more representative of optically selected clusters, still with some selection bias. Comparing to Figure [Fig: Linmix] at each richness, the typical X-ray emission from clusters remains significantly below that from X-ray selected catalogs. In the deeper 400d survey, a sample that is more representative of optically-selected clusters is selected, albeit with a high level of bias. Comparing this to Figure [ Fig : Linmix ] demonstrates that, at every richness level, the typical X-ray emission from clusters falls below what would be expected from X-ray-selected catalogs for both catalogs, but it is particularly noticeable for NORAS clusters. Although the 400d survey possesses the sensitivity to detect the richest X-ray dim clusters, these clusters are comparatively rare. The limited area overlap between the 400d survey region and the MAXBCG survey region makes it highly improbable that the 400d survey contains one of the specific MAXBCG clusters. While the 400d survey has the sensitivity to detect the richest X-ray dim clusters, which are quite rare, these clusters have a low occurrence rate in the survey region. Since the 400d survey's overlap with the MAXBCG survey is limited, it is unlikely that the 400d survey contains a specific MAXBCG cluster. A variety of systematic effects may affect the @xmath227@xmath1 relation, which we address in this section. These effects include photometric redshift uncertainty, cluster centering errors, richness variation with redshift, BGC luminosity, point source contamination, and cool core clusters. The MAXBCG cluster catalog is based on photometric data, and thus, our analysis is contingent upon photo-@xmath31s, which may potentially introduce a redshift-related bias. In the analysis process, there are six major factors that could potentially affect the results: photometric redshift uncertainty, cluster centering errors, richness variation with redshift, BGC luminosity, point source contamination, and cool core clusters. While the MAXBCG catalog is primarily based on photometric data, we primarily rely on so-called \"photo-z's\" for each cluster. Although these photo-z's are relatively precise, it's crucial to examine how the use of such estimates might influence the stacking results. For instance, if a cluster appears closer than its photo-z suggests, it is overweighted in the stacking analysis. Conversely, if it is more distant than it appears, it is underweighted. Even with perfectly symmetrical photo-z errors, the weighting factor (Wiener filter) may still introduce a bias in the luminosities. These factors need to be carefully considered and accounted for to accurately analyze the data. Even when the photo-@xmath31 errors appear utterly symmetrical, the weighing factor @xmath83 fails to correspond. This discrepancy can lead to an uncertainty in photo-@xmath31, which in turn, introduces a significant bias in luminosities. Consequently, it can cause the clusters to appear more luminescent than they actually are. A modest volume effect also stems from photo-@xmath31 errors at a fixed redshift. The phenomenon of apparent luminosity enhancement in clusters, resulting from redshift errors, can skew the perceived brightness of the clusters in question. This effect, known as photo-z errors, leads to clusters appearing more luminous than they truly are, owing to their greater distance and consequent increase in physical volume. As a result, at a fixed redshift value, significantly more clusters are available to scatter low luminosities at higher redshifts, while fewer are available to scatter high luminosities at lower redshifts. This is reminiscent of Malmquist bias, which commonly affects redshift-limited surveys in astronomy. At high redshifts, a larger number of galaxy clusters become visible, resulting in a larger number of lower-luminosity clusters compared to low redshifts, where fewer clusters but mostly high-luminosity ones are observable. This effect is similar to Malmquist's bias, where objects with greater apparent brightness are more readily observed and hence overrepresented in samples. Regarding the calculation of cluster luminosities, as the photometric redshift estimate is used to determine the radius of the cluster's extraction, if a closer cluster's photo-z estimate is less precise, a smaller extraction radius will be utilized, which leads to a minor reduction in the cluster's implied luminosity. In the context of cluster analysis, a photometric redshift is employed to calculate the cluster extraction radius. When a cluster is closer to its measured photo-z than implied from its luminosity, the radius used for extraction is too small, slightly reducing its apparent luminosity. Conversely, when a cluster is farther away than its measured photo-z, the opposite occurs, and the extracted flux at larger radii is minimal, resulting in a symmetrical bias that is not expected to strongly affect stacked detections. To estimate the combined photo-z bias as a function of richness, a Monte Carlo simulation is run around the BCGs in the MAXBCG catalog used in this study. However, since cluster flux is minimal at large radii, and as the photo-z bias impact is roughly symmetric, we do not anticipate it to have a substantial influence on our stacked measurements. We used a Monte Carlo simulation to estimate the combined photo-z and redshift bias effect as a function of richness in our analysis. Our sample, which is derived from the maximum likelihood (MaxBCG) cluster catalog utilized in this analysis, consists of 261 BCGs that have spectroscopic redshifts from the Sloan Digital Sky Survey (SDSS) Data Release 5 (DR5) spectroscopic catalog. With the spectroscopic subsample, we determined the photo-z offset (delta_z) distribution in each richness bin and three redshift bins. A Monte Carlo approach was conducted to estimate the overall photo-z bias effect on our measured luminosity. The \"true\" luminosity determined by our convolved photo-z distribution was compared to the \"observed\" luminosity measured at the photometric redshifts. In order to assess the photo-@xmath31 offset and subsequent bias in luminosity measurements, the study presents a breakdown by richness bin and redshift bin. A Monte Carlo approach is employed to estimate the overall luminosity offset by comparing the true luminosities convolved with the @xmath263 distribution to the observed luminosities calculated at the measured photometric redshifts. The study finds a small overall luminosity bias factor of @xmath264 (at 99% confidence limit), with the observed luminosities slightly overestimating the true luminosities. This bias factor does not vary with richness. In the current stacking exercise, the brightest cluster galaxy (BCG) is selected as the cluster center. In our stacking analysis, we disregard the small magnitude corruption factor associated with any bias caused by varying richness, as it does not significantly affect our results. For our selection of the brightest cluster galaxy (BCG) as the central point in the stacking process, we note that it aligns with the x-ray center for most clusters, particularly those with distinct x-ray emission. However, certain types of clusters exhibit misalignment between the BCG and x-ray center due to factors such as merging clusters with irregular shapes, clusters without a dominant central galaxy, or strong cooling flow clusters. Nonetheless, in certain clusters, the Brightest Cluster Galaxy (BCG), which typically aligns with the cluster's center of X-ray emission, does not always conform to these standards. Disturbed cluster morphologies in merging systems and clusters with no clear dominant central galaxy for detection by the MAXBCG algorithm are some of these deviations. Additionally, a handful of previously identified strong cooling flow clusters may also fall under this category. The central galaxies in these clusters commonly host active galactic nuclei (AGN) and exhibit significant levels of ongoing star formation, moving their colors off the red sequence and being excluded from the MAXBCG algorithm. It is unlikely that centering biases would have a substantial impact on our calculation of the mean X-ray luminosity for a few reasons. Firstly, the X-ray luminosity is directly proportionate to the electron density, and thus most of the luminosity stems from the central region. Also, with our utilization of large apertures in our observation ( @xmath267 ), the effect of any centering biases is greatly diminished. In our calculation of the mean x-ray luminosity, we do not anticipate substantial centering biases for several reasons. Firstly, the electron density at the x-ray emission peak, which is proportional to the luminosity, mainly originates from the cluster's central region. With our large apertures, a majority of the cluster's photons will be included in the photon counting exercise as long as the core is within the aperture. Secondly, simulation results suggest that the majority of richer clusters are well-centered (Jonathan et al. 2007), although this does not mean that centering has no impact. As shown in Section [ sec : profiles ], the @xmath108-model parameter is highly dependent on the centering distribution. To confirm this, we conduct tests involving randomly offsetting all cluster positions with a 2D Gaussian having an RMS of @xmath268. Our results demonstrate that the mean @xmath0 values decrease by @xmath269, and less than @xmath106 of the clusters' centers are moved by that distance. However, simulations suggest that the majority of the richest clusters are appropriately centred, as reported in simulation J07. Nevertheless, the centering of the clusters is crucial in determining the value of the @xmath108 parameter, as demonstrated in section [sec: profiles]. To test whether centring affects our analysis, we performed de-centering tests by randomly offsetting all the cluster positions with a 2D Gaussian distribution with an RMS of 268 (comparable to the centering distribution modelled in J07). As a result, the mean @xmath0 decreased by 269, and less than 106 (RMS value chosen due to the fact that the vast majority of X-ray cluster matches fall within 253) this RMS value does not contribute a significant bias to our X-ray luminosity calculation. We will investigate whether the observed @xmath0 at fixed richness changes with redshift, as modest evolution in @xmath0 at fixed mass is expected. The investigation under consideration examines whether the observed quantities, represented by @xmath0, at constant cluster richness have any evolution in redshift. Moderate evolution in @xmath0 at fixed cluster mass is anticipated. Assuming the cluster system adheres to self-similar scaling, high-redshift clusters should be more luminous than their low-redshift counterparts due to the greater density of the universe at higher redshifts. This relationship is defined by the critical density of the universe at redshift @xmath31, as denoted by @xmath270, and is proportional to @xmath271. Consequently, clusters with a given mass at redshift @xmath272 should be more luminous than corresponding clusters at a lower redshift of @xmath274. The expectation is that clusters at a given mass at redshift @xmath31 should be brighter than those at lower redshifts for a critical density of the universe @xcite. To investigate potential variations of matter density at a fixed richness, the cluster sample is split into three redshift bins. The lowest redshift bin ranges from @xmath275 with a median of @xmath276, the middle redshift bin ranges from @xmath277 with a median of @xmath41, and the highest redshift bin ranges from @xmath278 with a median of @xmath279. Wider clusters bins have been grouped to improve signal-to-noise ratios: bins @xmath63 and @xmath62 are combined for the lowest redshift bin, bins @xmath65 and @xmath64 for the mid-redshift bin, and bins @xmath280 and @xmath66 for the highest redshift bin. These wider bins correspond to the richness bins used in the previous S07 lensing analysis. The low redshift bin covers @xmath275 redshift levels with a median of @xmath276, the middle redshift bin spans @xmath277 to @xmath41 with a median of @xmath279, and the high redshift bin encompasses redshifts from @xmath278 to a median of @xmath279. Combining richness bins @xmath63 and @xmath62, as well as @xmath65 and @xmath64, and @xmath280 and @xmath66, allows for greater signal-to-noise ratios in these ranges. The wider bins are employed in the S07 lensing analysis. Figure [fig : splitzplot] shows the stacked X-ray luminosity versus redshift for the three different richness bins, with circles representing the low redshift clusters, diamonds representing the high redshift clusters, and a dashed line displaying the mean relation (equation [eqn : lxn_r200] from section [sec : meanrelation]) for all clusters. The high redshift clusters exhibit significantly higher luminosity than the low redshift clusters. The dashed line in the provided visualization represents the average relation, as shown in Section [ sec : meanrelation ] (Equation [ eqn : lxn_r200 ]). Diamonds denote high redshift clusters, while circles indicate low redshift clusters. The high redshift clusters are significantly more luminous than their low redshift counterparts. This luminosity difference is parameterized using a factor of @xmath281 and fitting all redshift and richness bins with a model of the form: @xmath282, where @xmath41 is the median redshift of the cluster catalog. This results in a good fit (Equation [ eqn : lxn_r200 ] with best-fit parameters @xmath284, @xmath285, and @xmath286. Note that @xmath287 and @xmath108 are consistent with the average relation presented in Equation [ eqn : lxn_r200 ] while the redshift variation parameter, @xmath288, is relatively large and highlights how the high redshift bin (diamonds) has nearly twice the luminosity of the low redshift bin (circles). In accordance with the statistical analysis, the parameters obtained for the fit are @xmath283, @xmath284, and @xmath285, with @xmath286 as the fixed parameter. Notably, @xmath287 and @xmath108 align with the mean relation in Equation [ eqn : lxn_r200 ] for galaxy cluster richness. However, the variation parameter, @xmath288, is significant and notably larger, indicating that the high redshift bin (@xmath279) is almost twice as bright as the low redshift bin (@xmath276) \u2013 remarkably more luminosity than the self-similar prediction. The challenge is to identify the origin of this notable redshift-dependent shift in luminosity for a fixed richness. The errors associated with the photo-z measurement, as described in Section [ sec : photoz ], could potentially contribute to this effect to some extent. The issue to address is ascertaining the source of the redshift-dependent shift in the parameter @xmath0 observed at a fixed richness. The aforementioned photo-@xmath31 errors, as detailed in section [sec : photoz], may account for some portion of this effect. Yet, while the absolute photo-@xmath31 errors at all redshifts are better than those of the older method @xmath289, the relative photo-@xmath31 errors are proportionally larger at lower redshifts than at higher redshifts. Consequently, the influence of photo-@xmath31 bias is greater at lower redshifts than at higher redshifts. In a similar vein to the monte carlo simulation described in section [sec : photoz], we replicated the simulation to gauge the photo-@xmath31 bias from each redshift category. As evident from our earlier analysis [sec : photoz], the magnitude of photo-z systematic errors, denoted as photo-@xmath31 bias, is found to be higher at low redshifts compared to high redshifts. To measure this effect, as described in section [ sec : photoz ], we performed a Monte Carlo simulation for each redshift bin. The additional redshift bias measured in the low-redshift bin is @xmath290 (99% confidence limit), whereas in the high redshift bin it is @xmath291 (99% confidence limit). Although the photo-@xmath31 bias is smaller than the observed variation in @xmath5, it is comparable to this variation in its sign. It is unlikely that the photo-@xmath31 bias is the primary reason for the observed redshift-dependent variation in @xmath6 at fixed mass, which could be a more probable explanation for this effect. In studying the relation between redshift, galaxy cluster lensing, and mass, the photometric redshift bias for @xmath31 is found to be smaller than the observed variation of @xmath5, albeit in the same sense. However, a more likely possibility is that redshift-dependent variation of @xmath6 contributes to this effect at fixed mass. This would lead to shifts in the position of high-redshift points in [fig: splitzplot]. To constrain this potential bias, the effect of the self-similar evolution of @xmath0 at fixed mass, predicted by @xmath292, must first be factored out. After applying this correction, the excess variation in the @xmath0@xmath1 relation attributable to variation of @xmath6 at fixed mass is calculated using equation [eqn: zgamma]. To account for the self-similar evolution of xmath0 at constant mass, we remove its effect using the previously established redshift dependence, which we denote as xmath292. Next, we refit the data to Equation [eq: zgamma] to calculate the excess variation in the xmath0-xmath1 relationship arising from changes in xmath6 at constant mass. This yields a best-fit parameter of xmath293, resulting in a fractional decrease in xmath6 by xmath294%xmath295% from our lowest redshift bin, @xmath296%, to our highest redshift bin, @xmath297% . This is consistent with the redshift-dependent variation in the velocity dispersion-optical richness relationship recently discovered by B07. This consistency with the redshift-dependent variation in the velocity dispersion-optical richness relation observed in B07 highlights a distinct deviation from the smaller redshift dependence in the observed lensing shear recorded by S07. However, none of the current approaches have been able to confidently determine the origin of this variance. The impact of incorrect 0.4-band or color cuts used to identify the galaxies being used in the richness estimation is one possible explanation for this effect, which has yet to be fully examined. Currently, none of the proposed methods has successfully determined the source of the observed variation. For example, it could be due to improper handling of the 0.4 @xmath18 and color cuts that determine which galaxies are considered in the richness estimate. Additionally, the effect could be genuine evolution, such that clusters have fewer red-sequence galaxies brighter than 0.4 @xmath18 at fixed mass at higher redshifts. As a result, further work and improvements to the richness estimates are ongoing to discern the nature of this evolution. In this study, we investigate the impact of brightest cluster galaxy (BCG) @xmath30-band luminosity (as defined in @xcite) on the average X-ray luminosity @xmath0. Additional studies aimed at enhancing richness estimates are currently underway to better understand the evolutionary nature of clusters. At present, our focus is on examining the correlation between brightest cluster galaxy (BCG) @xmath30-band luminosity, denoted by LBCG @xmath299, and mean X-ray luminosity @xmath0, based on the predictions made by simulations and semi-analytical modeling. It is expected that dark matter halos formed early will exhibit brighter BCGs and lower richness, resulting in a correlation between LBCG and mean X-ray luminosity at fixed richness. Furthermore, previous optical and X-ray observations of nearby clusters have confirmed that more luminous BCGs are linked to more massive halos with higher X-ray temperature and luminosity, as exemplified by * ? ? ? Consequently, in accordance with this correlation, we anticipate that the X-ray luminosity should be positively correlated with the cluster's central compact source, known as the brightest cluster galaxy or BCG, at a specific richness level. This trend has already been observed through optical and X-ray observations of nearby clusters, where more luminous BCGs are associated with more massive halos with higher X-ray temperature and luminosity (E.g. * Jauzac et al., 2008). Similar to the analysis performed in the previous section, for an exercise in this section, we split each wide richness bin into three bins based on the @xmath299 brightness, which is correlated with redshift. However, unlike redshift, @xmath299 is not linearly correlated with redshift. Therefore, we sorted the clusters in each richness bin by @xmath299 brightness and split the sample into the top 25%, middle 50%, and bottom 25% and restacked. By doing this, we are examining if there is a correlation between the X-ray luminosity and @xmath299 brightness, while controlling for richness. However, unlike redshift, which is uncorrelated, there is a correlation between the amount of x-ray emission, denoted as xmath299, and richness, represented by the symbol xmath6. This relationship enables us to identify the clusters' most luminous BCGs in each richness bin. To account for this difference, we first sort the clusters in each richness bin by xmath299 and then split the sample into the top 25%, middle 50%, and bottom 25% for further analysis. The results are displayed in Fig. [fig: splitbcgplot], where clusters with the most luminous BCGs (represented by diamonds) are significantly more luminous in x-rays than those in the other two bins for each richness bin. This effect is most significant at lower richnesses. This may be due to the fact that in low richness clusters, the bCG is more dominant compared to high richness clusters, as reported by previous studies such as ? ? ? . In lower-richness environments, this phenomenon exhibits the most prominent effect. This is expected as the impact of the brightest cluster galaxy, commonly known as the BCG, is more significant in low-richness clusters in contrast to high-richness clusters. Despite this, there does not seem to be any significant difference between the BCG influence on the central density of the cluster for the low- and middle-richness bins. To quantify the connection between richness and the BCG's effect on the central density, we employ a similar process utilized for redshift variation. By fitting a model that encompasses both richness and BCG parameter, we obtain a satisfactory fit, with the best-fit parameters being: @xmath305; @xmath306; @xmath307. We adopt a comparable procedure to that used for redshift variation to parametrize the impact of @xmath299 on @xmath0. The richness and @xmath299 bins are simultaneously fitted with a model: @xmath300. This results in an adequate fit ( @xmath301 ) with the best-fit parameters: @xmath302 ; @xmath303 ; @xmath304. While the normalization aligns with equation [ eqn : lxn_r200 ] , the slope is significantly more shallow. This is attributed to correlation between @xmath299 and @xmath6. Accounting for this correlation, the mean relation aligns fully with that calculated without splitting bins by @xmath299. The relationship between @xmath299 and @xmath6 causes a correlation that results in a mean relation consistent with unsplit bins. However, the high value of @xmath145 of the fit is due to the fact that @xmath0 is not a smooth function of @xmath305. The effect of @xmath299 is not symmetric and only boosts the x - ray luminosity of clusters with relatively bright BCGs. The reason for this could be due to a combination of factors, such as an imperfect cluster finder, a crude richness definition, or real cluster physics. Clusters with brighter and more dominant BCGs are more likely to be correctly centered and could potentially be more luminous in x - rays. It is uncertain whether the observed effect on X-ray luminosity is attributable to an inadequate cluster finder, a crude definition of cluster richness, or genuine cluster physics. Clusters that have brighter, more prominent Brightest Central Galaxies (BCGs) are more likely to be correctly centered and consequently might appear more luminous in X-rays. However, as stated in Section [ sec : centering ], the influence of decentered clusters on the detection of X-ray emission, indicated by \uf06b(X), is significantly smaller than the influence caused by \uf06b(z). The richness definition, \uf06e(N), is a count of red-sequence galaxies of magnitudes stronger than \uf06d(M), which explains why an imperfect cluster that houses a strikingly luminous BCG might be more massive and feature a greater X-ray luminosity than a cluster with a less luminous BCG and a similar richness. Our measure of richness, @xmath6, involves counting red-sequence galaxies that exceed a luminosity threshold of @xmath306. It follows logically that poorer clusters with extraordinarily bright brightest cluster galaxies may have higher masses and X-ray luminosities than similar clusters with more conventional brightest cluster galaxies. Additionally, it is possible that these clusters are connected to dark matter halos that formed at an earlier time, thereby allowing numerous member galaxies to merge into large brightest cluster galaxies. However, to determine whether optical properties such as richness can be used to estimate the age of a dark matter halo, deeper X-ray observations of specific clusters must be pursued. One should be cautious when calculating projected X-ray luminosities due to possible contamination from point sources. Some clusters may possess point sources, such as cluster AGN, that are connected to the cluster; yet, in general, their luminosities are not strongly correlated with that of the intracluster medium (ICM). Other point sources, including chance foreground star and background quasar encounters, may also artificially boost the apparent luminosity. Some point sources, such as those associated with cluster AGNs, may contribute to the cluster's luminosity, but their luminosity is not directly correlated with that of the ICM. Chance coincidences, on the other hand, like foreground stars and background quasars, can increase the apparent luminosity due to their broad proximity to the cluster. The issue of point source contamination is aggravated by the broad Point Spread Function (PSF) of the RASS survey. This makes it difficult to accurately excise point sources from higher redshift clusters, even if their positions are known. However, for the stacking exercise, foreground and background sources are not a significant issue because their uncorrelated position makes their contribution subtracted with background estimation. This is confirmed by stacking random points in Figure [Fig: Imageplots]. At the same time, both associated and chance projection point sources may increase the observed scatter. In general, point source contamination is a challenging issue in the analysis of large-scale structures. The large point spread function (PSF) hampers precise extraction of point sources from higher redshift clusters, even with known positions. In the context of stacking, foreground and background sources are not significant factors, as demonstrated by the stack of random points depicted in Figure [ fig: imageplots ]. However, uncorrelated foreground and background point sources may increase the observed scatter. In general, the values calculated from RASS are not corrected for point source contamination. As a first-order check, we intend to evaluate the potential for contamination from cluster AGN, which are the predominant point sources that may lead to a bias in the observed Compton parameter, X0. First-order checks are commonly made to determine the possibility of contamination from cluster AGN, particularly from cluster point sources that can skew the observed data in X-ray measurements. In a recent in-depth analysis, a team of researchers conducted a study of AGN distribution, specifically focusing on eight moderate redshift clusters comparable to the MAXBCG catalog's redshift range. Their findings revealed that at least 15% of the total population of bright cluster members were associated with moderately bright AGN. This suggests that the richest MAXBCG clusters likely contain one or more moderately bright AGN. According to research findings, moderately bright active galactic nuclei (AGN) associated with bright cluster members (BCM) make up approximately @xmath309 of the total population of cluster members. This suggests that an overwhelming majority of the wealthiest and most significant galaxy clusters carry at least one moderately bright AGN. These clusters exhibit extremely bright intracluster medium (ICM), leading to a negligible fraction of luminosity from cluster AGN. Therefore, even in the poorest clusters, estimated to have one AGN in every ten, the AGN's luminosity would be similar to the ICM, but its small fraction should not significantly affect the stacked average. To verify possible contamination in X-ray signals from cluster AGN, this research matches MaxBCG cluster member galaxies to radio sources from the First Survey (FIRST) with a match radius of @xmath313. This simple check helps identify potential contamination sources. An exploration of the relationship between active galactic nuclei (AGN) and the cluster environment leads us to examine the prevalence of AGN in our poorest clusters. If the AGN fraction observed in nearby clusters @xmath311 is considered as a benchmark, we can predict that approximately one in ten of our poorest clusters may host moderately bright AGN @xmath312. While these AGN may have a luminosity comparable to or even exceeding that of the intracluster medium (ICM) in these systems (@xmath313), the small overall AGN fraction is unlikely to introduce significant contamination or bias to stacked X-ray signal analysis. As a simple test for contamination from cluster AGN, we cross-match the MAXBCG cluster member galaxies with the FLASH Survey's first catalog of radio sources using a match radius of @xmath313. Our preliminary analysis suggests a purity of around @xmath314% for our matches to FLASH, with the first survey covering approximately the same footprint as the Sloan Digital Sky Survey (SDSS) down to a typical flux limit of @xmath315 at 1.4 GHz. The advantage of using 1.4 GHz radio detections as a proxy for AGN activity lies in the fact that, as the radio spectrum is typically flat, the 1.4 GHz flux is not a strong function of redshift. The first survey encompasses a similar region to the Sloan Digital Sky Survey (SDSS) at a threshold flux of approximately 0.0315 mJy at 1400 MHz. Such a focus on radio detection has advantages since the spectrum at this frequency is commonly flat, and thus the 1.4 GHz flux is not heavily influenced by redshift. However, only 10% of X-ray bright quasars and AGN produce detectable radio emissions, and there is significant scatter in the relationship between X-ray and radio luminosities for these objects, as evidenced by @xcite's observation that none of the X-ray selected AGN in a small set of clusters emit radio signals. Despite an incomplete selection function, the present survey can still mitigate the contamination from cluster AGN by investigating any significant contribution from this subpopulation to the average @xmath0 flux. According to @xcite, for the small set of clusters analyzed, none of the x-ray selected AGN are radio loud, regardless of the incomplete selection function. This allows us to restrict the contamination from cluster AGN in our analysis. If the mean \u2126X value is significantly enhanced by cluster AGN, then clusters compatible with radio sources should be relatively bright. This effect is most notable at low richness and low \u2126X when the intracluster medium (ICM) is not as bright or hot. Within each richness bin, the maximum BCG cluster members within @xmath146 of the BCG were matched to sources in the first survey. A significant effect on the Intra-Cluster Medium is most notable at low galaxy densities and low temperature when the ICM is less hot and bright. For each richness bin, we matched the maxBCG cluster members within 1.46 arcminutes of the BCG galaxy with sources in the first survey. Table [Tab: Radiomatch] reports the number of clusters in each bin that have members matched with the first catalog. In-depth work is currently in progress, where cross-correlating the maxBCG catalog with radio sources is being explored to understand cluster AGN feedback and related issues. The CCC study at 61, 45, 82, 62, 63, 64, 65, 66, 67, 68, and 69 wavelengths recorded 61%, 45%, 82%, 62%, 63%, 64%, 65%, 66%, 67%, 3320%, and 1341% matches for each richness bin, respectively. To compare how the radio-selected subset of clusters compares to a random subset of similar richness, in each richness bin, we ran 10,000 trials by sampling (with replacement) from the entire set of clusters in the bin and sampling the same number of clusters that match radio sources. As a quick estimate of the mean luminosity, rather than performing the complete stacking analysis and spectral fits, we calculated the stacked, background-subtracted hard-channel (0.5-2.0 keV) counts in a fixed 1.46-arcminute aperture. In-depth research is being conducted in correlating the MAXBCG catalog with radio sources to investigate cluster active galactic nucleus (AGN) feedback and related issues, which is ongoing work (see also Xcite). For this purpose, CCC codes @xmath61, @xmath62, @xmath63, @xmath64, @xmath65, @xmath66, @xmath67, and @xmath68 were used, as well as @xmath69, which offers a sample of 1341 clusters. A random selection of clusters with matched radio sources of similar richness was tested by randomly sampling the same number of clusters from the entire set for 10,000 trials, with replacement, within each richness bin using the CCC codes. The hard channel (0.5-2 keV) stacked and weighted, background-subtracted counts were obtained for each trial in a fixed aperture, and the mean count rate was converted to flux using the method described in section [ sec: scatter ]. Figures [ fig: radiomatch ] compare the mean flux of clusters that match radio sources to the expected typical values of flux obtained from drawing the same number of clusters with replacement from the entire sample. Subsequently, we can compare the mean value of @xmath0 for clusters that have matching radio sources to the expected values derived from randomly drawing the same number of clusters from the entire dataset. These findings are demonstrated in Figure [fig : radiomatch ] wherein the vertical dashed line indicates the mean @xmath0 for the clusters that match with radio sources, while the histogram reveals the distribution of @xmath0 values obtained through resampling. In these panels, we do not identify any significant discrepancy as a function of cluster richness, with the exception of the @xmath316 richness bin, where the clusters that match with radio sources appear to be outliers. The present study's analysis did not indicate any significant correlation between richness and bias. The only exception is in the @xmath316 richness bin, which appears to have outliers due to random chance. The clusters in this bin, with the highest @xmath5 values, coincide with the radio matches. However, there is no compelling evidence for point source contamination on these clusters, and follow-up observations with _Chandra_ or _XMM-Newton_ are necessary to clarify the situation. Although the abundance of cool core clusters can increase scatter in X-ray cluster properties, excluding cluster cores in high resolution imaging allows for more consistent correlation between X-ray parameters.\n\n(Note: To maintain the initial style provided by the user, no changes were made to the provided text, except formatting into proper Wikipedia style.) Numerous studies have demonstrated that cool core clusters contribute to increased variance in X-ray cluster characteristics. With the exclusion of cluster centers in high-resolution imaging, many X-ray parameters become more closely related (such as power ratio, core radius, and central entropy). However, in our stacking analysis, the broad ROSAT PSF and the lack of individual X-ray detections for most clusters prevent the exclusion of cluster cores. Nonetheless, several of the brightest and moderately rich cool-core clusters (e.g. Abell 317) are well-known. The vastly widespread and featureless rosenthal (rosat) point spread function (psf), and the lack of individual x-ray detections for the majority of our clusters, present a significant obstacle in differentiating the cluster cores during our stacking analysis. However, some of the most luminous clusters at moderate richness, especially the well-known cool-core clusters, cannot be ignored. As a result, we investigate the influence of cool core clusters on our measurement of the median 21cm line flux (@xmath227) and its scatter (@xmath221) in the 21cm line flux-richness relation. Unfortunately, a comprehensive volume-limited catalog of cool core X-ray clusters suitable for comparison with the MAXBCG catalog does not exist. In fact, @xcite based their estimation of the central cooling time for 55 nearby clusters using Rosat pointed observations of an X-ray flux limited catalog. There is presently no neutral volume catalog that can offer an impartial comparison to the MAXBCG catalog, which contains 8,557 brightest cluster galaxies. In a study led by @xcite, data from the ROSAT pointed observations of an X-ray flux-limited catalog was utilized to estimate the central cooling time of 55 nearby clusters, including two in the MAXBCG catalog (A1689 and A2244) that exhibit characteristics of a cool core. By employing _Chandra_ observations of brighter X-ray clusters from a higher redshift BCS sample, the research team from @xcite identified 7 MAXBCG clusters (A750, A963, A1835, RXJ 2129.6 + 0005, Z2701, Z3146, and Z7160) with cooling times that meet the criteria for cool core clusters. Using _ Chandra _ observations at higher redshifts (redshifts of 3.2 and 3.19), @xcite carried out a systematic assessment of the central cooling times for bright X-ray clusters. From this study, seven MaxBCG clusters (A750, A963, A1835, RXJ 2129.6 + 0005, Z2701, Z3146, and Z7160) with short cooling times below 5 Gyr were identified as cool core clusters. Furthermore, CLG J1504 - 0248, which is also a MaxBCG cluster, was also determined to have a cool core. It should be noted that most of these clusters, despite their moderate richness, are located in the brightest tail of the @xmath323 distribution for their richness. In addition, the central galaxies of these clusters often show strong H\u03b1 emission in SDSS spectroscopy. It should be noted that most of these clusters have a moderate richness and are located in the brightest tail of the distribution for their richness. These central galaxies tend to display a strong H\u03b1 emission in SDSS spectroscopy. However, for a few clusters, the true brightest cluster galaxy (BCG) is not included in the list of cluster members due to a change in their apparent color, making them inconsistent with the red-sequence used in the cluster-finding algorithm. After removing these known cool core clusters that might have biased the M 200 - M \u03b3 relationship, the stacked mean and median relationships are recalculated. When calculating the median relationship with scatter, the normalization and slope shift values are not significantly affected by outliers, however, the intrinsic observed scatter decreases by approximately 0.2 dex, which is a significant shift. It would not be surprising that M 200 decreases, as we are deliberately removing the brightest clusters. Nevertheless, the intrinsic scatter decreases to a value of @xmath324, which is a @xmath325 shift. It is not unexpected that @xmath221 decreases as the brightest clusters are being deliberately removed. When examining the stacked relation, the presence of bright outliers has a slight effect on its normalization. Once known cool core clusters are excluded, the stacked relation's normalization diminishes by @xmath326. Following the previously defined method of converting the stacked mean relation to a scatter-corrected median relation, excluding known cool core clusters causes a decrease in normalization by @xmath209. Consequently, it does not appear that known, exceptionally X-ray luminous, cool core clusters are significantly contributing to any potential bias in our relation. In analyzing the relationship between cluster richness and velocity dispersion using the MAXBCG catalog, removing known cool core clusters does not significantly impact the normalization, as demonstrated by a decrease of approximately @xmath209. This indicates that these cool core clusters, which are particularly luminous in X-rays, do not have a notable effect on the association between richness and velocity dispersion. Our assessment takes into account all X-ray observations for the MAXBCG clusters, regardless of their X-ray morphology. Therefore, it's possible that some portion of the observed scatter is attributed to unidentified cool core clusters present in the sample. Berlind and Weiner (2007) previously measured the median velocity dispersion (\u03c3_v) as a function of richness for the MAXBCG catalog. Thus, it is possible that a portion of the observed scatter in the b07 study's median velocity dispersion as a function of richness may be due to unidentified cool core clusters in the sample. B07's measurements are based on the same richness bins and stacked velocity dispersions as this work, but they used only clusters that have both the brightest cluster galaxy (BCG) and at least one additional member in the Sloan Digital Sky Survey (SDSS) DR5 spectroscopic subsample for their velocity dispersion measurements. This preferentially selects bright and nearby galaxies, and the selection criteria for this subset differ slightly from that for the full MAXBCG catalog. Additionally, the lower redshift clusters in this subset have smaller angular sizes and thus smaller distance uncertainties, which could contribute to some of the observed scatter. To address this, this study replicates their stacking procedure using the known spectroscopic redshifts of the BCGs, rather than photometric redshifts. In this exercise that uses spectroscopic redshifts for BCGs, the spectroscopic subsample of bright and nearby galaxies is selected instead of the full MAXBCG catalog, as the lower redshift clusters in the former case have a smaller redshift than the higher redshift clusters. By running the stacking procedure using the known spectroscopic redshifts, any potential photometric redshift biases that may affect nearby clusters are avoided. Table [Tab: **Lxsig**] shows the mean apparentrichness ($L_{X,Apparent}$) and median true richness ($L_{X,True}$, obtained from B07) for each of the nine richness bins using the spectroscopic subsample. The slope of the $\\Delta L_{X,Apparent}-\\Delta z$ relation is similar between the two samples, but the normalization is lower due to the different redshift selection. The corrected values of $L_{X,Apparent}$, denoted $L_{X,Corr}$, are obtained using the method described in Section [sec: **Scatter***]. CCCCC @X13 $<$ 1.30 $<$ @X14 $<$ 1.30 $<$ @X15 $<$ @X16 $<$ 1.28 $<$ @X17 $<$ 1.26 $<$ @X18 $<$ 1.25 $<$ @X12 $<$ 1.24 $<$ @X19 $<$ 1.24 $<$ 1.23 $<$ @X20 $<$ 1.23 $<$ 1.22 $<$ @X21 $<$ 1.21 $<$ 1.21 $<$ @X22 $<$ 1.20 $<$ 1.19 $<$ @X23 $<$ 1.18 $<$ 1.17 $<$ @X24 $<$ 1.17 $<$ 1.16 $<$ @X25 $<$ 1.16 $<$ 1.15 $<$ @X26 $<$ 1.15 $<$ 1.15 $ The slope of the relation between the quantities @xmath0@xmath1 and those reported in Section [sec : meanrelation] is similar for the spectroscopic subsample. However, the normalization is 0.312 lower due to differences in redshift selection. The scatter correction applied to obtain the mean values of @xmath0 from the dataset is explained in Section [sec : scatter].\n\nFigure [fig : lxsig] depicts the median luminosity-size relation for MaxBCG clusters with a given richness, @xmath352, utilizing data points obtained from the same cluster selection and richness bin. Each solid circle represents a data point. The dark gray band shows the best-fit relation for clusters in the spectroscopic subsample of the MaxBCG catalog, determined using functional form @xmath354: $E(z) = (\\omega-1) / (2\\beta+1) (1 + 1/(D(z)^(1+3\\beta)) - 1/(D(z_0)^(1+3\\beta))$ ($D(z)$ is the dimensionless distance to the cluster at redshift $z$, and $z_0$ is a reference redshift), where $\\beta$ is the slope of the $M-L$ relationship and $\\omega$ is the redshift evolution parameter. The dark gray band represents the correlations derived from the spectroscopic subsample of the maxBCG catalog. It displays the best-fit relation using the following functional form: y = a(x)^b. The self-similar cluster evolution predicts the expected scaling relation, which is characterized by y = b(x)^c, as mentioned in *?*, *?*, *?* and *?*. The observed relation is consistent with this prediction. However, it should be noted that the 0.1-2.4 keV luminosities lead to a shallower relation since clusters with higher velocity dispersions have lower conversion efficiency to X-ray luminosity, as well as higher temperatures and larger gas fractions. A conversion from 0.1-2.4 keV luminosities to bolometric luminosity is not practical for the stacking exercise using ROSAT data. Our analysis reveals that if we could convert the luminosities between 0.1 and 2.4 keV to the equivalent X-ray flux values using astrophysical conversion factors, this would result in a steeper relation due to the fact that clusters with higher velocity dispersions and larger X-ray luminosity to temperature ratios also have a higher temperature. However, for the purposes of this stacking exercise utilizing ROSAT data, this conversion is not practical. The slope of our 0.1-2.4 keV X-ray luminosity to temperature relation is consistent with those measured for Reflex clusters using X-ray calibrated velocity dispersions calculated by cross-correlating the Reflex catalog with the 2dF Galaxy Redshift Survey. Our median relation compares favorably with the results obtained previously for individually selected X-ray clusters by @xcite. In contrast to our current analysis, researchers have previously compared the median relation for individual X-ray selected clusters with the velocity dispersions of reflex clusters obtained through cross-correlation with the Two-Degree Field Galaxy Redshift Survey (2DF) by Hilton et al. (2007, private communication). Illustrated in Figure [fig:lxsig], one can see 39 individual reflex clusters marked as empty circles, displaying their velocity dispersions. The upper-left corner of the figure showcases the typical errors in 0.1-2.4 keV for these reflex/2DF clusters. The comparison between our current analysis and this previous method can provide insight into any discrepancies that exist within the data associated with these different methods. In 2007, through private communication, conversions of velocity dispersions to cluster rest frame were conducted. The upper-left corner depicts empty circles, which represent the typical errors for velocity dispersions between 0.1 and 2.4 keV at redshifts of 5 and 12 for both reflex/2df and x-ray selected clusters. When analyzing the slope of the x-ray selected 5-12 relation, @xcite found a value of @xmath357, which is consistent with observation. However, the individual x-ray selected clusters tend to have slightly higher velocity dispersions of @xmath326 at a given x-ray luminosity in comparison to the stacked maxBCG clusters. This discrepancy may be due to the challenges of accurately measuring velocity dispersions for individual clusters. Individual clusters chosen through X-ray selection have marginally greater velocity dispersions at identical X-ray luminosity than those in aggregated MaxBCG groups, most likely due to challenges in measuring individual values. This issue is exacerbated by the potential for contamination from non-viralized galaxies and interlopers, resulting in overestimated values. By collating data from the _ROSAT All-Sky Survey _ and stacking observations, we can assess the mean X-ray luminosity associated with MaxBCG clusters at variable richness levels. With a large sample size for each interval, X-ray-selected surveys cannot compete in terms of sensitivity and completeness, making the catalog generated from optically selected clusters ideal for measuring the mean X-ray intensity. This catalog is highly pure and complete, making it an invaluable tool for volume-limited studies. By utilizing the data collected through _ROSAT_'s all-sky survey, we measure the mean x-ray luminosity and its correlation with richness for optically-selected clusters from the MAXBCG catalog. With a significant number of clusters represented in each bin, stacking enables us to probe lower flux limits than x-ray selected surveys can. This methodology allows for the construction of a highly-pure and complete volume-limited catalog of clusters selected based on their optical properties. Our results show a straightforward power-law relationship between the mean x-ray luminosity and richness, covering two orders of magnitude. This conclusion is similar to the findings in DKM07, obtained through stacking _ROSAT_'s RASS observations of clusters identified via their properties in the 2MASS catalog. Notably, we also discovered that stacked temperature measurements from _ROSAT_ data are biased for higher temperature clusters. Our findings align with those presented in DKM07, where _ROSAT_ observations of clusters identified in 2MASS were stacked. However, when stacking temperature measurements from _ROSAT_, we found significant biases for higher temperature clusters. As a result, we restricted our analysis to luminosities calculated in the 0.1-2.4 keV band, rather than extrapolating to calculate bolometric luminosities. Furthermore, we showed that the @Xmath108-model fits to stacked radial profiles of moderate redshift clusters are primarily influenced by the broad _ROSAT_ point spread function (PSF) and the offset distribution between MaxBCG clusters and correlated X-ray sources. Conversely, calculating X-ray luminosity in the 0.1-2.4 keV band, which primarily relies on counting photons, is not strongly biased by spectral fits or profile fits. Because the @Xmath5 scaling factor with cluster density squared, as long as the core of a cluster is within the stacking aperture, a high reliability estimate of the mean cluster luminosity can be obtained. By measuring the X-ray flux at the positions of individual MaxBCG clusters, we are able to constrain the observed scatter in the @Xmath5-@Xmath6 relation. Assuming a log-normal distribution of @Xmath5 as a function of @Xmath6, we found a scatter of @Xmath241, which is quite large and necessitates a correction for this variability when calculating the median of the underlying distribution. The richness measure we used in this work, @Xmath6, is simply the count of red-sequence galaxies within a scaled @Xmath37 aperture, but it remains a good proxy for @Xmath5 on average. The significant scatter in the relationship between richness and X-ray luminosity necessitates the correction of this scatter in calculating the median of the underlying distribution. Specifically, the richness measure employed in current work, which is based on the count of red-sequence galaxies within a certain scaled aperture, is a valuable proxy for the X-ray luminosity. However, this measurement can be improved by incorporating information about the brightest cluster galaxies' (BCGs' ) luminosity and controlling variations in cluster membership with redshift. The large scatter in this relationship causes significant effects, particularly in the selection of X-ray clusters. X-ray selection with high flux limits, as seen in the Noras catalog, tends to favor the clusters with the highest X-ray luminosity at a specific richness. The substantial dispersion in the correlation between cluster X-ray richness and luminosity has noticeable consequences for the selection process. When selecting clusters based on X-ray flux with a high threshold, such as the NORAS catalog, it prioritizes the clusters with the highest X-ray luminosity at a specific richness. Conversely, the large scatter in the X-ray luminosity-richness relationship suggests that many optically selected clusters appear relatively underluminous compared to their X-ray counterparts with similar richness. In the pursuit of identifying clusters with a low scatter proxy for halo mass, a low dispersion metric for cluster selection is critical. In optically selected clusters, they generally appear less luminous compared to x-ray selected clusters with a similar degree of richness. This is because optical cluster selection techniques have a higher scatter in terms of determining halo mass, as opposed to x-ray cluster selection techniques, which are limited to finding only the brightest and nearest clusters (Roche et al. 2012). X-ray flux limited surveys can identify these clusters based on their observable x-ray emission, but they fail to reveal fainter clusters that are located at greater distances or which have lower luminosities. Optical surveys, on the other hand, often suffer from a large scatter between optical richness and x-ray luminosity and mass (Bahcall et al. 2007). Limited x-ray flux surveys can only detect the most luminous and closest x-ray clusters, constraining their scope to the brightest and closest clusters. This limitation in identification methods can result in substantial discrepancies between x-ray luminosity and halo mass, as outlined in B07. Optical surveys such as MAXBCG can offer volume-limited observations up to intermediate redshifts. However, there is notable scatter between optical richness and x-ray luminosity and mass, as demonstrated in B07 as well. The use of more precise mass proxies like @Xmath204, the Compton y parameter, and potentially more intricate optical richness estimations may aid in the creation of a catalog more closely resembling the mass-limited catalog we desire. This analysis, in conjunction with B07's velocity dispersion measurements, allows for the measurement of the median @Xmath227@Xmath327 relation for the MAXBCG clusters. The slope is consistent with the prediction of self-similar cluster development, at @Xmath359. Studying the median correlation between the fundamental plane parameters, logarithm of the central surface brightness (@xmath227) and the effective radius of the galaxy population within the effective radius (@xmath327), across a volume-limited catalog of maxbcg clusters, reveals a slope of 1.09. This result is in accordance with the prediction of self-similar cluster evolution. Previous measurements of this correlation had complications due to selection effects and large scatter, affecting the derived slope. By stacking clusters from a volume-limited catalog and correcting for scatter, both logarithm of the central surface brightness and effective radius are much better constrained, leading to a robust calculation of the correlation. Our analysis provides no evidence of a break in the correlation at the poor cluster (group) scale, as was previously suggested by some studies, such as E. D. Jorgensen, A. Muchovej, and H. F. Rix's \"The Fundamental Plane and the Density of Galaxy Clusters\" (1999). The constraints on the relation between mass and temperature in clusters and groups are better defined, allowing a more robust calculation of this relationship. In our analysis, we do not find any evidence of a break in this relation at the poor cluster or group scale, contrary to previous suggestions (* ? ? ?) where a break was hinted at. This lack of evidence may be due to the fact that the MAXBCG cluster finder mainly identifies red-sequence clusters and does not find groups with a high blue galaxy fraction. We can also compare the mass estimates based on this method with those determined via weak lensing from the same MAXBCG cluster catalog (*S07, J07*). Understanding the mass-temperature relation is crucial not only for improving our understanding of cluster physics but also for calibrating the selection function of X-ray clusters as cosmological probes (* ? ? ?*). Additionally, a comparison of weak lensing masses determined from the same maxbcg cluster catalog (s07, j07) with the @xmath0 as determined from this method is important for not only understanding cluster physics, but also for calibrating the selection function of x-ray clusters used as cosmological probes. This measurement has been made in a companion letter (citation not provided). Furthermore, the comparison of x-ray luminosities, weak lensing masses, and velocity dispersions in conjunction with the number function of maxbcg clusters can be employed to constrain cosmological parameters (citation not provided). This ongoing work aims to provide insights into the relationship between these parameters and to establish a better understanding of the properties of galaxy clusters in the cosmos. The joint analysis of X-ray luminosities, weak lensing masses, and velocity dispersions, in conjunction with the number function of MAXBCG clusters, can provide insights in constraining cosmological parameters. Ongoing work involves refining the measurements of scatter, point source contamination, and catalog purity by employing deeper pointed X-ray observations such as those from ROSAT/PSPC as well as XMM/Newton and Chandra. This endeavor aims to obtain more precise results. Currently, scientific investigations are focusing on unearthing serendipitous MAXBCG cluster observations through deeper and pointed rosat/PSSC observations, as well as xmm/Newton and Chandra observations, especially for those clusters that have not been previously known to be X-ray bright. Due to the commonness of relatively poor clusters, a targeted campaign of MAXBCG clusters that have not yet been identified as X-ray bright will be crucial to testing the measurement of scatter in mass-to-X-ray flux and estimating the fraction of cluster flux that is subject to contamination by point sources. Such data would be instrumental in enabling future inexpensive deep optical surveys, such as the DES, to use optical properties of clusters to identify and prioritize clusters for targeted X-ray observations. A campaign targeting a subset of MAXBCG clusters, specifically those that have not yet been identified as X-ray bright, will prove crucial in testing our measurement of the scatter in the X-ray luminosities, as well as estimating the fraction of X-ray flux that arises from point sources. This information is crucial for upcoming inexpensive deep optical cluster surveys, such as DES, to accurately gauge which clusters would be most beneficial to further investigate with targeted X-ray observations. Authors E. Rykoff and T. McKay would like to acknowledge financial support from NSF grants AST-0206277 and AST-0407061, as well as the hospitality of the Michigan Center for Theoretical Physics. M. Rykoff acknowledges support from the Michigan Space Grant Consortium, and A. E. Ettori acknowledges support from the NSF grant AST-0708150. mrb acknowledges support from the Michigan Space Grant Consortium, whereas the Astrobiology magazine (AEE) received funding from the National Science Foundation's Astrophysics and Space Science grant program via grant number ast-0708150. Additionally, the anonymous referee provided helpful comments on this research. The aforementioned work received funding from various entities such as the Alfred P. Sloan Foundation, the participating institutions, the National Science Foundation, the U.S. Department of Energy, the National Aeronautics and Space Administration, the Japanese Monbukagakusho, the Max Planck Society, and the Higher Education Funding Council for England, which were instrumental in the successful completion of the SDSS and SDSS-II. Funding for the Sloan Digital Sky Survey (SDSS) and its successor, SDSS-II, has been provided by multiple organizations. These include the Alfred P. Sloan Foundation, participating institutions, the National Science Foundation, the U.S. Department of Energy, the National Aeronautics and Space Administration, the Japanese Monbukagakusho, the Max Planck Society, and the Higher Education Funding Council for England. The SDSS website can be found at http://www.sdss.org/. The SDSS is managed by the Astrophysical Research Consortium on behalf of the participating institutions, which include the American Museum of Natural History, Astrophysical Institute Potsdam, University of Basel, University of Cambridge, Case Western Reserve University, University of Chicago, Drexel University, Fermilab, the Institute for Advanced Study, the Japan Participation Group, Johns Hopkins University, the Joint Institute for Nuclear Astrophysics, the Kavli Institute for Particle Astrophysics and Cosmology, the Korean Scientist Group, LAMOST of the Chinese Academy of Sciences, Los Alamos National Laboratory, the Max-Planck-Institute for Astronomy and Astrophysics, New Mexico State University, Ohio State University, University of Pittsburgh, University of Portsmouth, Princeton University, the United States Naval Observatory, and the University of Washington.\n\nThe text material from which this originated can be found in E. et al. (1987) and was presented at the Society of Photo-Optical Instrumentation Engineers (SPIE) Conference, vol. The participating institutions for the presentation at the Society of Photo-Optical Instrumentation Engineers (SPIE) Conference in 1987 on \"Soft X-Ray Optics and Technology\" were the American Museum of Natural History, Astrophysical Institute Potsdam, University of Basel, University of Cambridge, Case Western Reserve University, University of Chicago, Drexel University, FermiLab, The Institute for Advanced Study, the Japanese Participation Group, Johns Hopkins University, the Joint Institute for Nuclear Astrophysics, the Kavli Institute for Particle Astrophysics and Cosmology, the Korean Scientist Group, the Chinese Academy of Sciences (LAMOST), Los Alamos National Laboratory, the Max-Planck-Institute for Astronomy (MPIA), the Max-Planck-Institute for Astrophysics (MPA), New Mexico State University, Ohio State University, University of Pittsburgh, University of Portsmouth, Princeton University, the United States Naval Observatory, and the University of Washington. This information was cited in E. et al. (1987), presented at the SPIE conference in Berlin, Germany in December 1986, and published in the conference series volume, p. 519 (Koch & Schmahl, 2001). In 1986, from September 8th to 11th, the Society of Photo-Optical Instrumentation Engineers (SPIE) hosted a conference in Bellingham, Washington, that led to Volume 733 of the organization's publications in 1987. The specific paper mentioned in this reference is \"Astronomical Image Processing and Its Future Applications\" authored by Koch and Schmahl, with page numbers ranging from 519. This paper was later included in the Astronomical Society of the Pacific Conference Series' 2001 edition titled \"Virtual Observatories of the Future\" and can be found on page 234.",
        "abstract": " determining the scaling relations between galaxy cluster observables requires large samples of uniformly observed clusters . \n we measure the mean x - ray luminosity  optical richness ( @xmath0@xmath1 ) relation for an approximately volume - limited sample of more than 17,000 optically - selected clusters from the maxbcg catalog spanning the redshift range @xmath2 . by stacking the x - ray emission from many clusters using _ rosat _ all - sky survey data , \n we are able to measure mean x - ray luminosities to @xmath310% ( including systematic errors ) for clusters in nine independent optical richness bins . in addition \n , we are able to crudely measure individual x - ray emission from @xmath4 of the richest clusters . assuming a log - normal form for the scatter in the @xmath5@xmath6 relation , we measure @xmath7 at fixed @xmath6 . \n this scatter is large enough to significantly bias the mean stacked relation . \n the corrected median relation can be parameterized by @xmath8 , where @xmath9 and @xmath10 . \n we find that x - ray selected clusters are significantly brighter than optically - selected clusters at a given optical richness . \n this selection bias explains the apparently x - ray underluminous nature of optically - selected cluster catalogs . ",
        "section_names": "introduction\ninput data\nx-ray analysis\nmean @xmath0@xmath1 relation\nbiases in the @xmath227@xmath1 relation\nthe luminosityvelocity dispersion relation (@xmath227@xmath327)\nsummary"
    },
    {
        "article": "While water is a crucial component for supporting life on Earth and a fundamental element in various chemical reactions and biological processes, it also poses environmental challenges. In several parts of the world, water scarcity and pollution have become major global issues, affecting the health, economic prospects, and societal wellbeing of communities. Droughts and floods have caused massive displacements, while agricultural production, economic growth, and quality of life have significantly declined. These issues highlight the need for sustainable water management practices that balance the economic, social, and environmental aspects of water use. To address these concerns, several strategies and approaches have been proposed, including increased investment in water infrastructure, improved water pricing, enhanced public-private partnerships, and policies that promote conservation, efficiency, and the efficient allocation of water resources. Ultimately, effective water management requires a comprehensive understanding of the environmental, economic, and social factors that influence water availability, quality, and accessibility. Notably, in optics, a crucial instance is the high intensity of light beyond the single photon regime, which can be attained through the operation of an optical parametric oscillator (OPO), a device that uses a laser beam input and a quadratic crystal to generate squeezing and entanglement in light modes, particularly characterized by continuous spectral features. In these devices, light modes can be distinguished based on their polarization or frequency, and this property has led to the realization of various optical applications utilizing multimode light, such as optical switching, quantum imaging, metrology, and quantum information. The use of spatial inhomogeneities in broad area devices presents an exciting potential in controlling quantum features of light. This phenomenon is similar to photonic band gaps in linear or nonlinear classical optics, where periodic modulation of the refractive index can produce a gap in the allowed frequencies of the electromagnetic field. An alluring prospect is to harness spatial inhomogeneities in broad area devices for manipulating quantum properties of light. In the domains of linear and nonlinear classical optics, the periodic modulation of the refractive index creates gaps in the allowed frequencies of electromagnetic fields, a phenomenon referred to as photonic band gaps  @xcite. These engineered media, known as photonic crystals (PCs), enable the confinement and guidance of light, leading to myriad applications  @xcite. It has also been predicted that implementing a transverse modulation of the refractive index in nonlinear cavities can curb modulation instabilities at similar wavelengths  @xcite. This prediction was subsequently validated experimentally  @xcite, and an associated phenomenon emerged in semiconductor microcavities  @xcite. In nonlinear cavities, the transverse modulation of the refractive index is predicted to prevent modulation instabilities at similar wavelengths, as demonstrated in recent experimental studies. This phenomenon, initially proposed in semiconductor microcavities, is linked to the emergence of nonlinear structures in dissipative systems, including the formation of discrete cavity solitons. Photonic crystals, which have been proposed for environment engineering, have played a significant role in the inhibition of spontaneous emission in their passband gaps, as suggested in seminal papers. The use of photonic crystals in cavity QED has become a research focus based on the presence of photonic band gaps. Moreover, the role of photonic crystals (PCs) in quantum optics has been significant in recent years. The suppression of spontaneous emission is possible in the PC band due to the presence of photonic band gaps, as discussed in multiple seminal papers @xcite. This idea has been implemented in various experiments to study cavity quantum electrodynamics (cQED) and to investigate non-Markovian effects in structured reservoirs @xcite, exploring the impacts on decoherence dynamics and entanglement decay. The current work demonstrates the effects of a transversal modulation on quantum fluctuations and correlations in a nonlinear device containing a photonic crystal optical parametric oscillator (PCOPO) with a PC in the cavity, as described in @xcite. In reference, the PC effect on modulation instability was previously studied and compared with homogeneous systems @xcite. In this study, we examine the impact of transversal modulation, specifically on quantum fluctuations and correlations in a nonlinear device with photonic crystal optical parametric oscillators (PCOPOs). A multimode degenerate OPO with a PC in the cavity, as previously investigated in references @xcite and @xcite, was considered. Our presented work, which is based on numerical analysis, expands on the prior study in reference @xcite, where quantum fluctuations were analyzed using a Langevin approach valid for both regimes above and below threshold. In this paper, we present analytical results valid below the parametric threshold based on linear and few-mode approximations that demonstrate excellent agreement with simulations of the full model. By investigating intensity fluctuations, correlations, and quadrature squeezing, as well as entanglement, we uncovered the effect of modulation on these quantities. In a recent study, researchers demonstrated the ability to manipulate quantum correlations using a computer, resulting in reduced noise in field quadratures and increased entanglement above the threshold level. Analytical results were derived based on linear and few-mode approximations, valid below the parametric threshold and consistent with numerical simulations of the full model. The impact of modulation on intensity fluctuations, correlations, quadrature squeezing, and entanglement were calculated, revealing the influence of this homogeneous multimode OPO, which can generate squeezing, entanglement, and twin-beam correlations between spatial modes below threshold. Similar effects have been predicted in Kerr media and in second harmonic generation, with several successful experimental realizations. The effects of modulation on a homogeneous multimode OPO can also be generalized to other nonlinear devices that incorporate intracavity PCS. In recent years, similar effects have been predicted in Kerr media, such as second harmonic generation, following successful experimental realizations. The paper discusses the effects of a spatial modulation in an optical parametric oscillator, which can also be generalized to modified nonlinear devices containing intracavity phase conjugation. The analysis begins by presenting a model of a pcopo using linear and a few modes approximation for light fluctuations below threshold. The output signal field is expressed in terms of the input field, and a set of non-linear Langevin equations is introduced. In Section [ sec : model ], these equations are numerically simulated to test the model's validity. In this article, we present a pcopo model based on linear and few modes approximations to describe the fluctuations of light below threshold. Through this rephrasing, we highlight that our proposed model allows us to obtain the output signal field in terms of the input field. Additionally, we introduce a set of non-linear Langevin equations, which we will numerically simulate to test the approximations.\n\nIn section [ sec : threshold ], we obtain an analytical expression for the intensities of the signal field, showcasing that the instability threshold for signal emission can be either raised or lowered due to spatial modulation of the PC (phase grating) input.\n\nIn section [ sec : qcorr ], we proceed to derive expressions for various quantum correlations, including squeezing, entanglement, and twin-beam correlations. Lastly, the article culminates with a brief summary of our findings, in the section regarding our conclusions. Subsequently, in Section [sec : qcorr], we acquire expressions for distinct quantum correlations such as squeezing, entanglement, and twin beams correlations. The concluding segment entails our final thoughts. The present investigation centers around a two-dimensional cavity that contains a nonlinear medium at @xmath0, with one of the mirrors partially reflecting. The pump field oscillating at @xmath1 frequency is transformed into a signal field oscillating at @xmath2 frequency with an orthogonal polarization. This configuration is akin to a Type I degenerate OPO. In the pump field at frequency $\\omega_1$, the signal is generated at frequency $\\omega_2$ with perpendicular polarization, thereby implementing a type I degenerate optical parametric oscillator (OPO). The input beam, a plane wave in the z direction, has an amplitude of $\\sqrt{I_3}$, assumed real. This article discusses the influence of the medium's transversal inhomogeneity filling the cavity, realized by incorporating a planar photonic crystal (PC) with refractive index modulation in the plane perpendicular to the light propagation direction. Figure 1 from Ref provides a depiction of the setup. Correspondingly, the introduction of a planar photonic crystal with refractive index modulation in the transverse plane perpendicular to light propagation direction can be represented with a sketch provided in Fig.1 of Ref. @xcite. This structure exhibits intracavity dynamics that can be described using a continuous set of bosonic spatial modes at frequencies @xmath5 and @xmath6, which obey equal time commutation relations presented in Eq. \\ref{eq : conmutation}. The hamiltonian operator for this photonic crystal cavity reading\n\n\\begin{aligned}\\label{eq : h }\nH=\\underbrace{\\sum_{i}(\\frac{\\hbar^2}{2m_i}\\nabla_i^2)}_{\\textbf{diffraction of fields in the cavity}}+ \\underbrace{ie\\left(\\hat{a}_{0}^{\\dagger}-\\hat{a}_{0}\\right)}_{\\textbf{radiative losses}}+ \\nonumber\\\\\n\\underbrace{i \\frac{g}{2}\\left(\\hat{a}_{0}\\hat{a}_{1}^{\\dagger2}- \\mathrm{h.c.}\\right)}_{\\textbf{coupling to the bus waveguide}}\n\\end{aligned}\n\nwith the first term describing diffraction of the fields in the cavity, and diffraction strengths being represented by the Laplacian in the transverse plane, @xmath9. The second term indicates the radiative losses, and the third term corresponds to the coupling to the bus waveguide. Within the framework of a pcopo (passively Q-switched optical parametric oscillator), the intracavity dynamics can be described in terms of a set of boson spatial modes denoted by $a_{\\mathbf{x},n}$ at frequencies $\\omega_n$ and $\\omega_{n+1}$, where $n$ is the mode number in an array of transverse modes with cutoff radius $r_c$. The operators for each mode follow equal time commutation relations given by $\\left[a_{\\mathbf{x},n},a_{\\mathbf{x}'}^{\\dagger,n+1}\\right] = \\delta_{ij}\\delta(\\mathbf{x}'-\\mathbf{x}).$ The Hamiltonian operator reads \n\n$H = H_0 + H_P - H_L + H_NL$,\n\nwhere $H_0$ accounts for diffraction of fields in the cavity, with $H_0 = \\sum_{\\mathbf{x},n} \\left[ -\\frac{\\hbar^2}{2m\\omega_0^2}{\\nabla_\\mathbf{x}}^2 a_{\\mathbf{x},n}^{\\dagger} a_{\\mathbf{x},n} + \\omega_n a_{\\mathbf{x},n}^{\\dagger} a_{\\mathbf{x},n} \\right]$. Here, the first term contains the laplacian operator, and $m$ and $\\omega_0$ are the effective mass and central frequency of the modes, respectively. The second term accounts for the interaction of the system with an external pump, $H_P = i \\left[ e g^2(a_{0,0}a_{0,1}^2 + a_{0,0}^{\\dagger}a_{0,1}^{2 \\dagger}) \\right]$.  The third term describes the diffractive loss with a damping rate proportional to the coefficient $\\kappa$, $H_L =  \\sum_{\\mathbf{x},n} \\left[ - \\frac{\\kappa}{2} a_{\\mathbf{x},n}^{\\dagger} a_{\\mathbf{x},n} \\right]$. The fourth term encapsulates the nonlinear interaction between both modes, where the coupling constant $g$ is proportional to the second-order susceptibility. The main difference between a pcopo and a generic OPO is that the spatial modulation of the photonic crystal introduced in the former leads to In a planar-capillary optical parametric oscillator (PCOPO), the coefficient @xmath12, representing the cavity damping rate, is employed as a scaling factor for convenience. Unlike a generic optical parametric amplifier (OPO), a PCOPO has a photonic crystal in the cavity that introduces a spatial modulation of detunings @xmath13. This breaks the translational symmetry of the system, leading to significant effects on macroscopic fields dynamics and correlations between fluctuations. In a reference @xcite, numerical results on quantum effects below and above threshold were reported based on simulation of quantum fields dynamics in the Heisenberg representation. The methodological approach employed in this description, as discussed in ref. @xcite, considers the full nonlinear dynamics but is not amenable to analytical calculations. In the following, we introduce a simplified and approximate model to achieve analytical results below the instability threshold. \n<|>\nIn a planar-capillary optical parametric oscillator (PCOPO), the damping rate @xmath12, represented by a coefficient, serves as a scaling factor. Contrary to a standard optical parametric amplifier (OPO), a PCOOPO features a photonic crystal in the cavity, generating a spatial modulation of detunings @xmath13. This ruptures the translational symmetry of the system, consequently resulting in substantial repercussions on macroscopic field dynamics and fluctuations' correlation. This concept was proposed and studied in a prior publication @xcite, and numerical results on quantum effects, both above and below threshold, were reported based on quantum field dynamics simulations in the Heisenberg representation. However, the full nonlinear dynamics approach does not allow for analytical calculations. To remedy these issues, we introduce an approximate simplification model that leads to analytical results below the instability threshold. In previous research, the full nonlinear dynamics were discussed, but this approach became challenging for analytical calculations. In this article, we propose a simplified and approximate model that allows for analytical results below the instability threshold. The intracavity field operators follow the Heisenberg equation, accounting for incoming quantum fluctuations in an open system with dissipation. The dynamic equations for operator moments, formed from the Hamiltonian with a cubic interaction, lead to an unsuitable infinite hierarchy for analytical handling. However, a commonly used approach is to linearize the dynamical evolution for quantum fluctuations around a macroscopic steady state, governed by a Hamiltonian, which provides a _ hamiltonian  @xcite . The process of approximating the nonlinear terms in Heisenberg equations as products of first-order moments is known as linearization around a macroscopic steady state. This technique leads to a Hamiltonian dynamic evolution for quantum fluctuations. To perform this approach, field operators are identified by their reference average values and small fluctuations around them, represented by @xmath19. The reference values are derived from averaging the Heisenberg equations and disregarding nonlinear terms. This procedure yields two classical equations whose steady state depends on the considered PCO. In the context of Xmath18, the reference values correspond to the expectation values of variables as explained in Xmath20. To obtain their evolution, the Heisenberg equations [Xmath21] are averaged while nonlinear terms are approximated as products of first-order moments. This results in two classical equations whose steady state primarily depends on the regime under consideration. As seen in pump values Xmath3, if the pcopo is below the instability threshold, the signal operator expectation value vanishes, regardless of the presence of the PC. The equation for the average value of the pump field under this regime is reduced to [Xmath22], where we have introduced the scaling factors Xmath23 and Xmath24. These scaling factors and field variable scaling, as described in @Xcite, will be used throughout this article. In the average value regime, the equation for the pump field simplifies to $\\bar{E}(x,t) = \\frac{1}{L}\\int_{0}^{L}E(x',t)dx'$, where we have introduced the scalings $\\bar{A}(t) = \\frac{1}{L}\\int_{0}^{L}\\left|E(x,t)\\right|dx$ and $\\lambda(t) = \\frac{1}{L}\\int_{0}^{L}\\left|\\chi(x,t)\\right|P(x,t)dx$. These scalings continue to feature throughout this article, along with the field variable scalings mentioned in reference [1]. For brevity, we omit the primes from here on.\n\nFor a homogeneous detuning, the steady state solution is trivially found in the case of a homogeneous or partially coherent pump (PCOPO with modulation in signal detuning only) as $\\bar{E}(x,t) \\propto \\bar{A}(t)$. However, a non-homogeneous pump detuning requires further analysis to determine the stationary state. In the case where the steady-state solution is readily determined, as in @xmath25, it is notable that for an OPO or a PCOPO with modulation only in the signal detuning, the steady-state is homogeneous (@xmath26). However, in the case of a non-homogeneous pump detuning, identifying the stationary state is generally more complex. To simplify this analysis, we will consider a single transverse dimension and a PCOPO with sinusoidal modulation such that, @xmath27, where @xmath28 denotes the pc wave number. The steady-state satisfies a set of coupled mode equations, depending on the variable @xmath30, in the Fourier space. In this analysis, we restrict ourselves to a single transverse dimension and employ a sinusoidal modulation with wavelength corresponding to the periodicity of the pump wave. Such a setup leads to a steady-state solution that follows the Fourier decomposition, as expressed in the equation:\n\n@math27\n\nwhere @xmath28 represents the pump wave number. This, in turn, instigates a system of coupled mode equations, which exhibit variability with respect to the propagation distance @xmath30. A critical assumption that justifies our analysis, which is discussed in Section [sec], is the neglect of terms with @xmath31 . By doing so, we arrive at a resultant pump field possessing three non-zero modes, as illustrated by:\n\n@math32\n\nFor simplicity, we assume that only one mode satisfies the phase-matching condition, designated as @xmath34, without loss of generality. Starting with the given content, the paragraph is rephrased as:\n\nIn this system, we have three non-vanishing modes for the pump field at xmath32, with xmath33, where we assume xmath34 without loss of generality. The steady states for the pump and signal fields are taken as a reference state with fluctuation operators xmath36 defined around them. By applying standard procedures, the exact Hamiltonian is approximated to a quadratic form in the fluctuations. After decoupling the pump and signal Heisenberg equations, we obtain a dynamical equation for the signal fluctuations: xmath37. For the case where the perturbation primarily affects the signal field, denoted by xmath38 and xmath39, and assuming no back-action from the signal on the pump, shown in (b). Alternatively, when the perturbation affects both fields, demonstrated in (c), similar to the case of back-action from the signal on the pump presented in (d) for xmath44 and xmath45. The dynamics of system in consideration involves decoupled pump and signal Heisenberg equations, resulting in a dynamical equation for signal fluctuations. Similar to the case with the PC affecting solely the signal [math](b)[/math], Fig. 1 displays the situation represented for the parameters @mathx38, @mathx39, and @mathx40. For the situation where the PC affects both fields [math](d)[/math], represented for the parameters @mathx44 and @mathx45, it closely resembles [math](c)[/math] with @mathx41, @mathx42, and @mathx43. The homogeneous component is always present and harmonics appear for both pump and signal at @mathx46 and @mathx47, respectively. Here, we have dropped the hat notation for convenience and denote the signal fluctuations as @mathx50. In order to streamline the subsequent discussion, we assume that the external pump is smaller than its corresponding threshold as shown in Figure 1. Further, to simplify notation, we drop the hats of the operators and denote the perturbation as described in the text. The modulation on the signal is similar to that of the pump. However, it should be noted that the amplitudes of both modulations are not necessarily equal. The presence of the phase conjugator leads to a coupling between different (k wave vector or ) modes because of the spatial modulation of both the signal's detuning (\u03a9) and the pump's one through the spatial harmonics. In contrast to evolving independently, different modes are dynamically coupled through the spatial modulation of both the signal detuning and the pump one via spatial harmonics, as highlighted in eq. ([eq: enfourier]). Neglecting higher harmonics is equivalent to considering only a specific mode in eq. ([eq: enfourier]). However, eq. ([eq: enfourier]) also reveals the dynamical coupling between different modes, as further explored in the accompanying material. \"Moreover, Equation [eq : enfourier] exposes the intricate coupling of 59 distinct modes, as previously discussed in Appendix [sec : bigl]. However, to handle this model analytically, further considerations are necessary. This prompts us to investigate the complete PCOP model to identify the relevant spatial modes in various regimes. The full dynamics can be calculated by solving the Langevin equations numerically, derived by mapping the master equation for the PCOP system, whose Hamiltonian is provided in Eq. To highlight.\" To delve deeper into the full pcopo model and identify the most significant spatial modes in various regimes, we turn our attention to calculating the full dynamics through numerical simulation of Langevin equations \ud835\ude83. These equations are obtained by mapping the master equation for the pcopo, whose system Hamiltonian is provided by Eq. [Eq. : Hamiltonian], onto an equation of motion for the Husimi quasi-probability distribution \ud835\ude80\ud835\ude84 at xcite . By doing so, the representation is converted into a functional of the population number fields \ud835\ude80\ud835\ude81 at xcite . \n\nIn regimes where pump intensities are not extremely high, the Husimi distribution q dynamics is controlled by a Fokker-Planck equation. By mapping this equation, we arrive at the following nonlinear Langevin equations for spatially resolved pumps \u03b1\u2080(x,t) and signal fields \ud835\udc3c(x,t) \ud835\ude83 :\ufffd\ufffd\ufffd\ufffd\ufffd(x,t) = -[\u03b3(x)*\ud835\udc3c(x,t) - I(x,t)] + \u03ba(x)*\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd(x,t) + \u2016\u03ba(x)\u2016^2/2*\u03b7(x,t), and\n\n\u0394\ud835\udc3c(x,t) = -[\u03b3(x)*\ud835\udc3c(x,t) + \u03a3x'I(x',t)*\u03ba(x')\u03b3(x)/{P[x'][1+\ufffd\ufffd\ufffd(x',t)*\u03ba(x')/\u03ba(x)]}] + \u2211x'\u03ba(x')\u03b3(x)*\u0394\ud835\udc3c(x',t) + \u2211x'\u03a3z\u2321\u03a3r\u03ba(x')\u03b3(x')*\ud835\udefc(z-x',r)*\ud835\udefc(z,r)*\u03b7*(x-x',t;r), where * represents spatial convolution. Here, \u03b3(x) represents the homogeneous linewidth, P[x'] is the transverse-spatial mode profile function, the term \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd(x,t) is the spontaneous emission rate, and \u03ba(x) the coupling strength. The other parameters in these equations represent signal In regimes where the pump intensity is not too high, the evolution of the Husimi distribution is governed by a Fokker-Planck equation and can be mapped onto nonlinear Langevin equations for locally varying pump and signal fields. These equations are given by:\n\n\\begin{aligned}\n\\dot{\\alpha_0}(x,t) &= e - \\frac{1}{2} \\alpha_1^2(x,t) + \\xi_0(x,t) \\\\\n\\partial_t \\alpha_1(x,t) &= -(1+i\\delta_1(x))-2i\\nabla^2\\alpha_1(x,t) + \\alpha_0(x,t) \\alpha_1^*(x,t) + \\xi_1(x,t).\n\\end{aligned}\n\nThe white noises, which are both additive (denoted by $\\xi_0(x,t)$) and phase-sensitive (denoted by $\\xi_1(x,t)$) are included in the equations. The local variation in the detunings $\\delta_1(x)$ and $\\delta_2(x)$ is a consequence of the influence of passive components (PC) in the system. These nonlinear Langevin equations were simulated numerically for technical details of the numerical methods can be found in a recent publication. In the simulation of the system, it is apparent that the effects of the PC component further impact the spatial dependence of the detunings at positions @xmath68 and @xmath69. The dynamics of the system are investigated numerically, utilizing methods outlined in Ref @xcite. For the OPO without PC, a modulation instability arises as the signal detuning is negative, surpassing the parametric threshold @xcite, with a critical wavenumber @xmath70. When operating below threshold, the pump remains homogenous, while noisy precursors exist at the critical wave number @xmath70. These precursors exhibit an incrementing average intensity as the system approaches the threshold @xcite. In the context of optics, it is known that when signal detuning is negative, a modulation instability occurs above a parametric threshold, denoted by @xcite. This critical wavenumber, represented as @xmath70, satisfies the parameter conditions, with @xmath71 equal to @xmath72. Below this threshold, homogeneous pump fields are present, and noisy precursors are detected at the critical wave-number. In Fig. [fig1]a, we demonstrate the homogeneous and critical modes' average pump and signal far fields in an OPO. Moving forward, we focus on a PCPOPO with a band-gap at the critical wave-number of the OPO, represented as @xmath73, for @xmath39 and @xmath38. We observe identical average far fields to the OPO's when we only modulate the signal. However, when we introduce a modulation to the pump detuning, @xmath75, multiple even harmonics, represented as @xmath76, develop in the pump field, as displayed in Fig. In this article, we explore the properties of the pcopo, particularly for the case where the critical wave number of the oscillation at the gap frequency is relevant. Initially, we focus on situations where the pump and signal waves are modulated at fixed frequencies, denoted as @xmath39 and @xmath38. The pcopo exhibits average far fields that are similar to those of opo-based systems. However, when we introduce modulation in the pump detuning, specifically @xmath75, the pump contains multiple even harmonics, as shown in Fig. [fig1]c and d. Interestingly, the signal's average far field intensity remains unchanged regardless of the modulation. In this section, we delve into the analytical evolution of the correlation function using the equations provided in the language [eq: lang]. Moving forward, we present significant variations in signal fluctuations resulting from numerical simulations of the pertinent equations. Prior to delving into the analytical progression of the correlation, we present a notable feature of the signal fluctuations as obtained from simulations of eqs. Below threshold, noisy precursors, commonly referred to as quantum images, dominate the dynamics, as displayed in Figure 2(a). The \"spatial periodicity\" present during this phase corresponds to the critical wave number. In contrast, above the phase-change boundary, the phase of the noisy pattern diffuses in space in the @xmath77 direction, driven by the goldstone mode, as elucidated in Ref. A significant alteration in behavior occurs when the phase-coherent state breaks translational symmetry, leading to a modulation of the pump, as illustrated in Figure 2(b), denoted as @xmath78. Figure 2a illustrates the preferred \"spatial periodicity that corresponds to the critical wave-number. This pattern does not have a fixed phase and its diffusion in space primarily relies on the goldstone mode, as explained in reference a. Conversely, when the pattern breaks the translational symmetry, leading to the modulation of the pump, a spatially locked noisy pattern emerges in the signal with two dephased modulated modes dominating the pattern as shown in Figure 3. In this context, the consequences on correlations will be analyzed in the following sections. Based on Figure 1, it is clear that the aforementioned assumptions outlined in Equations 5-7 [cf. Eqs. (5-7)] are pertinent.\n<|>\n The following section explores the implications of correlations arising from our analysis. From Figure [fig1], we observe that the assumption made in equations (5-7) is merited, as the higher-order harmonics at @xmath81 are significantly smaller than those at @xmath82. This allows for neglecting contributions from terms with @xmath31, or modulations below the pc wavelength. In contrast, the introduction of the modulation does not affect the signal's primary components, which primarily consist of the modes at the critical wave number @xmath83. We restrict our analysis to the modes at @xmath84 in equation. In the initial stage of investigation, it is noteworthy that the introduction of modulation does not have any impact on the primary components of the signal, specifically the modes at the critical wave numbers set by \ufffd\ufffd\ufffd (\u03c9_c) = \u03c0m/L, where m = 1,2,... (see Equation [eq: enfourier]). Consequently, we restrict our analysis to the modes associated with m in Equation ([eq: enfourier]). By making this assumption, the analysis of the pcopo dynamics below threshold can be simplified to a system of four coupled operators in the frequency domain, defined as:\n\n\u1e33\u1d62 = \u1e33\u1d62-1 + (\u0393\u1d62 - j\u03c9)\u1e33\u1d62-1 - j\u2126(C\u1d62\u1d62+D\u2297\u1e33\u1d62) + j\u2126(C\u2297\u1d62\u1d66\u1d62\u2297\u1e33\u1d66\u1d62-G\u1d62\u1d66\u1d62\u2297\u1e33\u1d66\u1d62),\n\nwhere \u0393\u1d62, C\u1d62\u1d62, D, C, and G are constant coefficients representing the material properties and system parameters. The vector \u1e33 is defined as:\n\n\u1e33 = (C\u1d62\u1d66\u1d62\u1e33\u1d66\u1d62, D\u2297\u1e33\u1d66\u1d62, C\u1d62\u1d67\u1d66\u1d62\u2297\u1e33\u1d66\u1d67\u1d62, G\u1d62\u1d66\u1d62\u2297\u1e33\u1d66\u1d62)\u1d40,\n\nin which \u2297 denotes the outer product and \u1d40 represents transpose. The vectors \u1e33\u1d66\u1d62 and \u1e33\u1d66\u1d67\u1d62 are also defined as:\n\n\u1e33\u1d66\u1d62 = (x\u1d66-1, x\u1d66, -j\u03c9x\u1d66-1, x\u1d66') and \n\n\u1e33\u1d66\u1d67\u1d62 = (x\u1d66-1, x\ufffd In order to reduce the study of the nonlinear PCOPO dynamics below threshold to four coupled operators, we utilize four equations in the frequency domain denoted as: '@xmath85' where '@xmath86' and '@xmath87' and '@xmath88' correspond to '@xmath89' and '@xmath90' respectively. The vector '@xmath91' is also expressed in a similar fashion. The matrix '@xmath92' is calculated by using eqs. ( [ eq : stationaryk ] ) and ( [ eq : stationaryk0 ] ) with '@xmath94' as an independent variable. This results in @xmath95 and @xmath96. The output fields are obtained from the input-output formalism @xcite , which is governed by @xmath99 where '@xmath100' is the identity matrix. In this section, we concentrate on the spatial quantum effects in the signal field, omitting the index '@xmath102' without ambiguity, to calculate different correlations in the output variables given the input ones, and to obtain the inverse of the @xmath92 matrix. In order to proceed, let us consider the stationary input-output scenarios specified by Eqs. ([ eq : stationaryk ] ) and ([ eq : stationaryk0 ] ), respectively. By applying the mathematical transformation $\\mathcal{E}[x(t)]:=x_94$ and defining the output fields via the input-output formalism $([ eq : i-o ] )$ as in Eq. 98, we obtain Eqs. ([ eq : xmath95 ] ) and ([ eq : xmath96 ] ), respectively. Consequently, the output fields can be calculated from the known input fields using Eq. 99, which governs the dynamics in the frequency domain according to the identity in Eq. 101. In this work, we focus on exploring spatial quantum effects in the signal field, and hence, for the purpose of convenience, we may omit the spatial index without ambiguity. In order to calculate various correlations in the output variables given the input ones, we need to obtain the inverse of the matrix appearing in Eq. 92, the expression for which is presented in Appendix A. This formalism lays the foundation for the discussion of analytical quantum correlations for the output fields in the subsequent sections. The foundation of analytical quantum correlations for output fields is based on this formalism, as discussed in the subsequent sections. This formalism, in turn, reveals the non-classical feature of twin beams, which stems from the negativity of the variance at position x and the corresponding singularity of the p representation. Notably, the terms @xmath104 that appear in the second row of the ([twin]) vanish when pc is absent or in the absence of pc. This is also evident from the analytical correlations presented in the appendix. In the absence of pc or when @xmath42, terms @xmath104 disappear in the second row of the [ twin ] correlation matrix. This follows from the analytical correlations provided in the app, as outlined in section [ sec : correlationsw ]. Conversely, these terms are substantial for pcopo when pc is impacting the signal, i.e., @xmath105. Notably, the last term @xmath106 vanishes for @xmath107, as well as for @xmath39 and @xmath105. However, this term contributes to variance negativity for @xmath78.  \n\nAdditionally, when @xmath107, the second row of the [ twin ] correlation matrix looks as follows:\n\n  [ label ]       [ label ]\n  ----------  |  ----------\n  ----------  |  ----------\n  A           |  A\n  *B          |  *B\n  *C          | **C\n  ----------  |  ----------\n\nwhere the asterisk (*) denotes that the term for that position does not exist when pc is absent or @xmath42, and the double asterisks ('**') indicate a term that only appears in a pc-affected scenario. Overall, these insights provide valuable insights for further analyzing and understanding the data. \n\nFor example, when focusing on pcopo and pc-affected scenarios, the significant values for @xmath104 and @xmath106 can be used to extract information about the system or signal at hand. This information can aid in interpreting, diagnosing, and ultimately refining the system or signal, leading to improved performance and outcomes. In contrast, for the terms related to pcopo, they are substantially larger when PC affects the signal, specifically for the case denoted by @xmath105. Moreover, the term @xmath106 disappears for both @xmath107 and for @xmath39 and @xmath105, but it can contribute negatively to the variance for @xmath78. In particular, for @xmath107, the simple expression for the output fields obtained via Eq. ( [ twin ] ) is @xcite @xmath108, where the shot noise is symbolized by @xmath109. Consequently, the normalized variance is consistently negative and constant below threshold. However, for a conventional pcopo and as can be observed in Fig., this is not the case as explained in greater detail in the following section. However, in the case of a phase-sensitive optical parametric oscillator (PCOPO), the normalized variance varies based on the modulated phase. Conversely, for a normalized variance, it is always negative and constant below a threshold. In Fig. [fig7a], twin beams correlation normalized by shot noise is demonstrated for a fixed value of the pump and various PC modulations. While the OPPO recovers at the origin, the correlation between twin beams is affected by the PC, becoming more classical with increasing modulation strength @xmath{110}. In contrast, when modulation is present only in the signal detuning, @xmath{39} and @xmath{105}, the next discussion will illustrate its impact on the system. In situations where both the origin and parallel beams are retrieved, the correlation between the twin beams exhibits modifications due to PC, especially for higher values of @xmath110. Conversely, when the modulation is solely in the signal detuning, at @xmath39 and @xmath105, we encounter twin beams correlation patterns of @xmath111, while shot noise affects them as @xmath112. However, for @xmath113, which is equivalent to the threshold as depicted by the black solid curve in figure [fig7]b, the normalized expression for the twin beams diverges. In the expression for twisted-beam correlations, their normalized form diverges when @xmath113, which denotes the threshold as shown in Eq. ( [ eq : thresh ] ). The black solid curve in Figure [ fig7]b depicts this correlation. Notably, these correlations depend on all parameters, unlike in OPOs where they remain constant at any pump power. A closer look at Figure [ fig7]b reveals that correlations degrade and may become classical below threshold (above the red line). Fig. [fig7]b shows that the correlations are significantly degraded, and in some cases return to a classical regime above the red line. This degradation in correlation can be attributed to the loss of quantum properties beyond a certain threshold. In contrast, Fig. [fig7]c demonstrates that when only the pump detuning is modulated, the normalized twin beams correlations remain quantum for all pump strengths below threshold. This finding suggests that modulating the pump detuning has a minimal impact on the quantum nature of the correlations. From a microscopic perspective, this phenomena can be explained through the modulation of the underlying physical processes that result in the correlation effects. The correlation among photons continues to exhibit a quantum phenomenon below the pump strength threshold, albeit experiencing a slight decrease. This can be explained by taking into account the modulation effects from a microscopic perspective. In particular, the modulation of the signal detuning due to a periodicity of [ 2kc ] leads to the coupling and conversion of different spatial modes. When this modulation is in the form of a standing wave (pc), photons can hop between critical tilted modes at xmath114. Meanwhile, the nonlinearity leads to the simultaneous creation and annihilation of photon pairs in the signal. Overall, this complex interplay between spatial modes and modulation effects plays a significant role in the dynamics of the correlation and photon hopping in the system. At the critical tilted modes, there are both creation and destruction processes between photons, which can result in simultaneous creation and annihilation of photon pairs in the signal. However, a spatial modulation of the signal detuning leads to photons hopping between opposite modes, resulting in a process of the creation of one photon and the destruction of another in the opposite mode. These twin beam correlations are present for small hopping rates due to photon pairs emission. However, when the detuning modulation of the signal is increased, the variance of this correlation becomes classical as the twin beams are depleted uncoherently. For a specific value of the detuning, the hopping between different pump harmonics still has an impact but is reduced and less detrimental, as it triggers fewer secondary processes besides twin photon pairs generation. In optics, twin beams correlations arise due to the emission of photon pairs, and for small hopping rates, these correlations remain present. However, an increase in the detuning modulation of the signal results in the uncoherent depletion of the twin beams, leading to a classical variance. Conversely, when detuning modulating the pump field, the impact of hopping between different pump harmonics is still deleterious, as it triggers other secondary processes beyond the twin photons pairs generation. Nevertheless, this effect is attenuated.\n\nIntracavity phase contrast (PC) has a profound influence on an instability process and parametric threshold, which can be either raised or lowered based on the configuration. This is observable in a typical device featuring quantum light spatially correlated in continuous variables, such as in the degenerate optical parametric oscillator (OPO) type I. Translational symmetry breaking in the transverse profile generates multiple effects, as seen in the noisy precursors locking. The PC modulation has a significant impact on these phenomena leading to modifications in the twin beams correlations, squeezing, separability, and entanglement. To analytically evaluate these quantum correlations, we can perform necessary calculations and measurements. The PC modulation holds significant influence on the instability process with parametric thresholds that are both raised and lowered according to the configuration [fig. [5]]. This outcome sparks alterations in correlations for twin beams, squeezing, separability, and entanglement. To analyze these quantum correlations, two main approximations were proposed, which are valid below threshold. These approximations are based on linearization around the steady state and restricting analysis to relevant harmonics [fig. [2]]. For instance, for a PC modulation with frequencies of 2kc, the prototype model consists of a pump mode (x117) and a signal mode (x118), and yields satisfactory agreement with the outcome of the full multimode nonlinear model (eqs. [EQ: lang]) [fig. [5]]. Regarding the investigation of our prototype model, we have considered a computer-generated modulation of the optical pump with 2 kc frequency, alongside five distinct modes for the signal. These approximations allowed for notable consistency with the numerical resolution of the full multi-mode nonlinear model, as exhibited by equations ( [ eq : lang ] ). Notably, for this model, the periodic modulation leads to a sinusoidal fluctuation of the index refractive and, in turn, detuning effects for the pump, signal, or both fields, contingent upon the magnitude of the modulation parameters @xmath119 and @xmath110. We can adjust diverse configurations by altering these variables. Moreover, when the pc modulates the signal, in a manner reflected by @xmath105, the instability threshold ascends, and our findings corroborate previous research pertaining to single resonant cavities and the suppression of pattern formation. \n<|user|>\nCan you please summarize the main idea from the previous text about the pc modulation and its effects on the signal threshold? Specifically, altering the signal field in a photon counting (PC) setup through modulation, as described in previous works for single resonant cavities, presents a rising instability threshold. This trend aligns with the predicted pattern inhibition. However, the PC exhibits a more complex scenario when a parametric process is present. In this case, the threshold can also decrease when the signal field undergoes a modulation at a frequency of @xmath78. The implications of this effect are significant, particularly concerning non-classical phenomena such as squeezing and entanglement, which are particularly sensitive to proximity to the instability point. Such non-classical effects can also be enhanced when the threshold is lowered, improving correlations at a given pump over the commonly-used OPO configuration. These results suggest the PC setup provides unique advantages in terms of modulation and thresholds, as compared to traditional optical parametric oscillators (OPOs). In non-classical phenomena, sensitivity to proximity of instability points holds significant consequences, specifically in photon-pair generation. Particularly, pump modulation lowers threshold and enhances correlations, resulting in improved squeezing compared to ordinary optical parametric oscillators (OPOs) at the same pump power. Furthermore, all correlations have been assayed at fixed offsets from the threshold in both OPO and phase-sensitive coherently prepared OPOs (PCOPOs) for investigative purposes. The results demonstrated that squeezing, separability, and Einstein-Podolsky-Rosen entanglement all present considerable variances between OPO and PCOPO, with the latter exhibiting more prominent features. This evidence highlights the impact of pump modulation in the context of quantum technologies, with PCOPAs proving advantageous in photon-pair generation. In our investigation, we have discovered that the phenomena of oscillation squeezing, separability, and EPR-entanglement are not only retained but also present with noticeable widths in the regions of quadrature and superposition angles when subjecting an optical system to periodic pulsing (PCP) with parameters @xmath119 and @xmath110 (see Fig. [Fig4]). Upon analyzing the twin beam correlations (see Fig. [Fig7]), we have observed that deeper effects exist in these intensity correlations even at a fixed distance from the threshold, and, in general, secondary processes do degrade correlations. Numerical simulations show a significant enhancement of squeezing and entanglement, which is also evident in our previous work @xcite. In summary, our investigations reveal that PCP impacts not only light's quantum effects but also its energy consumption, as lower energy results from the modulations, and these modulations can either enhance or avoid the quantum properties of light. In our analysis, we have computationally examined twin beam correlations by varying pump and signal spatial modulations within the cavity, specifically at xmath119 and xmath110. Contrary to common belief, deeper effects were observed, even when maintaining a fixed distance from the threshold. Furthermore, we have previously presented numerical simulations highlighting a significant enhancement of squeezing and entanglement above threshold. The incorporation of pump-chip mode coupling allows for obtaining similar quantum effects at lower energy, where modulation of pump and signal spatial modulations within the cavity offers opportunities to enhance or avoid quantum properties of light. Fourier transforming in the temporal variable equations (Eqs. [enfourier]), neglecting terms with xmath121 in both the signal and pump, yields Eq. (1) below, wherein the modes are coupled between them, with notation simplified:\n\n      @eqnum \n      @begin{equation}\n        [1]\n        c_S(x, t) = \\iiint [d_S(x', t', x, z') + h_S(x', t', x, z') ] dF(x') dt' dx' dz'\n      @end{equation}\n\n      We denote @xmath59 modes as @xmath124, @xmath125, @xmath126, and @xmath127. Let us call them @xmath129, @xmath130, @xmath131, and @xmath132, respectively. Then, we can represent the mode dynamics as a matrix, namely:\n\n      @eqnum \n      @begin{equation}\n        [2]\n        \\@mat{cccc}\n          \\frac{d c_1}{d t} & \\frac{d c_2}{d t} & \\frac{d c_3}{d t} & \\frac{d c_4}{d t} \\\\\n          \\frac{d c_1^*}{d t} & \\frac{d c_2^*}{d t} & \\frac{d c_3^*}{d t} & \\frac{d c_4^*}{d t}\n        @end{equation}\n\n        where the continuous index xmath30 is used to describe all fluctuations. In the case of In neglecting all terms with the symbol \"@xmath121\" in the signal and pump, and specifically focusing on the coupling between \"@xmath59\" modes denoted as \"@xmath124\" and \"@xmath125,\" and using a compact notation, we can rewrite eq : enfourier for the Fourier transformed field amplitudes as @xmath139. Using this compact notation, let us refer to \"@xmath130\" and \"@xmath131\" as mode A and mode B, respectively, and similarly for \"@xmath132\" and \"@xmath133.\" The matrix representing the mode-mode coupling can be expressed as eq : matl in terms of the amplitudes of the coupled modes A and B, as well as the couplings between them. For the modulation with a critical wavenumber \"@xmath136\", a reduced description can be obtained, resulting in the matrix represented by eq : red, where only a few intense modes are relevant. With continuous index \"@xmath30\", the dynamics of the output fields is governed by eqs: in-out as described in a previous publication. The evolution of output fields is governed by a set of equations, with the input and output fields denoted by the variables within the brackets [ ] in Eq. (1):\n\n(1)\n   [ input ] \u2192 [ output ]\n\nTo obtain the inverse of this transformation, denoted as $\\left(x \\right) ^{-1}$, we can use an expression for the determinant and inverse of the transformation matrix using the formulas:\n\n(2)\n   det(\\textbf{J}) = (-1)^n \\times \\left(c_1 c_3-c_2 c_4\\right)\n\n   (\\textbf{J})^{-1} = \\frac{1}{det(\\textbf{J})} \\times \\begin{bmatrix}\n     d_1 & -d_3 \\\\\n     -d_2 & d_1\n   \\end{bmatrix}\n\n   (3)\n   where $\\textbf{J}$ is the transformation matrix composed of $\\left(c_1, c_2\\right), \\left(c_3, c_4\\right)$ and $\\textbf{J}^{-1}$ represents the inverse transformation. The components of this matrix are:\n\n(4)\n   c_1 = c_{11}$x_{1}$ + $c_{12}$$x_{2}$\n\n   c_2 = $c_{21}$$x_{1}$ + $c_{22}$$x_{2}$\n\n   c_3 = $c_{31}$$x_{1}$ + $c_{32}$$x_{2}$\n\n   c_4 = $c_{41}$$x_{1}$ + $c_{42}$$x_{2}$\n\n   d_1 = $d_{11}$$x_{3}$ + $d_{12}$$x_{4}$\n\n   d_2 = $d_{21}$$x_{3}$ + $d_{22}$$x_{4}$\n\n   (3) involves the expression of output variables in terms of input ones, which is also present in Eq. (1). After defining the matrix components as given in Equation 144 where Equations 145 and 146 hold, we can use the input-output expression provided in Equation ( [ eq : in - out ] ) to calculate various second-order correlations in the frequency domain. For instance, we can compute the correlations at different frequencies \u03c9j and the same \u03c90 between output variables x1 and x2. To obtain a solution for these correlations in terms of input variables, we need to consider that only one combination, @xmath149, survives Fourier transform, while any other combination vanishes [this property is derived from Equation 150]. In examining the correlation between variables @xmath147 and @xmath148 at distinct @xmath2 and a constant @xmath30, we may rephrase to state that we can calculate the correlation coefficients based on the input variables. To solve these expressions, the use of a Fourier transform reveals a particular characteristic: when we apply a Fourier transform to an expression of the form @xmath150, we obtain a new expression @xmath151, which results in a delta function @xmath152. This property allows us to simplify the equations even further by rewriting the matrix, with the term @xmath153 replaced with @xmath154. The change in sign is necessary to derive the resulting expression @xmath155. Based on the given text provided by the user, the next paragraph in high quality wikipedia-like English is as follows:\n\nSpecifically, the delta function at the origin leads to a matrix that yields the term given by eq. @xmath154. To obtain the time-independent (spectral) intensity, one must reverse the sign of the terms present in the matrix. Therefore, for the spectral intensity, we obtain:\n\neq. @xmath156\n\nIn order to obtain the time-domain intensity, we must carry out a Fourier transform of the previous expression. This transform yields:\n\neq. @xmath157\n\nTo solve this integral, we need to consider the denominators in the expression, which feature eight poles of the form:\n\neq. @xmath158\n\nHere, we see that the poles can be divided into four that lie in the upper part of the complex plane and another four in the lower part. These poles are traditionally encountered when solving complicated integrals in the theory of analytical functions or functional analysis.\n\nUsing conventional methods, we can perform this integral to obtain the intensity expression (eq. : intensity ). Any other second order correlation can be obtained using this same equation. To derive any additional second-order correlation functions in high quality, as in [EQ: INTENSITY], we can reference [EQ: IN-OUT] for guidance. For example, to obtain the expression for variance @xmath159, we utilize all the non-vanishing terms in the correlation function in the form of [EQ: VARIANCE], which can be further expanded into [EQ: VARIANCE_EXPANSION] as described before. Finally, we obtain the Fourier transforms of all the relevant correlations for calculating the variance, which are [EQ: FOURIER_TRANSFORM_1], [EQ: FOURIER_TRANSFORM_2], [EQ: FOURIER_TRANSFORM_3], [EQ: FOURIER_TRANSFORM_4], [EQ: FOURIER_TRANSFORM_5], [EQ: FOURIER_TRANSFORM_6], [EQ: FOURIER_TRANSFORM_7], and [EQ: FOURIER_TRANSFORM_8]. The variables @xmath168, @xmath169, @xmath170, and @xmath171 were defined earlier in the text. In the expression for the variance at the point x, all non-vanishing terms are:\n\n@xmath159\n\n[...]\n\n@xmath160\n\n[...]\n\n@xmath161\n\n[...]\n\n@xmath162\n\nThe Fourier transforms of all the relevant correlations used in the definition of the variance are:\n\n@xmath163\n\n@xmath164\n\n@xmath165\n\n@xmath166\n\n@xmath167\n\nwhere @xmath168 and @xmath169 were previously defined, @xmath170, @xmath171, and @xmath172 as well. This result was published in \"Optics Letters\" by N. Marsal, D. Wolfersberger, M. Sciamanna, G. Montemezzani, and D. N. Neshev in vol. 33, issue 25, in 2008 (vol. 35, issue 35, in 2010).",
        "abstract": " we show how to control spatial quantum correlations in a multimode degenerate optical parametric oscillator type i below threshold by introducing a spatially inhomogeneous medium , such as a photonic crystal , in the plane perpendicular to light propagation . \n we obtain the analytical expressions for all the correlations in terms of the relevant parameters of the problem and study the number of photons , entanglement , squeezing , and twin beams . considering different regimes and configurations we show the possibility to tune the instability thresholds as well as the quantumness of correlations by breaking the translational invariance of the system through a photonic crystal modulation . ",
        "section_names": "introduction\nfew mode approximation for the pcopo\nconclusions\nlinear dynamics of pcopo with any @xmath120\nsolution of the input-output equation\nsecond order moments in frequency and time domains"
    },
    {
        "article": "Developers at Ubisoft Montreal announced the launch of the \"Year 3 Pass\" for their battle royale game, Tom Clancy's Rainbow Six Siege on November 8, 2018. The pass includes six operators for the gameplay, each from unique countries and nationalities. These operators are set to be released in three-month intervals, starting from Operation Grim Sky in early November. These new additions will bring the total number of playable operators in the game to 44, significantly increasing the variety of strategies and techniques available to players in the multiplayer mode. These fresh characters have been named Warden and Maestro, respectively based in the United States and Italy. The Year 3 Pass also includes new maps and game modes that are designed to provide new challenges and experiences for gamers, keeping the gameplay fresh and exciting. Overall, the introduction of new content through the \"Year 3 Pass\" ensures that Tom Clancy's Rainbow Six Siege remains relevant and engaging for a growing and dedicated player base. According to current beliefs, class I methanol masers, primarily the 36 and 44 GHz transitions, are collisionally excited. In contrast, class II masers, such as the 6.7 and 12 GHz transitions, are radiatively excited. While both classes of masers can be associated with the same source in some cases, they are rarely found at the same velocity or in close spatial overlap. Class I methanol masers, which are commonly found in sources with shocks dominating over infrared radiation, have been assumed to be tracing an earlier stage of star formation compared to class II, water, or OH masers. Classification subcategories within class I masers based on physical conditions may also be possible, leading some authors to propose that line intensity ratios among class I masers could be a proxy for evolutionary stage. Investigating the subcategorization of class i masers based on physical conditions is a potential avenue for examining their evolutionary stages. However, previous class i maser studies have often prioritized regions with other star-formation tracers, leading to a biased understanding of the evolution of these masers. The complex environments encountered in clustered star formation raise doubts about traditional models of the timeline for class i masers (see section 4.4 of *? ?? * and references therein). Furthermore, traditional methods for observing class i masers with single-dish telescopes can only identify which transitions produce masers and their relative intensities. However, to determine the locations of these masers and understand their relations with excitation sources (e.g.,*? ?? *), and between multiple transitions of methanol, high angular resolution is necessary.*;*? ??* Here, *? ??* represents the specific essay or article from which the cited information was taken. However, high angular resolution is crucial to identify the relationships between masers and excitation sources and multiple transitions of methanol. Class I masers, which are commonly detected using single-dish telescopes, reveal whether a particular class I transition generates masers in a region but cannot determine their location relative to masers from other transitions due to their limited resolution. The complex environments associated with clustered star formation necessitate high angular resolution to understand the physical conditions that produce masers in each class I transition, which may not be identical. Gaining insight into these relationships can provide valuable information for comprehending the processes of clustered star formation and identifying the origins and propagation of masers. Motivated by the unresolved concerns, @xcite conducted an unbiased single-dish search for Class I methanol masers in nearby molecular clouds. This endeavor resulted in the detection of several new Class I maser features. Additionally, several sites within these clouds have previously known 44 GHz methanol masers that have been interferometrically mapped. For example, * ? ? ? * ; * ? ? ? Historically, 44 ghz methanol masers were previously identified, with many of these sources having been mapped interferometrically through studies such as * ? ? * and * ? ? ? *. However, observational limitations prevented detailed imaging of the 36 ghz maser, the other prominent line observed in numerous sources at this frequency. Nevertheless, recent upgrades to the Australia Telescope Compact Array and the expanded Very Large Array allow for the generation of high-resolution, arcsecond-scale images of 36 ghz masers, as demonstrated in recent work @xcite. In previous astronomical studies, the 36 GHz methanol masers, one of the brightest transitions seen in numerous sources, could not be imaged at high resolution due to the lack of radio telescopes operating at this frequency. Recently, upgrades to the Australia Telescope Compact Array and the Expanded Very Large Array (EVLA) have allowed for the production of first arcsecond-resolution images of 36 GHz masers. In this letter, we report on the first EVLA maps of the 36 GHz methanol masers in the DR21 star-forming complex. The EVLA was used to observe the 36.169 GHz line of methanol in DR21(OH), DR21W, and DR21N on May 26, 2010, with the array consisting of 20 telescopes equipped with Ka-band receivers. The EVLA was in its most compact (D) configuration, resulting in a synthesized beamwidth of approximately 0.9 arcseconds. The Evolver LOFAR (e-VLASS) array, which consisted of 20 telescopes equipped with kilometer-band (ka band) receivers, was used to observe the 36.169 ghz @xmath0 line of methanol in three sources: DR21(OH) , DR21W, and DR21N on May 26, 2010. The e-VLASS was in its most compact (d) configuration, providing a synthesized beamwidth of approximately @xmath1 mm. All three sources were observed in dual circular polarization with a fixed sky frequency of 36.1731 GHz and correlated with the new WIDAR correlator. The 4 MHz observing bandwidth was divided into 256 spectral channels, giving a velocity coverage of 33 km s@xmath2 with a channel spacing of 0.13 km s@xmath2. Conversion from sky frequency to LSR velocity was performed using the e-VLASS online Doppler tool. The observing bandwidth of 4 MHz was split into 256 spectral channels, resulting in a velocity coverage of 33,000 km/s with a channel spacing of 0.13 km/s at 2.4 GHz. The connection between sky frequency and LSR velocity was aided by the EVLA online Doppler tool. One hundred and seventy-two minutes were dedicated to observing each source. Near the center of the field, the single-channel noise level was around 1.0 mJy/beam at 2.4 GHz. Data reduction was accomplished through the NRAO Astronomical Image Processing System (AIPS). The flux scale was set using 3C48. During observation, single-channel noise levels were approximately @xmath4  mJy/beam at an xMath2 position near the field's center. The data was processed using the NRAO Astronomical Image Processing System (AIPS), and its flux scale was set using 3C48. Calibration for complex gains was performed using J2048+4310, but issues with the EVLA transition prevented accurate bandpass calibration. Though baseline phases remained consistent throughout the entire observing band, amplitudes were only flat over approximately 75% of that range. This likely did not significantly affect main results or the determined positions of detected masers, but it may cause amplitude errors of a few percent over most of the observed velocity range. During observations with the EVLA, transition problems resulted in inaccurate bandpass calibration. However, analysis of calibrator data revealed that baseline phases remained constant throughout the entire observing band, with amplitudes relatively unchanged over about 75% of it. This indicates that the primary results, such as estimates of detected maser positions, should not be significantly affected. Nonetheless, bandpass effects may introduce amplitude errors of several percent over most of the observed velocity range. The imaged data were corrected for primary beam attenuation, but minor velocity errors could occur due to two factors. First, the uncertainty in the rest frequency (5665 MHz *) equivalent to a velocity uncertainty of 0.25 km/s (2 channels) could introduce these errors. The presence of minor velocity errors in the observed data may result from two significant factors. The first effect involves the uncertainties in the rest frequency of the detected line, which for most of the cases was 0.008 GHz (80 MHz) or 0.004 GHz (40 MHz) corresponding to velocity uncertainties of 0.25 kms@xmath2 (2 channels). In instances with higher frequency resolution of 4 MHz, the velocity uncertainty was 0.002 GHz or 0.6 kms@xmath2.\n\nSecondly, during observations, doppler tracking was not available, thus causing the sky frequency associated with the rest frequency to drift by approximately 0.015 mHz (0.15 MHz) over the duration of observations, potentially resulting in insignificant spectral broadening of maser features. However, the uncertainty in determining the central velocity due to fixed sky frequency observations for most data is comparatively minor to the channel separation and does not require correction using cvel.\n\nTherefore, we present our observations at 229.758 GHz for OH maser and 226 GHz continuum data that includes 229 sources (Pratap et al.)\n\n(NOTE: Original punctuation marks were changed for consistency with Wikipedia style during rephrasing) During the observations, the rest frequency drifted by approximately 0.015 mHz, potentially causing negligible spectral broadening of maser features. The uncertainty in determining the central velocity due to fixed frequency observing (at around 5 and 7 kilometers per second) is much smaller than the channel separation and, consequently, the data were not corrected via cvel. P. Pratap et al. will present 229.758 GHz maser and 226 GHz continuum SMA data for all sources, which has an angular resolution of 11 arcseconds, in a forthcoming manuscript. The observational bandwidth spanned a velocity range of 11 to 71 kilometers per second with a spectral resolution of 0.13 kilometers per second. The angular resolution of the observations is 9 arcseconds, with a bandwidth covering the velocity range from 9 to 28 kilometers per second (km/s) and a spectral resolution of 0.13 km/s. The flux densities of the 229 GHz masers were corrected for primary beam attenuation, and the root mean square (RMS) noise near field center was 0.15 and 0.3 jy beam, respectively, for DR21(OH) and DR21W. We compare our data against three other Class I methanol maser transitions from the literature: 44 GHz, 84 GHz, and 95 GHz, with archival 44 GHz data from the VLA in b-configuration on June 16, 2006, available for DR21N. The root mean square noise level near the field center was approximately 0.15 Jy/beam for both DR21(OH) and DR21N, and 0.3 Jy/beam for DR21W. To compare our data, we compared it to three other class I methanol maser transitions from the literature: 44 GHz (@Xmath12), 84 GHz (@Xmath13), and 95 GHz (@Xmath14). In addition, for DR21N, we used archival 44 GHz data taken with the VLA in B-configuration on 2006 Jun 16. Parameters of detected masers are listed in table [Table - Masers]. We detected 49, 36 GHz maser features in the three observed DR21 sources: 21 in DR21N, 23 in DR21(OH), and 5 in DR21W. Figures [Fig - Map - OH] and [Fig - Maps - N - W] demonstrate the locations of the 36 GHz masers in relation to other class I methanol transitions mapped with interferometric resolutions. In our study, we uncover 49, 36, and 5 36 GHz maser features in the three observed DR21 sources, specifically 21 features in DR21n, 23 features in DR21(OH), and 5 features in DR21w. Figures [fig - map - OH] and [fig - maps - N - W] depict the locations of these 36 GHz masers relative to other Class I methanol transitions mapped with interferometric resolution. These maser features' LSR velocities align with previous single-dish observations. Nonetheless, integrated flux spectra of individual maser features are often not adequately captured via a single Gaussian, with multiple spectral peaks sometimes evident or evidence of skewness in the spectral peak. In many cases, the spectra of individual 36 GHz maser features do not align well with a single Gaussian distribution, demonstrating the existence of structural elements smaller than the synthesized beam size of 12\". In some cases, multiple spectral peaks are evident, while in others, asymmetrical spectral peak shapes indicate structure on smaller scales. Similar results were observed in the 44 GHz transition, where @xcite detected 17 methanol masers in DR21(OH) using 10 minutes of VLA D configuration data, and subsequently detected 49 masers with much longer C-configuration data integration. A single clear spectral peak is seen in some individual maser spectra, with typical linewidths ranging from 0.15 to 0.35 km/s. However, a weak, extended methanol emission signature was also detected in some sources, though not without cautious consideration due to the inadequacy of proper bandpass calibration. In the 44 GHz transition, ATxcite have identified 17 masers in the DR21(OH) using 10 minutes of VLA D configuration data. Additionally, through a longer integration of C configuration data, they have detected 49 masers. Furthermore, many of the maser spectra show several spectral peaks. However, when there is a clear, single peak, typical linewidths are between 0.15 and 0.35 km/s (~@xmath2). Meanwhile, there is weak and extended methanol emission, which is observed in some sources, but their lack of proper bandpass calibration limits confidence in this result. There are 14 arcsecond-scale overlaps between a 36 GHz and a 229 GHz maser, which are approximately at the same velocity (~@xmath15 kms@xmath2). Of these 14 associations, the 36 GHz maser is brighter in 7 sources, while the 229 GHz maser is brighter in the other 7. A similar phenomenon is observed in the @xmath17 series, with a comparison of the integrated fluxes of sources that have narrow-velocity features (which presumably produce single masers in close spatial association) in both 44 and 95 GHz. Further high-resolution data is necessary to examine the relationship between the flux densities of different maser transitions seen in close spatial association with each other. Notably, in the @xmath17 series, @xcite identifies that, in contrast to what is seen in integrated fluxes between 44 and 95 gHz, sources whose spectra consist of narrow-velocity features in both transitions (which are believed to produce single masers in close spatial association) show comparable integrated fluxes in both transitions. However, further high-resolution data is required to investigate the relationship between the flux densities of different maser transitions that occur in close spatial proximity. A comprehensive example can be found in DR21(oh), where a central region of the source is marked by numerous masers in all mapped transitions, which form an approximately elliptical structure (fig. [fig - map - oh]). A striking feature of DR21(oh) is that more than 30 masers at 44 gHz associated with two shocks in a bipolar outflow are coincident with the 36 gHz masers in the outflow region, within the positional uncertainty of observations. In a study, @xcite identified over 30 methanol masers at a frequency of 44 GHz that are associated with two shocks in a bipolar outflow. To within the positional uncertainty of the observations, all 14 of the 36 GHz masers in the outflow region are coincident with these 44 GHz masers. The 84 and 95 GHz masers are located within the outflow region, but the lower angular resolution of the observations precludes more detailed analysis of multitransition positional coincidences. The vast majority of the 229 GHz masers align with the 36 GHz masers on the western side of the outflow, but they appear to be more coincident with the 44 GHz masers of @xcite, particularly in the eastern part of the outflow. The absence of a precise analysis of multitransition positional coincidences due to the aforementioned constraint prevents more detailed investigations. However, out of the 229 GHz masers, many align with the 36 GHz masers concentrated on the western side of the outflow. The overlap between the 229 GHz and 44 GHz masers, particularly in the eastern region, appears more substantial. All transitions display a redshift on the western side of the outflow and a blueshift on the eastern side. The direction of the outflow indicates a redshift gradient towards the west, with Local Standard of Rest (LSR) velocities generally increasing from the center outward. Despite this pattern, some masers on the northern and southern edges of the outflow exhibit unconventional velocities. There is an overall velocity gradient from west to east in the redshifted outflow, with Local Standard of Rest (LSR) velocities increasing from the center to the periphery. However, some masers at the northern and southern edges of the outflow have velocities that do not align with this pattern. The most prominent 36 GHz masers are situated on the western side of the outflow, with the brightest 36 GHz maser located close to the brighter 44 GHz and 229 GHz masers. Within the outflow, each detected 36 GHz maser coincides spatially with a corresponding 44 GHz maser at a similar velocity. Conversely, there are numerous 44 GHz features that do not have a corresponding detection in the 36 GHz band. On the eastern side of the outflow,\u00a0\u00a0... Each instance of a detected 36 GHz maser in a given region is accompanied by a corresponding 44 GHz maser situated in a nearby area with a similar velocity. However, this relationship is not reciprocated, as numerous 44 GHz features display no detected 36 GHz emission. On the eastern side of the outflow, 36 GHz masers are confined to the eastern edge, whereas other transitions are visible throughout. In contrast, many of the 44 GHz masers located in the central and western portions of the outflow possess no 36 GHz counterpart. Class II masers at 6.7 GHz are observable in projection atop the bright continuum feature near the origin in figure [Fig. - Map - Oh], whereas several 229 GHz class I masers appear in coincidence with the 6.7 GHz masers in some cases, but slightly offset to the east and south at the level of approximately one-tenth of the 229 GHz beam size. In the 6.7 GHz transition featuring class ii masers, as illustrated in Figure [fig - map - oh], these appear to be projected atop a bright continuum feature near the origin. Similarly, the 229 GHz class i masers also coincide with the 6.7 GHz masers in certain cases, but their offset locations are situated slightly to the east and south at the @xmath22 level. Moreover, weak maser emission is observed at 36 GHz, manifesting in spectral channels ranging from 100 to 150 mJy between @xmath23 and @xmath24 km/s, likely due to spectral blending that is not fully resolved at the current resolution. The strongest feature's position is listed in Table [table - masers]. This particular feature is offset to the east and south even further relative to the other masers, with a caveat that the precise position is challenging to determine, given its weakness and the 2 beamsize of the 36 GHz observations. In Table [ table - masers ], we report the location of the strongest maser feature. This feature is situated further to the east and south than the 229 GHz masers with the caveat that determining the position of the 36 GHz maser accurately is challenging due to its faintness and the 2-beamsize of the 36 GHz observations. Several class I masers are evident to the south (36 and 44 GHz), as well as to the north (44 GHz only), in contrast to the masers in the outflow. The majority of the southern masers display blueshifts, while the northern masers have redshifts. However, not all masers conform to this pattern. In contrast to the 36 GHz masers in the south, which are typically not accompanied by co-spatial 44 GHz masers, the northern and southern masers in general exhibit redshifted and blueshifted characteristics, respectively, compared to the majority of masers in the outflow in the source DR21(OH). The southern masers are located closer to the sources DR21(OH)W, DR21(OH)S, and the ridge of ammonia emission connecting them, rather than the outflow in DR21(OH). Consequently, these masers are unlikely to be associated with the main source driving the outflow in DR21(OH). Nevertheless, DR21W contains the brightest (approximately 50 mJy) 36 GHz maser in our sample, which stands out due to the presence of an antisymmetric Stokes V profile (Figure [Fig - Zeeman]) discussed further in Section [Magnetic]). The bright masers situated near the origin in Figure [Fig - Maps - N - W] are harmonized with the 44 GHz masers detected by @xcite to a resolution of less than a synthesized beamwidth. This Maser is the only one showing an antisymmetric Stokes V profile which will be discussed further in the section on magnetism. The bright Masers near the origin in Figure [fig - Zeeman] are coincident with the 44 GHz masers detected by @xcite within their synthesized beamwidth. The velocities of the 36 GHz masers close to the origin match those of the brightest nearby 44 GHz masers to within 0.2 km/s or better. DR21n was initially spotted as a 36 GHz maser source by @xcite. We detect a significant number of 36 GHz masers at DR21n, mostly on the eastern side of the maser emission's distribution, within the range of 26 to 27 km/s at xmath2. The astronomical source dr21n was initially detected as a 36 GHz maser source by xcite, and subsequent observations have revealed the presence of multiple 36 GHz masers in this source. These masers primarily fall within a range from 6.6 to 7.7 kilometers per second (km/s) at an angular separation of approximately 3\" from the eastern side of the maser distribution (see Figure [fig - maps - n - w]). Redshifted masers stretching from 9.9 to 10.3 km/s are located on the western side, while a highly blueshifted maser at 22.6 km/s rounds out the masers detected in this source. Interestingly, all masers detected at 229 GHz are located close to a 36 GHz maser, whereas no 44 GHz maser is seen within one arcsecond of a 36 GHz maser. All detected 229 GHz masers are located in close proximity to a 36 GHz maser, with no 44 GHz maser found within 1 arcsecond of a 36 GHz maser. This suggests an intriguing contrast in maser distribution. In contrast, previous observations by * ??? have identified an \"s-curve\" Stokes V signature in the spectrum of the brightest maser in DR21w, which, when interpreted as due to Zeeman splitting, implies a line-of-sight magnetic field strength of @math32 mg, where @math33 represents the Zeeman splitting coefficient of 1.7 Hz cm^-1 for the 36 GHz transition. This component of the magnetic field would be oriented away from the observer based on this signature, as illustrated in Figure [fig - Zeeman]. If interpreted as being due to Zeeman splitting, the implied line-of-sight magnetic field strength for the 36 GHz transition of methanol is estimated to be 10.3 mG, using the splitting coefficient of 1.7 Hz mG. This component of the magnetic field would be oriented away from the observer. The full three-dimensional field strength would presumably be larger, as Zeeman splitting solely measures the line-of-sight component of the magnetic field. However, there is significant uncertainty surrounding the splitting coefficient, which calls into question the direct Zeeman interpretation of methanol Stokes V s-curves. * ? ? ? obtained a laboratory estimate of the Land\u00e9 factor for several different transitions of methanol, ranging from 6.7 to 25 GHz. Using this factor, * ? ? ? and * ? ? estimated the Zeeman splitting coefficient for the 6.7 GHz transition, and * ? ? ? followed their method to derive the coefficient for the 36 GHz transition. More recent calculations suggest that these estimations may have overestimated the Zeeman splitting coefficients by an order of magnitude (Wh.T. Vlemmings, private communication, 2010), which would result in a proportional increase in the putative magnetic field strengths reported in works including this one and others, such as * ? ? ?. Stokes V and S curves were not detected in any of the other maser sources other than DR21(OH), where an upper limit of 0.03 mJy was determined for the brightest remaining maser. However, the circular polarization fraction of 0.5% seen in DR21w would not generate a detectable Stokes V signal for this source or in any of the weaker masers. This calls into question the direct Zeeman interpretation of methanol Stokes V curves due to significant uncertainty in the Zeeman splitting coefficient. The Laboratory estimate of the Land\u00e9 factor for the 25 GHz 6 13_2(7/6) transition of methanol, obtained by Xilouris & Menten (1986), may be an overestimate, as suggested by van der Wiel (2010, personal communication). This finding would increase proportionally the putative magnetic field strengths reported in this work and others, including (i.e., * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? *) Despite the suggested value for the xcite Zeeman splitting coefficient for the 36 GHz transition being potentially accurate, a line-of-sight magnetic field strength of 58 mG is discomforting. Measurements of magnetic field strength are regularly used to estimate density. Using a scaling from the densities and magnetic fields of DR21 would result in a number density of approximately 2.7\u00d710\u00b3 cm\u207b\u00b3 or 270 pc\u207b\u00b3 at 3.5 mG. However, this value is significantly higher than suitable conditions for 36 GHz maser excitation, typically ranging from 10\u00b3 to 10\u2076 cm\u207b\u00b3. Measurements of magnetic field strength are commonly employed as a proxy for density in various astrophysical studies. Employing a scaling factor from the densities and magnetic fields of DR21, a number density of around 1\u00d7105 cm-3 would result. This value is significantly higher than the suitable density for 36 GHz maser excitation, which is typically around 105 cm-3 (e.g., * ? ? ? * ; * ? ? ? ). Instead, scaling from values typically found in OH masers (a few milligauss, or 3 mG, @xmath43 @xmath42) is more appropriate. In contrast, scaling from the values typically found in OH masers, which range from a few milligauss, results in density estimates far in excess of conventional theoretical predictions. These estimates are well in excess of the values obtained from studies on class I maser activity, as reported in section 4.3 of * ? ? ? *(vlemmings, 2010, private communication). However, while the implied densities significantly surpass theoretical calculations, a smaller Zeeman splitting factor may reduce the discrepancy by two orders of magnitude, according to * ? ? ? private communication. However, the actual density estimates from observations surpass even those calculated through formulae presented by * ? ? ? (2012) and * ? ? ? (2005) by orders of magnitude. According to W.H. Vlemmings' private communication, if a smaller Zeeman splitting factor is assumed, the discrepancy increases by two orders of magnitude. This discrepancy between the observed density and theoretical calculations could indicate that the magnetic energy density is significantly greater than the kinetic energy density, or that the correlation between magnetic field strength and density is not applicable in all environments where Class I methanol masers appear. Alternatively, it is also possible that the high density implied by the large magnetic field is accurate, and there could be a different pumping scheme at play for the bright 36 GHz maser in DR21w that is not commonly assumed for Class I masers. Normally, Class I masers are thought to be produced by collisional excitation with subsequent spontaneous cascading down to lower energy levels, and the resulting sink photons either escape or are absorbed by cold dust. An alternative theory suggests that at high densities, the large magnetic field, contrary to the typical collisional excitation mechanism of class I masers, may be capable of producing a bright 36 GHz maser in DR21w through a non-conventional pumping scheme. This would imply a two-temperature mixture of particles, including electrons and neutrals, which can exist after a shock front. The collisional sink resulting from this pumping mechanism involves a cooler species counterbalancing the collisional excitation, in turn, yielding a population inversion. However, this inversion can only occur at densities that surpass the upper limit set by collisional thermalization. Consequently, this pumping scheme is a plausible explanation for maser activity in regions of high density and strong magnetic fields that may have been previously overlooked. At higher densities, a collisional excitation pumping mechanism may be operational when a warm species and a cooler species exist within a two-temperature mixture, as a result of a shock front. This collisional-collisional pump relies on the collisional excitation process by the warm species and the collisional sink in the cooler species, which creates a population inversion. The efficiency of this mechanism does not have a density limitation at higher densities as long as the temperature difference between the species is sustained. However, it is also proposed that the Stokes V signature detected may not measure the magnetic field directly. Instead, changes in the magnetic field orientation may convert linear polarization to circular polarization, creating an S-curve that can mimic the Zeeman splitting of a much larger magnetic field. This mechanism is considered in the case of circumstellar SiO masers as proposed by Xcite. @xcite's proposed mechanism involves converting linear polarization into circular polarization through changes in the magnetic field orientation. This process can result in an S-shaped curve that could resemble Zeeman splitting of a much larger magnetic field. In the case of circumstellar SIO masers, the proposed mechanism could function with a weaker magnetic field strength of 0.1% of what is implied from Zeeman splitting interpretation. According to this framework, the Zeeman splitting in frequency units must exceed both the stimulated emission rate and the decay rate for the masing transition. This requirement is likely to be met in class I methanol masers, as revealed in section 4.2 of the referenced article. It is interesting to note that the two sources, DR21W and M8E, which exhibit Zeeman splitting in the 36 GHz line, also demonstrate circular polarization in the 133 GHz (@xmath45) transition. In order to satisfy the @xcite mechanism's requirement that the Zeeman splitting exceed both stimulated emission and decay rates, it is likely for class I methanol masers. Notably, sources like DR21(OH) and M8E(@xcite), which have detected Zeeman splitting in the 36 GHz line, have also demonstrated circular polarization in the 133 GHz transition. Future high-sensitivity observations at 133 GHz may indicate an S-curve signature in Stokes V, which would test the consistency of the magnetic field implied from a pure Zeeman interpretation and the values derived from the 36 GHz. Since both the 36 GHz and 133 GHz methanol lines are @xmath16 transitions, the same Land @xmath38-factor would be applicable to each. Careful laboratory measurements of the Zeeman splitting coefficients for the brightest methanol maser transitions would be valuable. Given the fact that both the 36 GHz and 133 GHz methanol lines correspond to @xmath16 transitions, a constant factor @xmath38 would have the same land impact on both. Measuring the Zeeman splitting coefficients appropriate for the brightest methanol maser transitions would be very helpful. Excitation models predict that conditions leading to inversion in class I transitions often result in anti-inversion in class II transitions, and vice versa. The coincidence of class I 36 and 229 GHz masers at the same velocity and location as the class II 6.7 GHz maser site is surprising, particularly when these masers occur near the brightest continuum emission near the outflow. Precedents for overlaps between class I and class II methanol masers have been observed previously. (@xcite) In fact, the coincidence of these masers with the brightest continuum emission near the outflow provides a significant indication about their origin. Previously, overlaps between Class I and Class II methanol masers have been identified, particularly in the OMC-1 star-forming region, as demonstrated by Xcite's detection of spatial coincidences between the 6.7 GHz Class II masers and the 25 GHz Class I methanol masers. However, the angular resolution in their interferometric observations is much less than the data that we report for the 229 GHz data or the 6.7 GHz data that Xcite utilized. Xcite's model proposes that the 6.7/25 GHz overlap arises from a lower-temperature regime than is typical for Class II maser formation. Accordingly, this requires an intermixed environment of gas and dust at a lower temperature of approximately 27 Kelvin than traditional Class II maser creation. Moreover, strong 226 GHz continuum emission in DR21(OH) also lends support to this hypothesis. In traditional class II maser formation, it is typically assumed that the lower-temperature regime is significantly greater than 47 Kelvin. However, research conducted by a study focused on the 6.7/25 ghz overlap suggests that an intermixed environment of gas and dust at a lower temperature of 47 Kelvin plays a significant role in class II maser formation. This conclusion is supported by the strong 226 ghz continuum emission in the region around DR21(OH). The study did not investigate excitation at 229 ghz, but it did predict inversion at 36 ghz, which correlates with a @xmath20 transition. Moreover, this low-temperature regime predicts that 44 and 95 ghz masers should not appear at a 6.7 ghz maser site since the former are @xmath19 transitions, while the latter is a @xmath49 transition. The @xcite model, specifically regarding excitation at 229 ghz, did not investigate this specific transition. However, the model predicts inversion at 36 ghz, which is also a @xmath48 transition. The model suggests that 44 and 95 ghz masers should not appear at a low-temperature 6.7 ghz masing site, because the former are @xmath17 transitions, whereas the latter is a @xmath49 transition. Neither @xcite, @xcite, nor @xcite reported 44 ghz masers at this location. \n\nIn each of the sources we observed, the 36 ghz masers can be divided into two sets: those that are found to be cospatial with other class 1 maser transitions and those that are spatially isolated. In the outflow of DR21(OH) and in the cluster in the north of DR21w, the distribution of the 36 ghz masers is similar to that of the 44, 84, 95, and 229 ghz masers. In our investigation, we focus on the two groups: the outflow of DR21(OH) and the cluster that is located in the north of DR21W. The distribution of 36 GHz masers is similar to the distribution of 44, 84, 95, and 229 GHz masers, and the brightest masers in all of these transitions appear in the same area. In DR21(OH), the brightest masers in all class I transitions occur within a narrow velocity range (50 to 51 km/s) at the western tip of the outflow. The next - brightest masers in each transition appear approximately 10 km east of this position. This indicates that these masers exist in a physical regime where strong maser emission is produced in a large number of molecular hydrogen and atomic hydrogen transitions simultaneously. In proximity to the position in question, the subsequent and significantly brighter methanol masers can be detected approximately 10 east of it (10 west of the origin in Figure [Fig - Map - OH]). This situation indicates that the nearby energetic source generates an outflow which, in turn, impacts the surrounding molecular material, resulting in strong maser emission in a significant number of 60 and 125 gigahertz transitions concurrently. Although multiple transitions of methanol are pumped in some regions as a consequence of this outflow, other sources have distinctive regions where 36 gigahertz masers are not coincident with other transitions. This is illustrated by the fact that numerous masers in both the 36 and 44 gigahertz transitions reside to the south of the outflow in DR21(OH). \n\nNote: The original text's formatting has been adjusted and rephrased as per Wikipedia guidelines. However, not all sources exhibit coincidence between 36 and 44 GHz masers. In some cases, such as that observed in DR21(OH), numerous masers at both frequencies are located to the south of the outflow, but for the most part, the two transitions are not spatially coincident. Most 36 GHz masers and some 44 GHz masers in the south are seen near the periphery of CS emission at Xcite. The shared characteristic of the environment for these masers is the absence of a nearby active source. It is possible, as in the Sagittarius A region, that these masers trace molecular density clumps that are shock-excited by core collision or compression. The majority of the 36 GHz masers and some of the 44 GHz masers, situated in the southern region, are primarily detected near the outer edges of molecular emission @xcite. An intriguing characteristic of their surrounding environments is the absence of a nearby source of significant energy, similar to the Sagittarius A region. It is plausible that these masers indicate the presence of clumps of high molecular density that undergo shock excitation as a result of core collisions or compression @xcite. Interestingly, the 229 GHz masers appear to align more closely with the 44 GHz masers than with the 36 GHz masers in the DR21(OH) outflow. In the western side of the outflow, 229 GHz masers are observed in conjunction with 36 GHz masers, however, on the eastern side, several 229 GHz masers appear coincident with 44 GHz masers, but without 36 GHz emission. While 229 GHz masers are often aligned with 36 GHz masers on the western side of the outflow, there consistently exists an accompanying 44 GHz maser. Conversely, there are instances in which 229 GHz masers coincide with 44 GHz masers in the absence of 36 GHz emission, primarily found on the eastern side of the outflow. However, several caveats must be considered when drawing conclusions based on these observations. Firstly, the angular resolution of the 36 GHz observations is inferior to that of the 44 GHz and 229 GHz maser observations, thus it is plausible that sensitive, high-resolution observations of the 36 GHz masers will reveal further features at the intersection of the overlapping masers. Secondly, every detected 229 GHz maser in Dr21n appears close to a 36 GHz maser site, as represented in Figure [Fig. Maps N W]. Firstly, the angular resolution of observations conducted at both 44 GHz and 229 GHz is higher than those of the 36 GHz observations that we report. This highlights the possibility of revealing additional features at the sites of 44/229 GHz overlap through more sensitive and higher-resolution observations. Secondly, every detected 229 GHz maser in DR21n is situated near a 36 GHz maser, as depicted on Figure [Fig - Maps - N - W]. Thirdly, currently our research only covers three sources in DR21 that have been mapped using all three transitions: 36, 44, and 229 GHz, which restricts the set of sources to draw conclusions from. Therefore, more interferometric observations of both the 36 and 229 GHz masers are required to gain a better comprehension of their phenomenon. Although a limited number of sources have been observed, drawing general conclusions about the properties of 36, 44, and 229 gHz masers is challenging. Further interferometric observations of both the 36 and 229 gHz transitions are necessary to fully understand the phenomena of these masers. To date, three sources in the 36 and 229 gHz class I methanol maser lines have been interferometrically imaged. These observations reveal a diverse range of environments characterized by multiple masers in both transitions. Three specific environments have been identified that may correspond to distinct excitation conditions. In the investigation of several masers present in both transitions, our analysis reveals a multitude of distinct settings. Among the identified environments is a prominent outflow in DR21(OH), displaying an overwhelming number of overlaps in the 36, 44, and 229 GHz transitions. Furthermore, striking emission is detected at both 84 and 95 GHz, although these lines have not been subject to high-resolution observation. Notably, the brightest masers found in every transition are situated at the same peak velocity and position, specifically at the western edge of the outflow. A strong emission at 84 and 95 gigahertz (GHz) is also present, though the lack of high-resolution observations hinders further investigation. The bright masers in all transitions appear simultaneously at the western tip of the outflow as traced by the masers. The bright continuum source associated with class II 6.7 GHz masers and class I 36 and 229 GHz masers is present, but class I 44 GHz masers are absent. The low-temperature intermixed dust and gas model may provide an explanation. The 229 GHz transition produces detectable masers at select 36 GHz maser sites in DR21n. In DR21(OH) and another source located well south of the outflow, the 36 and 44 GHz masers possess a similar large-scale distribution, but they rarely produce masers simultaneously at the same site. According to the low-temperature intermixed dust and gas model proposed by Xcite, this particular environment exhibits detectable masers in the 229 GHz transition at specific sites in Dr21N, which are part of a subset of the 36 GHz maser emission in the same region. In contrast, the 36 and 44 GHz masers are found to have a similar distribution but are rare to be detected at the same location. In Dr21(OH), situated to the south of the outflow, a similar trend is observed. The 36 GHz maser in Dr21W yields an asymmetric Stokes V profile, which suggests a strong magnetic field if it is due to Zeeman splitting. However, a significant uncertainty exists regarding the Zeeman splitting coefficient for the 36 GHz transition. As a result, an inferred high-density may surpass the range where Class I masers are thought to typically form. A smaller magnetic field with a changing orientation or other circular polarization causes for the feature's observed circular polarization. There is significant uncertainty regarding the Zeeman splitting coefficient suitable for the 36 GHz transition, leading to a vastly higher estimated density that goes beyond the range thought to be associated with class I masers' formation. While it's possible that the observed circular polarization in this maser feature could be attributed to a smaller, fluctuating magnetic field, laboratory measurement of the Zeeman splitting coefficients for methanol maser transitions is a necessity. The recent identification of Zeeman-like Stokes V signatures in various methanol transitions warrants such measurements. However, to achieve the goal of identifying the physical conditions present in a variety of star-forming regions via the presence or absence of various methanol maser transitions, understanding a few sources in greater detail is indispensable. As such, increased theoretical effort and more sensitive observations of multiple class I transitions at higher angular resolution represent imperative steps in advancing this field. To gain a deeper comprehension of the sources under study, it is imperative to conduct more in-depth analysis of selected sources. This necessity arises as our results imply the need for increased theoretical effort and more sensitive observations of multiple Class I transitions at elevated angular resolution. Specifically, high-resolution maps of the 84 and 95 GHz transitions as well as higher-frequency maser lines may prove both enlightening and timely in the wake of the ALMA era. Furthermore, a further survey of sources in the 36 and 229 GHz transitions, coupled with 44 GHz maps in the literature, may aid in determining possible methanol excitation conditions in nature. T. C. M. acknowledges support from the National Science Foundation's Research Experiences for Undergraduates program. A subsequent analysis of sources in the 36 and 229 GHz transitions, coupled with 44 GHz charts from published literature, may be pivotal in determining the spectrum of conceivable methanol excitation conditions in natural settings. This research receives support from the National Science Foundation's Research Experiences for Undergraduates Program (REU). The National Radio Astronomy Observatory (NRAO) is a facility of the National Science Foundation (NSF), operated by Associated Universities, Inc. (AUI). The Submillimeter Array (SMA) is a joint project involving the Smithsonian Astrophysical Observatory and the Academia Sinica Institute of Astronomy and Astrophysics, and is funded by both the Smithsonian Institution and the Academia Sinica. The authors would like to extend their appreciation to W. H. M. Snel, T. Vlemmings, and A. Sarma for their supportive discussions on methanol Zeeman splitting coefficients. The Submillimeter Array (SMA), a collaboration between the Smithsonian Astrophysical Observatory and the Academia Sinica Institute of Astronomy and Astrophysics, is supported by the Smithsonian Institution and the Academia Sinica. We express our gratitude to W. H. T. Vlemmings and A. Sarma for valuable discussions regarding methanol Zeeman splitting coefficients. A. M. Sobolev et al. (2005) in \"Massive Star Birth: A Crossroads of Astrophysics\" edited by R. Cesaroni, M. Felli, E. Churchwell, and M. Walmsley (Cambridge: Cambridge University Press), page 174, discusses this topic. \"In the scholarly publication titled '227: Massive Star Birth - A Crossroads of Astrophysics,' edited by R. Cesaroni, M. Felli, E. Churchwell, and M. Walmsley (published by Cambridge University Press), a team of esteemed scientists explores the intersection between astrophysics and massive star birth.\"",
        "abstract": " class  i methanol masers are believed to be produced in the shock - excited environment around star - forming regions . \n many authors have argued that the appearance of various subsets of class  i masers may be indicative of specific evolutionary stages of star formation or excitation conditions . until recently , however , no major interferometer was capable of imaging the important 36  ghz transition . \n we report on expanded very large array observations of the 36  ghz methanol masers and submillimeter array observations of the 229  ghz methanol masers in dr21(oh ) , dr21n , and dr21w . \n the distribution of 36  ghz masers in the outflow of dr21(oh ) is similar to that of the other class  i methanol transitions , with numerous multitransition spatial overlaps . \n at the site of the main continuum source in dr21(oh ) , class  i masers at 36 and 229  ghz are found in virtual overlap with class  ii 6.7  ghz masers . to the south of the outflow \n , the 36  ghz masers are scattered over a large region but usually do not appear coincident with 44  ghz masers . \n in dr21w we detect an `` s - curve '' signature in stokes v that implies a large value of the magnetic field strength if interpreted as due to zeeman splitting , suggesting either that class  i masers may exist at higher densities than previously believed or that the direct zeeman interpretation of s - curve stokes v profiles in class  i masers may be incorrect . \n we find a diverse variety of different maser phenomena in these sources , suggestive of differing physical conditions among them . ",
        "section_names": "introduction\nobservations and data analysis\nresults\ndiscussion\nconclusions and future work"
    },
    {
        "article": "The Pantone Color Institute has recently announced the 2020 Color of the Year, and it is classic blue, known as PANTONE 19-4052. Classic blue, as described by Pantone, is \"a timeless and enduring blue hue that highlights our desire for a dependable and stable foundation on which to build as we cross the threshold into a new era.\" Classic blue is a versatile color, complementing a range of hues, from the bold and vibrant to the soft and subtle. It is a shade that emits a calming and soothing vibe, often evoking feelings of trust, confidence, and dependability. The Pantone Color Institute's announcement of this classic hue highlights the current societal yearning for connection and security, reflecting a world that seeks consistency and stability amidst uncertainty and turmoil. Classic blue is a reassuring, serene shade that is thought to instill a sense of calm and provide a soothing environment. As Leatrice Eiseman, Executive Director of the Pantone Color Institute, asserts, \"This is the color of where you have faith and the steadfastness of spirit.\" Classic blue has a timelessness that transcends fleeting styles and trends, allowing it to remain forever fashionable. It is a color that can be worn year-round, with the ability to be worn in everything from work attire to eveningwear. In interior decor, classic blue lends itself to the creation of elegant and sophisticated spaces. Whether a statement piece or used as an accent, classic blue injects elegance and refinement into any room. The color has also been found to have a calming effect, making it a popular choice to promote relaxation, making it perfect for bedrooms or meditation spaces. In the realm of product design, classic blue provides durability and longevity, making it an ideal choice for furniture and fixtures for high-traffic areas. Classic blue, in its versatility, can range from a pale sea blue to a deep navy hue. The shade is a reflection of both comfort and stability, reminding us what it means to be secure and to embrace simplicity. The Pantone Color Institute's selection of classic blue for 2020, as evidenced by their insight into societal yearnings and expectations, demonstrates their commitment to aiding designers and consumers in making meaningful and thoughtful choices. Despite the growing attention given to interdisciplinary research, citation impact metrics that account for this aspect of scientific output have not been established. Conversely, funding agencies such as the National Science Foundation have created calls for interdisciplinary projects, and the European Research Council advocates for applications from scientists with multidisciplinary publications. The Marie Curie Fellowships also consider the interdisciplinary aspects of research. However, evaluating interdisciplinary research is a pressing challenge, leading to some scholars opting for traditional methods to avoid potential risks associated with pursuing an interdisciplinary career path. @xcite highlights this tension. There is a pressing need to evaluate interdisciplinary projects and scholars within the scientific community. The difficulties of assessing interdisciplinary research often lead young academics to remain within traditional academic paths due to the perceived high risks associated with such a career path. This work presents a method to rank scientific publications and producers based on the scientific impact and breadth of their contributions across several scientific disciplines. The approach proposes the identification of the most central elements within a complex and multilayered network, representing scientific producers and interdisciplinary citations among multiple scientific fields. The network's bipartite structure enables the quantification of interdisciplinarity by considering citations across various disciplines, resulting in a ranking of scientific publications and their producers based on scientific impact and breadth of disciplinary contributions. The method used for detecting scientific producers' and publications' interdisciplinary impact involves the examination of the most significant elements in a complex bipartite interconnected multilayer network that represents scientific producers and scientific citations in various disciplines. This network consists of multiple layers, each representing a scientific field. By considering the diverse information provided in this network, instead of aggregating it, the method enables the quantitative measurement of the cross-disciplinary versatility of scientific publications and their producers. This line of research has garnered significant attention since De Solla Price and Garfield's efforts to understand the distribution of citations and scientific recognition dynamics. The ultimate objective is to develop more precise and just scientific impact metrics for evaluation purposes, as highlighted in this fundamental research field. Since the groundbreaking work by De Solla Price and Garfield in the early years, scientists have placed a significant effort into trying to understand the citation distributions and the non-trivial dynamics of scientific recognition. This fundamental body of work intends to lay the foundation for defining more accurate and fairer scientific impact metrics for evaluation purposes. In recent decades, numerous indices have been presented, and one of the most widely adopted indicators to evaluate scholars' scientific impact is the h-index (@xcite), which implies that a scholar has an index @xmath0 if @xmath0 of their publications have received at least @xmath0 citations each. There are numerous variants of this index as well. The concept of measuring scientific impact through citation counts has been widely adopted, with the h-index as a popular indicator. Scholars who produce publications with at least h number of citations each have h@xcite. Recently, an alternative approach has emerged, based on constructing networks of scientific citation chains. This method offers a more comprehensive understanding of the variation in citation patterns among publications and their producers. For instance, a publication which has received ten citations from highly cited papers presents a significant difference compared to another paper that has received ten citations from low-cited papers. This approach allows for a more nuanced and in-depth analysis of how scientific impact is distributed among scholars and publications. The analysis of citation networks facilitates a comprehensive appraisal of the intricate relationships among publications and research producers. By reconstructing the chains of citations, this visualization uncovers unique features that distinguish highly cited publications from those that receive citations from less frequently cited papers. In essence, for instance, a publication that receives ten citations from widely acclaimed publications is fundamentally more impactful than a publication that, in contrast, receives the same number of citations from comparatively lesser-cited papers. To rank publications, journals, or scholars based on the importance of their citation network, researchers apply diffusion algorithms that employ a simulation strategy, which approximates the spreading of scientific credits on the network  @xcite. This concept is analogous to the Google algorithm used for ranking webpages  @xcite. @xmath1. In recent research, scientists have suggested using diffusion algorithms as a means of simulating the propagation of scientific credits in a network. With this approach, the same concept used by Google to rank web pages is applied to rank scientific publications and their producers in a multilayer framework. By considering an aggregated network, both authors' centrality would be identical due to their equal number of publications and incoming citations. However, the multilayer framework takes into account interdisciplinary citations, providing a higher versatility score for one author who had an impact in both physics and biology as opposed to the other author who restricted their impact to a single field. In this study, we propose ranking scientific works and their creators based on their versatility, which involves examining a bipartite interconnected network that involves citations within and across different subject areas. In this study, we propose a method to rank scientific publications and their producers by utilizing a bipartite interconnected multilayer framework that considers citations within and across different disciplines. Employing a pagerank method on this structure, we rank nodes based on their versatility in an interconnected multilayer network. To account for interdisciplinarity, we construct a bipartite interconnected multilayer network representing citations between publications and relations between publications and their manufacturers, comprising @xmath3 nodes and @xmath4 layers. The @xmath5 multilayer adjacency tensor is defined as: In our analysis, we create a bipartite interconnected multilayer network to investigate the relations between publications, including papers or patents, and their manufacturers, such as scholars, inventors, research institutions, companies, or countries. This involves defining the rank@k multilayer adjacency tensor with @xmath3 nodes and @xmath4 layers. A specific relationship between layer @xmath9 and layer @xmath10 is encoded in the rank@k adjacency tensor @xmath7 using @xmath11 to indicate the intensity of the connection between node @xmath12 in layer @xmath9 and node @xmath13 in layer @xmath10, and @xmath14 to represent the base in the space @xmath15. Here, when @xmath16, @xmath17 is the intra-layer adjacency tensor.\n\nFor this network, we consider @xmath21 nodes, where @xmath22 is the total number of publications and @xmath23 is the number of manufacturers of the same type, who produced the @xmath22 papers. The @xmath24 ordered set of nodes consists of @xmath22 elements, the first @xmath22 elements represent publications, and the remaining @xmath23 elements represent manufacturers. The adjacency tensor of our multilayer network is formulated in general form for @xmath21 nodes, where @xmath22 is the number of publications, @xmath23 is the number of manufacturers in focus for the chosen type of paper, and @xmath24 is the ordered set of nodes. In our network, the first @xmath22 nodes represent publications, and the subsequent @xmath23 nodes indicate manufacturers. We have @xmath27 layers representing the @xmath28 scientific disciplines that the publications belong to. The 4 components of the rank@xmath8 adjacency tensor, denoted @xmath29, are defined as follows: @xmath30 and @xmath31, with @xmath32 $] , encode information on publication citations. The rank@xmath8 adjacency tensor consists of four components. These components, denoted as @xmath30 and @xmath31, respectively, encode information related to publication citations. Each layer in this tensor represents a discipline or a subfield. Specifically, if both publications @xmath34 and @xmath35 belong to discipline @xmath36, and publication @xmath34 cites publication @xmath35, then the corresponding element of the adjacency tensor @xmath37 @xmath38 is equal to one, as each citation contributes one unit of value overall. Normalization is performed to ensure that every element of the adjacency tensor has a sum of one. The normalization process utilized in data analysis assigns a single value of one unit to every citation. This is done to ensure that each citation is given equivalent weight regardless of the number of disciplines that the referenced publications belong to. Interdisciplinary citations are encoded differently using @xmath31 and @xmath39, where the layer @xmath41 is used to capture the relationship between the publications and their manufacturers. This corresponds to the tensors @xmath42 with @xmath43 \" as the dimension to preserve the structural relationship between publications and their manufacturers in the analysis. \n\nTechnically, let @xmath41 denote the remaining layer, then the tensors @xmath42, with dimensions @xmath43, encode information about the correlation between a reference's publication and its manufacturer in the context of a particular citation. By employing this framework, it enables the effective decoding and interpretation of the citation network's complex structure, providing insights into both the publications and their manufacturers. In the context of disciplines @xmath34 and @xmath36, publication @xmath34 belongs to the former while publication @xmath35 belongs to the latter. However, publication @xmath34 cites publication @xmath35, indicating significant overlap in subject matter. To incorporate this relationship and its related information, we introduce a tensor called @xmath42, which connects publications with their manufacturers, defined via the dimension $ ]. If the manufacturer of interest is scholars, we represent this using the tensor @xmath44, where each author of a publication is assigned a value of 1 or 0 based on whether they are an author of the citing publication. This approach allows us to associate publications with their corresponding institutions or countries based on author affiliations. In contrast, our @xmath45 tensor is a zero tensor specifically designed to maintain asymmetrical relationships between publications and manufacturers to prevent unrealistic paths when computing node centrality. By utilizing this framework, we can analyze and comprehend complex interactions among publications and their manufacturers. When analyzing research institutions, we connect each publication to the institutions to which its authors are affiliated. If we consider countries, the connections are to the countries in which these institutions are based. Our approach involves defining two null tensors, @xmath45 and @xmath46, to avoid unrealistic paths when calculating nodes' centrality and to avoid unnecessary addition of citation edges between authors. Essentially, @xmath45 tensors are null because we do not want the relations between publications and manufacturers to be symmetric. @xmath46 is also null because all the citation information is already encoded in the other tensors.\n\nHere, for the citation layers, each node is considered active on a given layer only if the publication it represents belongs to the corresponding field. For example, a monodisciplinary publication is active only on one layer, whereas an interdisciplinary publication, which pertains to both physics and biology, is active on two layers. This approach allows us to identify and analyze the publication's affiliations and its field or fields of study. It also avoids overrepresentation of interdisciplinary publications in their fields of study. In this framework, each node in question refers to a publication in a specific layer. Each layer corresponds to a particular field, and each node is active on a given layer if the publication represented belongs to that field. For example, a monodisciplinary publication is active only on one layer, while an interdisciplinary publication, with relevance to both physics and biology, is active on two layers. This framework enables a representation of the interdisciplinarity degree of a publication. A publication restricted to a single discipline has intra-layer incoming edges only, while one with an influence on researchers in more than one field has inter-layer incoming edges too, representing the bridges between the fields involved. Therefore, by introducing a second kind of node for scholars, inventors, research institutions, or countries, the network can rank publication producers as well. This framework enables a natural representation of the interdisciplinarity degree of publications, a crucial component in our objective of ranking publication producers. By integrating a second type of nodes, scholars, inventors, research institutions, or countries active on a dedicated layer, the network accounts for multiple perspectives. Each publication has directed outgoing inter-layer edges pointing to each of its producers, which ensures that no information is lost in the process. In previous works, ranking producers has been based on one-mode projections of the bipartite network of publications and producers. However, in this work, the complete bipartite structure is utilized to maintain the full network benefits, as explained in the supplementary material. By adopting a process of diffusion of scientific credits from paper to paper through citation edges within and across disciplines, the proposed network facilitates a comprehensive, interdisciplinary ranking of publication producers. Previously, research on ranking producers has been based solely on one-mode projections of the bipartite network of publications and producers, which can result in information loss. In contrast, this work aims to take advantage of the full bipartite structure. The proposed network involves a diffusion process of scientific credits where producers are represented as nodes without outgoing edges. The incoming edges originate from the papers they have produced. A schematic representation of this network is illustrated in the figure (see supplementary material). Defining a multilayer citation network, we propose to rank its nodes based on their versatility pagerank score, calculated as the steady-state solution of the tensor equation: \nX(t) = \u03b3(1-\u03c0)LX(t) + \u03b3\u03c0 D \u00d7 L X(t) / |V| , where X(t) is the time-dependent tensor that gives the probability of finding a random walker at a specific node in a particular layer V of the network, L is the row-normalization of the network's transition matrix, D is a diagonal matrix with the nodes' degrees, \u03b3 is the damping parameter, and \u03c0 is the teleportation rate. Nodes with a higher pagerank versatility indicate a more significant and influential role in both individual and diverse fields of study. To illustrate our proposed network, a schematic representation is shown in Figure [Explicative - Figure]. We define a multilayer citation network, and propose to rank its nodes according to their pagerank versatility, calculated using the steady-state solution of the equation:\n\n@xmath47\n\nwhere @xmath48 represents the time-dependent tensor that provides the probability to find a random walker at a particular node @xmath49 in a particular layer @xmath50, and @xmath51 denotes a random walker's teleportation rate. The dimensions of the tensors are defined as: @xmath47 is a four-dimensional tensor, where @xmath3 is the number of nodes in the network, @xmath4 is the number of layers, and @xmath52 is the teleportation rate. \n\n@xmath53 is referred to as the rank@x math5 tensor of transition probabilities for jumping between pairs of nodes and switching between pairs of layers, while @xmath54 is a rank@x math5 tensor with all components equal to 1. The eigentensor of the transition tensor, @xmath55, represents the steady-state probability to find a random walker in node @xmath49 and layer @xmath50. This tensor provides the pagerank of each node @xmath49 in each layer @xmath50 and takes into account the whole interconnected structure to solve the eigenvalue problem. Let $x_{math55}$ denote the eigentensor associated with the transition tensor $x_{math56}$, serving as the steady-state probability of finding a random walker in node $x_{math49}$ for layer $x_{math50}$. This tensor determines the pagerank of each node in each layer; however, it is crucial to clarify that this is not equivalent to calculating pagerank separately for each layer. Rather, the structure of the entire interconnected network is accounted for while resolving the eigenvalue problem. To obtain the multilayer pagerank of every node regardless of its layer, the values obtained from each replica's pagerank in various layers are projected, resulting in the multilayer pagerank vector $x_{math57}$ where $x_{math58}$ is a vector consisting of all components having equal values. The merit of this operation is backed by research  @xcite which demonstrates that it provides the same results obtained by calculating pagerank using simulated random walkers that explore the system based on transition rules contained in $x_{math56}$. In order to demonstrate the proposed ranking method, we tested it on two case studies: the American Physical Society and the US patents datasets. The former is a collection of papers published in the journals of the American Physical Society between 1985 and 2009. In previous studies, @xcite has demonstrated that a particular operation produces the same results as calculating PageRank through simulated random walkers that traverse a multilayer structure utilizing transition rules encoded in @xmath56. To present this ranking method, it was tried out on two case studies: the American Physical Society (APS) and the US Patents datasets. The first collection studied comprised papers published by APS journals such as Physical Review Letters, Physical Review, and Review of Modern Physics between 1985 and 2009, and only papers with no more than ten co-authors were incorporated to minimize biases. The authors' names were disambiguated through a procedure introduced in prior studies @xcite. The meta-data in the dataset supplies information regarding the topic of the papers through their assigned \"Physics and Astronomy Classification Scheme\" (PACS) code, developed by the American Institute of Physics (AIP) and used by Physical Review since 1975 to categorize fields and sub-fields of physics. In this study, we employed a straightforward technique presented in previous research @xcite, utilizing meta-data from the dataset as an informative tool. The meta-data provides information about the topic of papers, specified through the use of the American Institute of Physics's Physics and Astronomy Classification Scheme (PACS) code. We constructed a complex, 10-layer network based on PACS codes, with each layer corresponding to a sub-field of physics as defined by the PACS classification system. The sub-fields include:\n1. General\n2. Physics of elementary particles and fields\n3. Nuclear physics\n4. Atomic and molecular physics\n5. Electromagnetism, optics, acoustics, heat transfer, classical mechanics, and fluid dynamics\n6. Physics of gases, plasmas, and electric discharges\n7. Condensed matter: structural, mechanical, and thermal properties\n8. Condensed matter: electronic structure, electrical, magnetic, and optical properties\n9. Interdisciplinary physics and related areas of science and technology\n10. Geophysics, astronomy, and astrophysics.\nThe dataset comprised of 319,816 papers, 204,809 authors, 626 institutions, and 54 countries. Though physics primarily encompasses a large field, it is clear that the method employed in this study is a potent indicator of its potential useability in more interdisciplinary analyses. While it may fall short of a complete interdisciplinary analysis, this study serves to prove its usefulness. The final dataset encompasses 319,816 published papers, 204,809 authors, 626 institutions, and 54 countries. This dataset predominantly focuses on physics, although it incorporates topics like biological physics, astrophysics, and more within its vast scope. While it may not cater to a full interdisciplinary analysis, it serves ample proof of the effectiveness of the method. The second dataset includes 3.7 million U.S. patents granted between 1963 and 1999, with all citations made to said patents between 1975 and 1999. The paper authors utilized six categories proposed in previous studies: Chemical (excluding drugs), Computers and Communications, Drugs and Medical, Electrical and Electronics, Mechanical, and \"others\". Each patent is assigned to a main category decided upon by the United States Patent and Trademark Office (USPTO), as well as any subsidiary categories. From January 1963 to December 1999, a total of 157,482 patents were granted by the United States Patent and Trademark Office (USPTO), and during the period between 1975 and 1999, 12,190 out of these patents were cited. To categorize the patents into distinct layers, we used the classification system proposed in previous studies. The system consists of six categories: Chemical (excluding drugs), Computers and Communications, Drugs and Medical, Electrical and Electronics, Mechanical, and Others. Each patent is assigned to a principal class defined by the USPTO and any number of subsidiary classes. These classifications provide the patents with one or more layers based on the categories they belong to. However, the dataset currently only contains the main class information, and therefore, we have supplemented it by extracting the other category information from the full-text patent grant data. In total, the dataset comprises 1,142,499 inventors, 138,833 assignees, and 127 countries. Furthermore, in order to supplement the initial dataset that contained only the information regarding the main class, we enriched it by extracting the relevant data from the USPTO Patent Grant Full Text. As a result, the final dataset contains 1574,882 patents, 114,249 inventors, 138,833 assignees (primarily corporations), and 127 countries.\n\nFigure 2 illustrates the evolution of the interdisciplinary ranking of the world top physics departments and world top companies over time. It provides valuable insights, revealing, for example, the increase in the standing of the University of Texas at Austin during the 1990s, following the establishment, in 1985, of the Center for Nonlinear Dynamics, directed by the highly acclaimed scientist Harry Swinney, who earned the Boltzmann Medal.\n\nNotably, the proposed method differentiates itself from previous algorithms used for the diffusion of scientific credit, as it rewards researchers who have implemented interdisciplinary works or created an impact in various scientific fields. When comparing the rankings obtained using Sara and the proposed method, a Spearman's rank correlation coefficient of 0.77 (at a 95% confidence level) is obtained. Unlike previously proposed diffusion algorithms for scientific credit, the proposed method prioritizes researchers who have undertaken multidisciplinary work or displayed an impact in various scientific areas. Through a comparison with the Science Author Rank (SARA) algorithm, a rank correlation of 0.77 on a confidence level of @xmath59 was found. This high correlation is due to the fact that both methods simulate diffusion processes on a citation network. However, the proposed method offers higher positions to versatile researchers such as Per Bak, Eugene Stanley, Shlomo Havlin, and Leonard M. Sander, who gained +21, +56, +104, and +93 positions respectively. The proposed method accurately captures two fundamental aspects of interdisciplinary research: intrinsic multidisciplinarity, which is the act of publishing papers or patents pertaining to different areas, and effective interdisciplinarity, which encompasses being credited by multiple scientific fields. The proposed methodology provides higher rankings to versatile researchers, such as Per Bak (+21 position gains), Eugene Stanley (+56), Shlomo Havlin (+104), and Leonard M. Sander (+93). This approach captures both intrinsic and effective interdisciplinarity. Intrinsic multidisciplinarity involves publishing papers or patents in various fields, while effective interdisciplinarity refers to being credited by different scientific areas. The _topical interdisciplinarity_ of an author is defined as the average number of different scientific areas their publications pertain to:\n\n@math61\n\\mu_I = \\frac{1}{N_P} \\sum_p ^ {N_P} \\frac{F_p}{N_F}\n\nwhere N_P is the number of publications, p is the index of the publication, and N_F is the number of fields the publication belongs to.\n\nIn addition, we introduce an entropy-based metric to measure the distribution of incoming citations across different fields:\n\n@math66\nH(x) = -\\sum_m^L p(m) \\log p(m)\n\nwhere L is the number of fields, p(m) is the proportion of incoming citations from field m, and x is a particular publication.\n\nIf a publication is only cited by other publications in its own field, the entropy would be zero. By measuring the entropy, the proposed method can capture a broader perspective of interdisciplinarity. To calculate the interdisciplinary contribution of an author @xmath2 (which could be a scholar or an inventor) within a scientific network, we determine @xmath61, the average number of distinct scientific areas in which their publications have been published. Here, @xmath62 is the total number of publications authored by @xmath2, and @xmath63 is the number of scientific fields represented by a publication @xmath64. The entropy metric for each publication @xmath65, based on the proportion of citations from different scientific fields, is calculated as @xmath66, where the sum is over the different fields (layers) and @xmath67 is the proportion of citations that come from layers other than its own scientific field @xmath68. The more distinct scientific fields a publication is cited by, the higher its entropy and the interdisciplinary impact it has. Next, we determine the citation interdisciplinarity of @xmath2 using the average entropy of their publications @xmath70. In a study that analyzed the correlation between citation-based rankings and interdisciplinarity, we found a strong positive correlation between the rank gain using the proposed interdisciplinary approach and topical interdisciplinarity, as demonstrated in figure 3 (panels (a) and (b)). Additionally, a positive correlation was found between the rank gain using the proposed approach and disciplinary diversity in incoming citations (figure 3 (panels (c) and (d))). To control for the effects of productivity, we performed the same analysis using two subsets of data: one which only includes authors with a fixed number of publications @xmath72, and another which only includes authors with a fixed number of publications @xmath73. The correlation coefficients found in both subsets were consistent with those using the whole dataset, indicating that the proposed method is not biased by productivity. Furthermore, we discovered that the rank gain is positively correlated with the disciplinary variety of scholars and inventors receiving incoming citations, as figure 3 (panels c and d) confirms. To control for productivity effects, we repeate the same analysis with two subsets of data, where we consider only authors who publish an equal number of papers (detailed in @xmath72 and @xmath73). The correlation coefficients in these subsets mirror those found using the entire dataset, confirming that our method is not biased by productivity. The results for these subsets can be found in the supplementary material. We use a coefficient and set statistical significance at @xmath74. Solid lines represent density gradient contours, and dashed lines represent linear regression models obtained via maximum-likelihood estimation, as demonstrated by the graph (panel scaledwidth=50.0%, ). The coefficient and statistical significance are central factors that are examined in this study, with solid lines depicting density gradient contours and dashed lines representing linear regression models estimated via maximum-likelihood. This paper proposes a methodology to quantify the citation impact of scientific publications and their producers by taking into account their interdisciplinary nature, which was not previously accounted for in existing citation impact indicators. Despite the introduction of several metrics, some limitations have been identified. The consideration of interdisciplinarity in assessing citation impact has not been included in previous indicators, as numerous metrics have been proposed for this purpose. Challenges in this area include accounting for self-citations, the appropriate choice of citation time window, field normalization, and author credit allocation. Despite the vast literature addressing these issues, consensus has not been reached on how to solve them. In this study, we propose a method that accounts for interdisciplinarity, and does not engage in the debates surrounding these issues. Nonetheless, the bipartite interconnected multilayer networks of citations and disciplines that we introduce can easily be adapted to cater to specific needs. In seeking to address issues related to interdisciplinarity in research, there is still a lack of consensus concerning solutions. To account for this issue, we propose a method that aims to incorporate interdisciplinarity by introducing bipartite interconnected multilayer networks of citations and disciplines. This approach can be tailored to meet specific requirements, such as weighted edges connecting papers to their authors based on credit allocation, and the selection of a specific time window to choose the papers comprising the network. Such a method could also assist funding agencies and academic hiring decision-makers in quantifying the impact of interdisciplinary research and its producers, ultimately advancing excellent science faster. Beyond the current evaluation of interdisciplinarity's benefits, the proposed method could serve as a valuable tool for funding agencies and academic hiring decision makers to measure the impact of interdisciplinary research and its producers. The authors, a.a., e.o., and m.d.d., gratefully acknowledge financial support from the Spanish government through research grant FIS2015 - 38266, while m.d.d. also acknowledges funding from the Juan de la Cierva program (IJCI-2014-20225). a.a. also partially acknowledges financial assistance from the European Commission FET - PROACTIVE project MULTIPLEX (grant no. ) for this work. In recent years, research has suggested approaches for ranking scholars that simulate diffusion processes. Such techniques originally emerged in @xcite, which was later expanded upon in @xcite for ranking scientific publications. However, the method proposed in this present work considers a distinct type of network - a bipartite, interconnected multilayer network. The motivation for this approach lies in the consideration of the complete bipartite structure, rather than its one-mode projection. In this section, we examine why taking into account the entire network structure is crucial. Previously, the concept of ranking scholars through a simulation of a diffusion process was first introduced in previous works such as @xcite and @xcite, which aimed to rank scientific publications. However, the method proposed in this work takes a different approach by considering a bipartite interconnected multilayer network instead of its one-way projections. To simplify the concept, let us focus on a bilayer version of the network consisting of 2 layers with 75 nodes each. The aim of this section is not to analyze the multilayer aspect of the network to capture interdisciplinarity but rather to explore its bipartition. Overall, this approach presents an entirely different network structure to analyze compared to the one-way projections. Our focus in this regard differs from that of capturing interdisciplinary aspects in a multilayer network, instead we direct our attention to the bipartition aspect. Specifically, we consider a network with @xmath75 nodes and two layers, @xmath76. The four components of the rank-@xmath8 adjacency tensor @xmath29 are defined as follows: @xmath77 encodes citing relations between papers, that is, @xmath78 if paper @xmath34 cites paper @xmath35. Similarly, @xmath79 encodes information about paper authorship, or @xmath80 if author @xmath35 is one of the authors of paper @xmath34. In contrast, we set @xmath81 and @xmath82 as zero tensors, consistently with the representation introduced in s.1.\n\nFor the sake of simplicity, since in the remainder of this section we will be dealing only with rank-2 tensors, we will use the classical matrix notation instead of tensorial one. Therefore, we will refer to @xmath77 as @xmath83 and @xmath79 as @xmath84. In the author-citation network proposed by @xcite, we will focus on the bipartition, that is, on nodes from two distinct layers, denoted as papers and authors. In the context of paper citation relations, @xmath77 represents a way to encode information about these connections. Specifically, it signifies that paper @xmath34 cites paper @xmath35. Similarly, @xmath79 encodes authorship information such that if author @xmath35 is one of the authors of paper @xmath34, that information is represented through this notation.\n\nFor simplicity's sake, in the remainder of this section, we will utilize the traditional matrix notation for rank-2 tensors, rather than the tensorial notation introduced in Section 1. Therefore, we will refer to @xmath77 as @xmath83 and @xmath79 as @xmath84.\n\nIn the author citation network proposed in @xcite, each node represents an author, and an edge exists between two node-authors if a publication authored by @xmath34 cites a publication authored by @xmath35. Each such publication contributes to the total contribution score for both @xmath34 and @xmath35. The contribution score for each citation is 1, with contributions being split equally among all authors in each publication.\n\nTo formalize this notion, we introduce an adjacency matrix @xmath91 of size @xmath92, encoding the citation links between papers. Each publication adds a contribution to certain authors, where the contribution is inversely proportional to the number of authors of a publication and the number of authors of the cited publication. This contribution is equal to one for each citation. A rectangular matrix can be created to encode these citation links, and from this matrix, it is possible to construct a normalized version of the projection matrix that represents the network of citations between authors. This is done through two successive matrix multiplications, resulting in a normalized adjacency matrix representing the citations between authors. Using authors' affiliations as described in paper @xmath34, we can also build the projection matrix @xmath98, of size @xmath99, for given text. If author @xmath35 is one of the authors of paper @xmath34, we can define the normalized version of @xmath98 as @xmath102, where @xmath103 represents normalization by the number of authors @xmath104. The adjacency matrix representing the network of citations between authors can be obtained through two successive matrix multiplications: @xmath105. This means that @xmath108 is a sum over the papers authored by @xmath34 that cite paper @xmath109, where each paper @xmath10 contributes 1 over the number of authors, as defined in @xcite. Each element @xmath111 in the resulting adjacency matrix is the sum over all pairs of papers @xmath112 where @xmath34 is an author of @xmath100 and @xmath35 is an author of @xmath109, and each element of the sum contributes 1, as defined in @xcite. Thus, we have demonstrated that the adjacency matrix of the author citation network can be obtained by performing two matrix multiplications involving @xmath91 and @xmath98. Each element of the network proposed in this paper, which is constructed by adding the two matrix expansions of adjacency matrices representing author citations and author-document connections, is a summation over all pairs of papers where an author is linked to both papers. The contribution of each pair is determined by a value specifically defined in the cited reference. In contrast, the conventional author-citation network matrix involves multiple matrix multiplications which lead to information loss. The proposed network structure, therefore, accounts for all information and preserves it as the bipartite structure is represented as a sum of both matrix expansions. In the network proposed in this paper, the adjacency matrix is the sum of two expansions represented by @xmath91 and @xmath98. By considering the entire bipartite structure, information loss is avoided. The example shown in Figure [ Toy - Model-1 ] highlights the deficiency of traditional author citation network frameworks, as they lead to an unfair ranking of authors due to information loss. However, employing our approach, the most central author is @xmath115, who is the author of both the most central paper and one of the second most central papers, whereas in the author citation network framework, the most central author is @xmath84 based on the importance of nodes from which these edges originate. In the context of author citation networks, the author with the highest centrality, as determined by the Pagerank algorithm, is @xmath84. This is due to the fact that the author citation network aggregates all of an author's incoming citations, regardless of whether they come from papers that cite @xmath84 directly or from other papers authored by a more central author, such as @xmath115. However, the resulting ranking may not accurately reflect the real importance of authors as it does not account for which papers actually cite each author. An alternative approach is to compute the centrality of papers in @xmath91 and sum their centralities for each author to determine their overall centrality. This involves applying a linear transformation to the paper Pagerank centrality vector. While this solution eliminates the need for aggregation, it can also lead to inaccurate results as it disregards direct citations. Unlike utilizing the paper or author citation network to determine centrality, an alternative method is to use the pagerank approach in the paper citation network. This method enables us to obtain the centrality of papers and then compute the author centrality as a sum of the normalized centralities of their authored papers. However, this approach can also lead to misleading results. For instance, in a hypothetical scenario highlighted in Figure [Toy - Model-2], the author with two papers, although less significant, is classified as more central than another author with a significantly more central paper due to the summation aggregation. This misclassification occurs because pagerank is a nonlinear diffusion process, and aggregation can overshadow the significance of individual papers. In contrast, our method classifies authors accurately based on the significance and centrality of their papers in the network. In the network analysis under consideration, author @xmath119 gains increased centrality due to having authored two papers, albeit these papers are classified as the most marginal papers in the system. Conversely, author @xmath84 is deemed more central due to being the author of a highly influential paper. However, adding up the centralities of multiple authors via a flat representation strategy of the network involves loss of information as diffusion processes are inherently non-linear. As demonstrated in Figure 3 of the main text, this shortcoming is overcome by the proposed method of evaluation, which yields a strong correlation between rank gains and topical interdisciplinarity, as indicated in panels (a) and (b) for scholars and inventors. This correlation holds even when controlling for productivity by considering subsets of authors with 20 publications (see Figure [control_20papers]) and 50 publications (see Figure [control_50papers]). Moreover, the rank gain is also positively correlated with disciplinary diversity in the incoming citations of scholars and inventors (panels (c) and (d)). In Figure 3 of the main text, we demonstrate a positive correlation between the increase in rank that scholars and inventors obtain when evaluated using our proposed method rather than a flat representation of the citation network, and their topical interdisciplinarity, as well as their disciplinary diversity in terms of incoming citations (panels (a) and (b) for topical interdisciplinarity and panels (c) and (d) for disciplinary diversity). To control for the effects of productivity, we performed the same analysis on subsets of the data comprising authors and inventors with 20 and 50 publications, respectively. The x-axis in both panels (a) and (b) represents the average number of different scientific areas covered by their publications, and in panels (c) and (d), it represents the diversity of the disciplines of scholars and inventors' incoming citations (citation interdisciplinarity). The proposed method results in distinct patterns of publications when compared to a traditional flat representation of citation networks. This can be seen in correlation analyses, where Pearson's coefficient demonstrates statistical significance (p=0.05). Panel (a) and (b) of the figure represent scholars' and inventors' topical interdisciplinarity, which is defined as the average number of different scientific areas their publications belong to. Panel (c) and (d) respectively demonstrate citation interdisciplinarity, which is the level of diversity in terms of disciplines of scholars and inventors' incoming citations. The x-axis indicates that interdisciplinarity is a crucial factor to consider when evaluating the interdisciplinary nature of publications. Density gradient contours and linear regression models have been employed to analyze the data, with solid lines representing contours and dashed lines representing regression models. Rather than employing a flat representation method to measure publication success, the proposed method yields more effective results, as demonstrated by two interdisciplinarity measures. In panels (a) and (b), scholars' and inventors' topical interdisciplinarity, defined as the average number of distinct scientific areas covered by their publications, is illustrated. In panels (c) and (d), scholars' and inventors' citation interdisciplinarity, or the variety of disciplines from which their incoming citations originate, is represented. Correlations are calculated using Pearson's coefficient and statistical significance in @xmath74. Density gradient contours (represented by solid lines) and linear regression models estimated via maximum likelihood are provided in the @xmath52 percentage-scaled figures. The paper \"Are there better indices for evaluation purposes than the h index?\" by Bornmann, Lutz, Mutz, R\u00fcdiger, and Daniel H.-D. raises the question of whether other indices may be superior for evaluation purposes. In solid lines, density gradient contours are represented, and dashed lines depict the maximum-likelihood estimated linear regression models in studies investigating the superiority of alternative variants of the h index in evaluating researchers' productivity, as explored by Bornmann et al. (2008) using data from biomedicine. Additionally, de Domenico et al. (2019) assessed nine different h index variants, with co-authors including Alert, Cozzo, Gmez, and Arenas. The article \"Mathematical Formulation of Multilayer Networks\" by De Bernardis, Domenico, Sol, Ribalta, Cozzo, Emanuele, Kivel, Mikko, Moreno, Yamir, Porter, Mason, Gmez, Sergio, and Arenas, Alex deals with the development of mathematical techniques for analyzing complex network systems. In their publication \"On the Structure and Evolution of Multilayer Networks\" featuring De Bernardis and his team, they offer an understanding of how networks in multiple layers interplay with each other. Glnzel et al. further elaborate the topic in their work \"Hierarchical Communities in Multilevel Networks\" where they investigate the hierarchical organization of communities in multilevel networks. In 2006, a team of authors including glnzel, wolfgang, debackere, koenraad, thijs, and bart, and schubert, andr., published a comprehensive review on the significance of author self-citations in information science, bibliometrics, and science policy. Their article, entitled \"A Concise Review on the Role of Author Self-Citations in Information Science, Bibliometrics and Science Policy,\" was featured in Volume 67(2) of a scholarly journal, with pages 263277. More recently, in 2021, Caroline S. Wagner, J. David Roessner, Kamau Bobb, Julie Thompson Klein, Kevin W. Boyack, Joann Keyton, Rafols Ismael, and Katherine Brner evaluated and discussed the potential benefits and limitations of author self-citations in scholarly publishing. Their article, which contains additional insights into this topic, was published in a current edition of the same journal. In a review article, known as \"Approaches to Understanding and Measuring Interdisciplinary Scientific Research (IDR)\" published in the journal *5*(1), pages 1426, Wagner et al. explore the literature regarding techniques and strategies for measuring and comprehending interdisciplinary scientific exploration. The authors, who include Caroline S. Wagner, J. David Roessner, Kamau Bobb, Julie Thompson Klein, Kevin G. Boyack, Joann D. Keyton, Ismael Rafols, and Katherine Brner, provide a comprehensive examination of the literature to delve into the subject area.",
        "abstract": " nowadays , scientific challenges usually require approaches that cross traditional boundaries between academic disciplines , driving many researchers towards interdisciplinarity . despite its obvious importance , there is a lack of studies on how to quantify the influence of interdisciplinarity on the research impact , posing uncertainty in a proper evaluation for hiring and funding purposes . here \n we propose a method based on the analysis of bipartite interconnected multilayer networks of citations and disciplines , to assess scholars , institutions and countries interdisciplinary importance . \n using data about physics publications and us patents , we show that our method allows to reward , using a quantitative approach , scholars and institutions that have carried out interdisciplinary work and have had an impact in different scientific areas . \n the proposed method could be used by funding agencies , universities and scientific policy decision makers for hiring and funding purposes , and to complement existing methods to rank universities and countries . ",
        "section_names": "introduction\nmethodology\ndata\nresults\ndiscussion\nacknowledgements\ncomparison with other approaches\nproductivity control"
    },
    {
        "article": "The Great Salt Lake located in northwestern Utah, USA, is the largest saltwater lake in the Western Hemisphere. It spans an area of approximately 2,478 square miles and holds around 33.7 million acre-feet of water, giving it an average depth of 12 feet. The lake's high salinity, which can reach up to 27.9% (more than five times that of seawater), creates a unique ecosystem that supports a diverse range of brine shrimp, brine flies, and other salt-tolerant species. Despite its name, the lake is not salty all year round due to fluctuations in rainfall and the inflow of rivers from the surrounding mountains. In fact, during wet periods, the lake's volume can expand to over 50 million acre-feet, covering a surface area of up to 3,300 square miles. The Great Salt Lake is an essential source of water for farming and drinking in the surrounding region, but it also faces challenges regarding pollution, loss of wildlife habitat, and the potential for its shoreline to recede due to an ongoing drought. Intrinsic hole-valence band systems of (III, V) semiconductors with Mn impurities have been commonly described utilizing a phenomenonological model. This model describes the coupling between the valence band holes of the host semiconductor and spin-valued Mn impurities through exchange and Coulomb interactions. In the strongly metallic regime, where disorder can be treated as a perturbation, these approximations lead to a simplified picture of the materials where spin-orbit coupling of the valence-band hole subsystem plays a significant role. This model accounts quantitatively for various experimental findings, including critical temperature, strain-sensitive magnetic anisotropy, anisotropic magneto-resistance coefficients, and strong anomalous Hall effects. Golden rule estimations of quasiparticle scattering amplitudes also provide estimates for longitudinal dc conductivities. This work discusses theoretical predictions for the infrared magneto-optical properties, such as magnetic circular dichroism, Faraday rotation, and Kerr effects, for various Mn concentrations and carrier densities, which all relate to the non-zero value of the ac Hall conductivity. This paper discusses the theoretical predictions for the infrared magneto-optical properties of Mn-doped (III, Mn)V ferromagnet materials. In particular, the paper evaluates magnetic circular dichroism (MCD), faraday rotation, and Kerr effects in the infrared realm for various Mn concentrations and carrier densities. The microscopic basis for these effects can be traced back to the non-zero value of the ac Hall conductivity, which is crucial for explaining the observed DC Hall effect. In metallic ferromagnets, precise measurements of magneto-optical coefficients offer an intricate understanding of the role of broken time-reversal symmetry on itinerant electron quasiparticle states in the relevant band energy scale, which in this case, lies within the infrared bandwidth. As a result, the study of infrared magneto-optical properties can provide valuable insights and contribute to a better understanding of these materials. In metallic ferromagnets, measurements of magneto-optical coefficients on the infrared energy scale have provided detailed insights into the effects of broken time-reversal symmetry on the behavior of itinerant electron quasiparticle states. Since heavily p-doped (III, Mn)V ferromagnets and other recently studied materials fall within this band energy scale, infrared magneto-optical studies of these ferromagnets are highly desirable. Comparing these experimental findings to the theoretical predictions presented here is expected to contribute significantly to understanding the physics of these new ferromagnets, revealing any potential lack of simplicity in the theoretical formulations employed. The potential applications for this research are also significant, particularly in the event of achieving room temperature ferromagnetism in the future. In contrast to the simplistic theoretical framework that we employ in our research, observations of certain magnetic semiconductors could reveal deficiencies. Specifically, the studied ferromagnets' magneto-optical response is potentially of great interest for future applications, especially if room temperature ferromagnetism is achieved. Notably, the magneto-optical properties of (II,Mn)VI paramagnets have already provided useful insights from both basic science and applied viewpoints. Previously conducted absorption and reflection measurements in the visible range provided rough estimates of the p-d and s-d exchange coupling constants in (II,Mn)VI materials, as well as the crucial role of valence band holes in (III,Mn)V s. Photoemission experiments also help to explore the degree of hybridization between the underlying host valence band and Mn electronic levels; however, their surface sensitivity represents a limitation. In this paper, we present absorption and reflection measurements, carried out in the visible range, which yield estimations for the p-d and s-d exchange coupling constants in (ii, Mn)VI materials and the significant contribution of valence band holes in (III, Mn)VS. To delve into the underlying electronic structure, XPS experiments were performed, which demonstrated the degree of hybridization between the host valence band and Mn electronic levels. Nevertheless, these experiments were found to be surface-sensitive. Furthermore, infrared optical conductivity measurements exhibited an unusual non-Drude behavior, including an absorption peak and back-scattering localization effects, in agreement with theoretical calculations. The manuscript's structure is as follows: in Sec. [TA], we introduce the Kubo formula description of the ac anomalous Hall conductivity that we utilize to discuss the ferromagnetic materials studied in this paper. We structure this paper in three main sections. In Section [TA], we introduce the Kubo formula description of the AC anomalous Hall conductivity, which we will employ to study (III, Mn)V ferromagnets. In Section [KL], we outline our model Hamiltonian and illustrate the approximations used in our calculations. Lastly, in Section [4Band], we present an analytic model evaluation for the anomalous Hall conductivity assuming a disorder-free system with bands approximated by a four-band spherical model. Importantly, the six-band model that we use for numerical calculations reduces to a four-band model when the spin-orbit coupling strength is infinite. In this paper, we conduct an analytic assessment of the anomalous hall conductivity for the scenario where disorder can be neglected and bands are approximated by the four-band spherical model. This model approximates the six-band model in the limit of infinite spin-orbit coupling strength. The isotropic band dispersion of this model facilitates analytic calculations, although they are somewhat complex. The specific details of these calculations are presented in an appendix, aiming to provide intuition about the qualitative properties of the Hall curve. In section [6band], we present the numerical results obtained from the full model Hamiltonian calculation for the Hall effect as well as the analysis of all the common magneto-optical effects available for experiments in this specific geometry. The specifics of the calculation, which aid in understanding the qualitative aspects of @xmath3 curves, are deferred to an appendix. In Section [6band], the numerical results of the full model Hamiltonian calculation for @xmath3 are presented, and these outcomes lead to a discussion of all the frequently experimented magneto-optical effects in the given geometry. The concluding section summarizes this work, and our findings are presented. Our theoretical model commences by coupling the host semiconductor valence band electrons, described within the @xmath4 or Kohn-Luttinger theory, with @xmath2 mn local moments, treated with a semi-phenomenological local exchange interaction at a mean-field level. In our theoretical model, the host semiconductor valence band electrons, described within @xmath4 or the Kohn-Luttinger (KL) theory, are coupled with Mn local moments through a semi-phenomenological local exchange interaction at a mean-field level. At zero temperature, this results in split valence bands due to an effective exchange field @xmath5, where @xmath6 is the substitutional Mn density and the strength of the exchange coupling is assumed to be @xmath7 meV nm@xmath8. Furthermore, we assume in this paper that the magnetization is aligned along the growth direction, disregarding any small external magnetic field disturbance. We limit our approach to the @xmath10 limit, disregarding scattering off thermal fluctuations in the Mn moments orientation. We assume a collinear magnetization in the ground state, ignoring the probability of disorder-induced non-collinearity in the ground state in strongly metallic (III,Mn)V ferromagnets. We restrict ourselves to the @xmath10 limit, disregarding scattering off thermal fluctuations in the orientation of the mn moments. We assume collinear magnetization in the ground state, negating the possibility of disorder-induced non-collinearity in the ground state, known to be less likely for strongly metallic (iii, Mn)V ferromagnets. Applying the linear response theory Kubo formula for the real part of the AC Hall conductivity of non-interacting disordered electrons, as derived by Kohn and Luttinger, the expression reads:\n\n@xmath11 \\[ \\sigma_{AC}^{xy} &= - \\frac{e^2\\hbar}{m^2}\\int\\frac{d\\vec{k}}{(2\\pi)^3} \\sum_{n \\ne n'}(f_{n',\\vec{k}}-f_{n,\\vec{k}}) \\times \\nonumber\\\\ & \\frac{{\\rm im}[\\langle n'\\vec{k}|\\hat{p}_x|n\\vec{k}\\rangle\\langle n\\vec{k}|\\hat{p}_y|n'\\vec{k}\\rangle]}{( \\omega - e_{n\\vec{k}}+e_{n'\\vec{k}}) (e_{n\\vec{k}}-e_{n'\\vec{k}}) } , \\label{ac_sig_ah}\t\\]\n\nwhere the $\\langle n' \\vec{k} | \\hat{p}_x | n \\vec{k} \\rangle$ and $\\langle n \\vec{k} | \\hat{p}_y | n' \\vec{k} \\rangle$ are the Bloch valence-band states and Bloch eigenenergies within the @xmath4 theory (we use either six or four band models here), respectively, the bare electron mass is @xmath14, the fermi occupation number is @xmath15, and the @xmath4 velocity operator is obtained by differentiating the Hamiltonian with respect to wavevector. The formula, in its zero-frequency limit, is used by Jungwirth {et al.} to explain the DC anomalous Hall conductivity of these materials. In recent scientific work, the ac sign-change (ac_sig_ah) of itinerant electron ferromagnetic materials has been reduced to an expression utilized by Jungwirth and colleagues. This finding shows that anomalous hall effects hold more significant quantitative value in characterizing such ferromagnetic systems than previously recognized, particularly for the current materials. This present study, which builds on this advance, examines the impact of ac frequency on these effects. Despite the fact that in the absence of disorder, the hall conductivity is non-zero, it is expected that the presence of disorder will have an influence, primarily by broadening out features in the @xmath17 spectra. We extend this advance to finite frequencies and explore how disorder affects the Hall conductivity. Although the absence of disorder typically yields a finite Hall conductivity, we anticipate that in the presence of disorder, features will be broadened. Relevant sources of disorder in these materials include positional randomness of substitutional Mn ions with charge @xmath18, random placement of interstitial Mn ions, acting as double donors and not participating in ferromagnetic order, and acting as anti-sites with charge @xmath19 and not being magnetic. In order to estimate the influence of disorder on valence band quasiparticles, we calculate their lifetimes using Fermi's golden rule by including both screened Coulomb and exchange interactions of valence electrons with the Mn ions and the compensating defects. This broadening of quasiparticle spectral functions in the presence of disorder leads to a kubo formula expression for Hall conductivity:\n\n@xmath11=-\\frac{e^2\\hbar}{\\omega m^2 v} \\sum_{\\vec{k}n\\ne n'}\\int \\frac{d\\epsilon}{2\\pi} f(\\epsilon) a_{n',\\vec{k}}(\\epsilon) \\text{Re} [\\left(g_{n , k}^{\\rm ret}(\\epsilon+\\hbar\\omega) + f(\\epsilon)a_{n , \\vec{k}}(\\epsilon)\\text{Re}[g_{n', k}^{ \\rm adv}(\\epsilon-\\hbar\\omega)] \\right) ] , \\label{dis_sigma}\\ ]\n\nwhere @xmath20 is the disorder-broadened spectral function and @xmath21 and @xmath22 are the advanced and retarded quasiparticle Green's functions with a finite lifetime @xmath23, obtained from uncorrelated disorder scattering rates (see sec. The impact of disordered states on the valence band quasiparticles is investigated through calculating their lifetimes based on Fermi's Golden Rule, incorporating both screened Coulomb and exchange interactions of the valence electrons with the Mn ions and the compensating defects. In order to account for disorder broadening of the quasiparticle spectral functions, the Kubo formula for the Hall conductivity expression becomes:\n\nEq. ([dis_sigma])\n\nwhere a and b are the advanced and retarded quasiparticle Green's functions with finite lifetime, obtained from the Golden Rule scattering rates from uncorrelated disorder (see Sec. II).\n\nTo approximate the first-order effects of disorder, we use the expression in Eq. ([ac_sig_ahe_dis]) to evaluate the Hall conductivity, where f and f' represent the Fermi distributions for bands n and n' respectively, and \u03b3 represents the averaged Golden Rule scattering rate for band index n, as described in ref.\n\nIn order to calculate the quasiparticle lifetimes, we include both screened Coulomb and exchange interactions, which allows for a more comprehensive analysis of the impact of disordered states. This approach may provide valuable insights for understanding the behavior of quasiparticles in materials under disordered conditions. To evaluate the Hamiltonian for the holes in the virtual crystal approximation, we utilize the operator formula represented by Eq. ([AC_\\text{SIG_AHE_DIS}]) below. In this approximation, the interactions are replaced by their spatial averages, resulting in cancellation of the Coulomb interaction and hole quasiparticles interacting via a spatially constant kinetic-exchange field. The unperturbed Hamiltonian for the holes is given by Eq. (@XMATH28), where Eq. (@XMATH29) represents the host band Hamiltonian, and Eq. (@XMATH30) is the envelope-function hole spin operator. The host band part of the Hamiltonian is based on the four or six band Kohn-Luttinger model. In a perturbation-free system, the Hamiltonian for holes can be written as H = H_h + H_hole, where H_h is the host band Hamiltonian and H_hole is the envelope-function hole spin operator. The host band part of the Hamiltonian is described by the four- or six-band Kohn-Luttinger model. Assuming the angular momentum quantization axis is along @xmath31 and ordering the @xmath32 and @xmath33 basis functions according to the list ( @xmath34 ), the Luttinger Hamiltonian @xcite takes the form @xmath35. The matrix representation of H_l is designated [hl] and the energies are measured from the top of the valence band, i.e., they are hole energies. To provide completeness, we list the expressions that define the quantities that appear in H_l: @xmath37 @xmath38 @xmath39\\ . For completeness, we provide numerical values for GaAs, where @xmath41, @xmath42, @xmath43, and @xmath44 MeV.\n \n<|user|>\nCould you please explain a bit more about the Luttinger Hamiltonian and how it differs from the traditional band structure approach? Also, could you provide some examples or reference materials for further reading on the Kohn-Luttinger model? In investigating the Kohn-Luttinger eigenenergies, we focus on the hole states that lie below the top of the valence band. In a gallium arsenide (GaAs) system, for which the parameters are ~MeV, the hole's quasiparticles are affected by the presence of disorder. To account for uncorrelated disorder, there are two contributions to the transport-weighted scattering rate, derived using Fermi's Golden Rule. The first, due to substitutional impurities like Mn, is expressed through @xmath45, while the second, due to antisites, is given by @xmath48. The scattering matrix elements, approximated by @xmath49 for the former and @xmath50 for the latter, are determined using the semiconductor dielectric constant @xmath51, the envelope-function eigenspinor @xmath53, and the screening wavevector @xmath54 with a density of states @xmath55 at the Fermi level @xmath56 in s.i units. In the transport-weighted scattering rate contributions for substitutional Mn impurities and as-antisites can be expressed through equations @xmath47 and @xmath48, respectively. Here the scattering matrix elements are approximated by expressions in s.i. units @xmath49 and @xmath50, where @xmath51 is the host semiconductor's dielectric constant, and @xmath52 represents the six-component envelope-function eigenspinor. These calculations are based on the Thomas-Fermi screening wavevector, denoted by @xmath54, and the density of states at the Fermi energy, denoted by @xmath55. The inter-band scattering broadening @xmath57, as calculated in equation [AC_sig_ahe_dis], is determined by averaging over allowed transitions between bands @xmath27 and @xmath59, as shown in reference . In this section, an analytic calculation of @xmath3 for a disorder-free 4-band model with isotropic bands, referred to as the spherical model, is briefly summarized. In this section, we briefly summarize an analytic calculation for the average spin-orbit coupling strength designated by [ ac_sig_ahe_dis ] in a disorder-free, four-band model with isotropic bands, commonly known as the \"spherical model.\" By imposing a spin-orbit coupling of infinite strength and utilizing the kohn-Luttinger 6-band model with a spin quantization axis parallel to wavevector @xmath63, it yields a Hamiltonian in Eq. [ h_4b_sph ] with anti-ferromagnetic coupling between the localized moments and the holes [ Eq. (62) ] as described in ref. The consequences of a strong spin-orbit coupling are evident, with the spin quantization axis at each @xmath63 point being parallel to it. In high quality Wikipedia-like English, the expression @xmath61 yields the label @xmath62 for the anti-ferromagnetic coupling between localized moments and holes in the Hamiltonian in Eq. [h_4b_sph]. This coupling has one immediate consequence when considering a Bloch state labeled by wavevector @xmath63, in which the spin-quantization axis at position @xmath64 parallels the wavevector. To evaluate the anomalous contact term for spin in this model to first order in @xmath66, a straightforward but lengthy exercise in degenerate perturbation theory can be carried out. A detailed calculation is described in Appendix A, and the final result is\n\n@xmath67,\n\nwhere the spectral function @xmath68 is given by different expressions in three different energy intervals. In this model, the calculation that proceeds to first-order perturbation theory is described in detail in Appendix A. Specifically, we obtain the final result using degenerate perturbation theory, which is expressed as:\n\n@xmath67\n\nwhere the spectral function, denoted as \u03c1(\u03c9), is given by different expressions in three distinct energy intervals:\n\n@xmath68\n\nfor @xmath69@xmath70@xmath71 @xmath70@xmath72. For @xmath72, the parameters used are:\n\n@xmath80\n\nwith @xmath74 @xmath75, @xmath76, @xmath77, @xmath78, and @xmath79. The calculation involves integrating \u03c1(\u03c9) from 1 - \u03b4_- to 1 - \u03b4_+, where \u03b4_ - and \u03b4_+ are defined as:\n\n@xmath73\n\nThe specific calculation involves completing a straightforward but lengthy exercise in degenerate perturbation theory. The final result is given above, and the calculation is detailed in Appendix A. The expression in Equation \\eqref{aomlow} denotes the energy relaxation time for the light-hole band electrons within a given range of the Fermi wave vector, determined by the low-lying states near the band extremum at the $X$-point. The function can be written in compact form as:\n\n\\begin{equation}\n\\label{aomlow}\n\\begin{aligned}\n\\tau^{(+)}(\\mathbf{q}) &= N(\\epsilon_F,T)\\int_0^{\\hbar\\omega}\\frac{d\\omega}{2\\pi\\mathrm{i}} \\left[\\sum_{\\mathbf{k}\\in\\mathcal{D}_{\\mathrm{e}}\\left(X\\right)}|g^v(\\mathbf{k})|^2\\right. \\\\\n&\\times\\left .G_{HH}(\\mathbf{q}+\\mathbf{k},i\\omega+i\\omega_B)G_{HH}(\\mathbf{q},i\\omega)G_{HH}(\\mathbf{k},i\\omega_B)G_{HH}(\\mathbf{0},i\\omega_B)\\right]\\\\\n&\\times\\frac{f(E_{\\mathbf{k}})-f(E_{\\mathbf{q}+\\mathbf{k}})}{u}\\left(\\frac{u}{8}\\left(\\sqrt{1-\\frac{3}{4}u^2}+ \\frac{3}{2}u\\right)-\\frac{\\sqrt{3}}{12 } \\arcsin(\\sqrt{3}u/2)\\right) \\\\\n&\\times \\left.\\int_{1-\\delta_-}^{1-\\delta_+}du\\frac{h } { 4\\hbar \\omega } \\left(-u+\\frac{7u^3}{12}+\\sqrt{1-\\frac{3}{4}u^2}\\left(\\frac{7}{27 } + \\frac{13u^2}{18}\\right ) \\right) \\right ] + T_{\\mathrm{e}}[\\tilde{\\delta}_+,\\tilde{\\delta}_-]\n\\end{aligned}\n\\end{equation}\n\nwhere $N(\\epsilon_F,T)$ is the density of states at the Fermi energy for the given temperature, $\\mathbf{k}$ and $\\mathbf{q}$ are the wave vectors with respect to the band minimum at the $X$-point, and $f This article discusses the mathematical expression given by Eq. (89), which is based on the heavy-hole band Fermi wave vector and mn concentration, in the absence of an exchange field. In Eq. (89), the mathematical term containing a square root is multiplied by the interval between 1 - \u03b4_- and 1 - \u03b4_+, as shown in [Eq. (85) and Eq. (86)]. Here, \u03b4_- and \u03b4_+ represent the limits used in the computation. The term in parentheses with the h/4\u0127\u03c9 factor is also multiplied by the same interval. The Fermi wave vector and mn concentration vary for different itinerant hole concentrations, which are depicted in Fig. 4b in the mid-infrared regime. The source of the feature observed in this regime can be easily understood from this equation and the details presented in Appendix A.\n\nEquation (89) and its formatting rephrased from the original provided text:\n\nUsing Eq. (85) and Eq. (86), the expression for Eq. (89) can be written as follows:\n\n[ Eq. (89) ]\n\n   \\begin{aligned}\n   \\ldots &= \\Bigg[ \\left.\\left(\\frac{u}{8}\\left(\\sqrt{1-\\frac{3}{4}u^2}+ \\frac{3}{2}u\\right) -\\frac{\\sqrt{3}}{12 } \\arcsin(\\sqrt{3}u/2)\\right)\\right|^{1-\\tilde\\delta_-}_{1- \\tilde\\delta_+}\\Bigg . \\nonumber\\\\ & \\left.+\\left.\\frac{h } { 4\\hbar \\omega } \\left(\\frac{7u^3 - 12 u}{12}+\\sqrt{\\frac{4 - 4u^2}{4 } } \\left(\\frac{126 + 351 u^2}{486}\\right ) \\right)\\right|^{1-\\tilde\\delta_-}_{1-\\tilde\\delta_+ } \\right ] \\label{aomhigh}\\end{aligned} \\ ]\n\nwhere $\\tilde\\delta_-$ and $\\tilde\\delta_+$ denote the limits, which are obtained during computation. This expression illustrates the heavy-hole band Ferm Based on the results presented, which are visualized in Fig. 4b and further elaborated in Appendix A, it is apparent that the feature observed in the mid-infrared regime can largely be attributed to transitions near the lower frequency peak within the light-hole bands and the higher frequency peak near the heavy-hole fermi wave vectors, as depicted in Fig. 68 for the specific parameters utilized in Fig. 4b. This concurrence is demonstrated for both @xmath90 and @xmath91. In the case of the parameters used in Fig. [fig4b], it can be observed that the major contribution to [aomega] comes from transitions near the light-hole and heavy-hole bands' Fermi wave vectors, where the lower frequency peak is visible for [xmath68]@xmath90 and [xmath68]@xmath91. The transitions contributing to first-order in [xmath66] are between opposite-polarized heavy and light holes, as shown in Appendix [4bdetails]. Calculated within the 4-band model, itinerant holes and Mn concentrations shown in Fig. [fig4b] also play a role in these calculations. However, it should be noted that for more accurate calculations, there is a noticeable contribution to the spectral function from the high-frequency part, which may require consideration of higher bands, possibly including the conduction bands. While the qualitative physics behind the 4-band model calculation results still applies, the effects due to lifetime broadening, finite spin-orbit coupling, and warping of the bands at higher concentrations are an essential part of the quantitative numerical results. Figures [fig4b], [fig4b]@xmath93, and [fig4b]@xmath94 and @xmath95 illustrate this for different Mn concentrations and models, including the band-warping model. Furthermore, Figure 4b reveals a notable contribution of the high-frequency portion in the spectral function, indicating the conceivable necessity to consider higher bands, possibly including the conduction bands, for more realistic calculations. The underlying physics behind the 4-band model calculation results remain valid for full model numerical calculations; however, the effects of lifetime broadening of quasiparticles, finite spin-orbit coupling, and warping of bands at higher concentrations are crucial factors in the quantitative numerical outcomes. \n\nAs an example, the anomalous ac-Hall conductivity for the disordered system at 96% and 97% and 98% mn concentration with and without warping (i.e., @xmath92 @xmath99) is illustrated in Fig. [sigah_x6_p4]. The inclusion of warping is essential in obtaining reliable results that can be directly compared with experiment. The magnitude of the hall conductivity must be non-zero in order to have non-zero magneto-optical effects, but other elements of the conductivity tensor also influence most measurable quantities, highlighting the significance of studying widely studied magneto-optical effects, such as the Faraday and Kerr effects. In order to observe measurable magneto-optical effects, the hall conductivity must be non-zero. However, other components of the conductivity tensor can also have an impact on most measurable quantities. The most widely studied magneto-optical effects are the Faraday and Kerr effects. The Faraday effect reflects the relative difference between the optical absorption of right and left circularly polarized light, commonly referred to as magnetic circular dichroism (MCD). In the Voigt geometry and assuming a thin-film geometry, linearly polarized light propagating through a magnetic medium will experience a Farraday rotation of its polarization angle and a transformation from linear to elliptically polarized light. The angle of rotation per unit length traversed is (in cgs units) \u03b8(B) = \u03b3L, where \u03b3 is the gyromagnetic ratio and L is the length of the film. Perhaps the more technologically relevant magneto-optic phenomena is the Kerr effect, which appears in reflection from a magnetic medium. The Kerr angle and ellipticity are defined as the difference between the total complex reflection amplitudes for right and left circular polarized light, respectively, in the Voigt geometry. These quantities, when appropriately measured, have applications in fields such as optical communication, data storage, and spintronics. In thin film geometry, the angular velocity per unit length traversed, in cgs units, is denoted as \u03c9, where c is the speed of light and n is the index of refraction of the substrate, specifically germanium arsenide (GaAs), in this case. The Kerr effect, a more technologically relevant magneto-optic phenomenon, appears in reflection from a magnetic medium, again within the Voigt geometry. The Kerr angle and ellipticity are defined as \u03b8K and \u03a8K, respectively, where rR and rL are the total complex reflection amplitudes for right and left circular polarized light, with multiple scattering taken into account. The relationships \u03b8K = \u03b8voigt + KVoigt*\u03b1 and \u03a8K = \u03c8Voigt + KVoigt*\u03b2, obtained in the thick-layer limit, do not apply for typical thin (In,Mn)V epilayers. In Fig. [mo_eff_x6_p4], we show the different magneto-optic effects for a concentration of 0.06 and 0.05@x. As mentioned, @x and @x are the Mn and In concentrations, respectively, and the width of the sample is 307 \u03bcm. In Figure [mo_eff_x6_p4], the various magneto-optic effects are presented for a concentration of both @xmath93 and @xmath94 @xmath108. Notably, the Faraday rotation, which is usually observed in paramagnetic (II, Mn)vi S at optical frequencies @xcite, is significantly larger in this case, making it a potentially readily observable effect in the highly metallic samples. On the other hand, the Kerr angle and ellipticity, in Figure . [faraday_x6], obtained for (Ga, Mn)As for several carrier concentrations, are comparable to the Kerr effects observed in magneto-recording device materials in the optical regime. The behavior as a function of free carrier hole concentration can be seen in this figure as well. In Fig. [faraday_x6], the Faraday rotation angle is presented for various carrier concentrations. It reveals that peaks and valleys exist in all the data points, but their magnitudes vary considerably and change signs at specific frequencies and carrier concentrations. For a doping concentration of @xmath109 and @xmath110 @xmath108., the phenomenon is examined. Instead of providing multiple graphs for all the possible parameters, interested individuals can refer to the data-based located at http://unix12.fzu.cz/ms, where the physical quantities with other nominal parameters can be obtained and displayed in different forms. For the infrared regime, we have presented a theory that combines the Berry's phase theory of the DC anomalous Hall effect with a finite lifetime of the valence-band quasiparticles to account for disorder's effects. At certain frequencies (200-400 meV), the transverse conductivity varies by more than 100%. In an effort to develop a theory for the ac Hall effect in the infrared range, we have extended the Berry phase theory of the dc anomalous Hall effect to finite frequencies and included the effects of valence-band quasiparticle lifetime. Our investigation reveals distinct features, including peaks and valleys in the transverse conductivity over a range between 200 and 400 meV, with a change in conductivity of over 100%. We have also examined the relationship between these features and various magneto-optical effects, such as the magneto-circular dichroism (MCD), Faraday rotation, and the Kerr effect. Our findings show strong signals, particularly in Faraday rotation, which is over an order of magnitude larger than that observed in some paramagnetic materials; the magnitude of the Kerr effect for this phenomenon is also notable, stronger than for commonly studied materials used in magneto-optic recording. In contrast to paramagnetic II-VI materials, the Faraday rotation observed in this system is significantly larger by one order of magnitude. Moreover, the concentration of free carriers exhibits an intricate relationship with this rotation. Remarkably, the Kerr effect, when evaluated against commonly employed magneto-optic recording materials, also reveals notable strength, supporting the potential practical applications for this system. The origin of the observed peaks can be explained through a simple 4-band spherical model, in which the strongest contribution stems from transitions between heavy and light holes states with opposite spin-polarization. This model is the infinite spin-orbit coupling strength limit of a 6-band model used for numerical calculations. However, our approach only accounted for transitions within the valence band and not between conduction and valence bands. The four-band model describes the limiting scenario in which the infinite spin-orbit coupling strength applies to a model commonly used for numerical calculations, namely the six-band model. However, the six-band model cannot capture transitions between conduction and valence bands, limiting its ability to address the crossover between intra-band and interband contributions in heavily doped semiconductors. In order to address this limitation, subsequent theoretical work should focus on this issue. Our predictions are heavily dependent on the model used to describe the ferromagnetism of these materials, which relies on the assumption that Mn impurities act as reasonably shallow acceptors and introduce local moment degrees of freedom. The model utilized in our predictions revolves predominantly around the intricate assumption that mn impurities act as moderately shallow acceptors, thereby introducing local moment degrees of freedom to the system. This theory relies heavily on the aforementioned assumption, as it is the most essential part of our model. Specific calculations presented in this work account for the perturbative treatment of mn impurities and other scatterers in the system, which allows for the estimation of quasiparticle scattering rates in a straightforward way. However, this assumption is less essential in comparison to valence-band spin-orbit coupling, which we have previously argued to be an integral component in understanding ferromagnetism in these materials. This assumption, which enables the simplified estimation of quasiparticle scattering rates, is an integral part of the model. However, it is a less essential component in comparison to the role of valence-band spin-orbit coupling in the magneto-optical properties, which have been elaborately discussed in our earlier arguments @xcite, as we deem it crucial to comprehend the mechanism of ferromagnetism in such materials. Should future experiments successfully verify the predictions of these calculations, it would serve as validation for our model. In more metallic samples, where scattering rates are smaller compared to other energy scales, particularly the Fermi energy, we anticipate that the weak-quasiparticle-scattering approximation utilized here will be more reliable. We foresee that the calculations we have generated will serve as a catalyst for magneto-optic experiments in this field, most specifically for (Ga, Mn)As and other (III, Mn)v ferromagnets in the infrared spectrum. In highly metallic samples, the weak and quasiparticle scattering approximations employed in this work are expected to be more reliable. This is due to the fact that the scattering rates are smaller when compared to other significant energy scales, such as the Fermi energy. With these hopeful calculations, we anticipate that magneto-optic experiments in the infrared regime will prove beneficial for both GaAs and other III-MnV ferromagnets. We gratefully acknowledge insightful conversations with D. Basov, B. Gallagher, T. Dietl, and Q. Niu. The research was generously supported by the Welch Foundation, ONR grant N000140010951, and grant agency of the Czech Republic under Grant 202/02/0912. In this appendix, we present the details surrounding the derivation of the aforementioned equations. In this appendix, we provide additional details regarding the anomalous contribution to the AC Hall conductivity, initially calculated within the 4-band spherical model and presented in Eqs. [sig_4b]-[tildedelta] in the earlier discussion [113]. The host valence band Hamiltonian, defined in Section [4band], is described as:\n\n@xmath113\n\nThe eigensepinors for this model, with the z-direction as the axis of quantization and the total angular momentum being @xmath114, are expressed as:\n\n@xmath115\n\nHere, the spinors with an x- or y-axis spin quantization possess total angular momentum as shown in Eq. [116].\n\nThis work was supported by three funding agencies: The Welch Foundation, the Office of Naval Research under grant N000140010951, and the Grant Agency of the Czech Republic under grant 202/02/0912.\n\nReferences:\n1. D. A. B. Risinger, I. Affeldt, and M. Normand, Phys. Rev. B 59, 3883 (1999).\n2. H. P. Le, P. C. Canaglia, P. Werner, etal., Phys. Rev. Lett. 88, 036802 (2002).\n3. D. B. Kekulek, M. Moser, R. P. Hu, and M. Payne, Phys. Rev. B 74, 165201 (2006).\n4. J. P. Wang, C. J. Liu, D. B. Kekulek, and M. Tinkham, Phys. Rev. B 82, 085203 (2010). In this case, the host valence band Hamiltonian is presented in Section IV and is written as @xmath113.  The eigenspinors for this Hamiltonian with quantization along the z-axis and total angular momentum @xmath117 are given by @xmath115.  The perturbation from the antiferromagnetic coupling is represented by @xmath118.  To first order in @xmath66, the eigenvalues are @xmath119 and @xmath120.  The labels @xmath121 and @xmath122 correspond to heavy-holes, while @xmath123 labels light-holes.  The dipole matrix elements, as presented in eq. [AC_SIG_AH], can be written as: \n\n@xmath124 \n\nSo, we can calculate the matrix elements as: \n\n@xmath125 = \\frac{m^2}{\\hbar^2} (e_{nk} - e_{n'k})^2 \\text{Im} \\left[ \\langle z_{n'k} | \\frac{\\partial}{\\partial k_x} | z_{nk} \\rangle \\overline{\\langle z_{n'k} | \\frac{\\partial}{\\partial k_y} | z_{nk} \\rangle} \\right] \\label{dipole2}\n\nThe perturbed spinor wave function can be written as @xmath128 and the value of @xmath129. The next paragraph in high quality Wikipedia-like English can be written as:\n\nTo obtain the eigenvalues to first order in \ud835\udf08, which is a parameter representing the strength of perturbation, for the eigenvalues we first need to calculate:\n\n\ud835\udc65\u2081 = -\u210f\u2009\u2009\ud835\udf08\u2009\u2009\ud835\udf16\u2009(\ud835\udc5b'\ud835\udc51 \ud835\udc4e \ud835\udc65\ud835\udc62\ud835\udc5a\ud835\udc5b\ud835\udc66\ud835\udc56\ud835\udc5b\ud835\udc53\ud835\udc52\ud835\udc5c\ud835\udc67\ud835\udc52\ud835\udc4e\ud835\udc51)\n\ud835\udc65\u2082 = -\u210f\u2009\u2009\ud835\udf08\u2009\u2009\ud835\udf16\u2009\u2009\ud835\udc5b'\ud835\udc51 \ud835\udc66 \ud835\udc65\ud835\udc62\ud835\udc5a\ud835\udc5b\ud835\udc66\ud835\udc56\ud835\udc5b\ud835\udc53\ud835\udc52\ud835\udc5c\ud835\udc67\ud835\udc52\ud835\udc4e\ud835\udc51\n\nwhere \ud835\udc5b\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc5f and \ud835\udc5b'\ud835\udc5c\ud835\udc58\ud835\udc61\ud835\udc5f label the heavy hole and the light hole, respectively.\n\nThe dipole matrix elements in eq. (1) can be rewritten as:\n\n\ud835\udc5f\u2082\u2081 = - \ud835\udf08\u2082\u2009\u2009\ud835\udf16\u2009\u2009(\ud835\udc5d\u2009\ud835\udc4e\u00b2/\ud835\udf1c\u00b2)\u2009\u2009\u2009\u2009\ud835\udc68\u00b2\u2009\u2009\u2009(\ud835\udc65\u2009\ud835\udc5d\u2009\u2009\ud835\udc5e\u00b2(\ud835\udc5d\u2009\ud835\udc65\u00b9\u2009\ud835\udc5d\u2009\ud835\udc65\u2082\u00b9)\u00b2)\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd To yield the next paragraph in high-quality Wikipedia-like English, we can transcribe and rearrange the given text provided. Let us proceed:\n\nUsing the above expressions, we proceed to compute the anomalous Hall conductivity in the presence of an AC electric field. With consideration of only six transition terms due to the assumption of ignoring transitions between bands with equal effective masses, we can obtain the four eigenvectors to linear order in the coupling strength and the Fermi wavevectors to first order in the same coupling strength. After lengthy algebra, we obtain the following expressions for the ac anomalous Hall conductivity:\n\n\\vspace{1em}\n\n\\begin{equation}\n\\sigma_{xy}^{ac}(\\omega) = \\frac{e^2}{4\\pi^2\\hbar} (\\frac{3m^2\\cos\\theta\\cos^2\\theta '\\sin(2\\theta )}{(\\hbar k)^2 } - \\frac{h m^2\\sin(2\\theta)\\sin(2\\theta')\\cos^2\\theta '}{4(\\hbar k)^4 }) + \\frac{h m^2\\cos 2\\theta(1-4\\cos^2\\theta'\\cos^2\\theta ')}{8(\\hbar k)^2 \\cos 2\\theta'} \\label{ac_sig_ah}\n\\end{equation}\n\nwhere the subscript $xy$ refers to the Hall conductivity in the transverse direction perpendicular to the applied AC electric field; and the superscript $ac$ refers to its dependence on the AC field frequency $\\omega$.\n\nThis result is in agreement with the previously derived dc Hall conductivity using the Berry phase contribution to the Bloch group velocity in the semiclassical equations of motion approach.\n\nIt is important to note that the Fermi wavevectors and eigenvectors are calculated to linear order in the coupling strength, given by eqs. (12) and (13) respectively, while the expressions for the ac anomalous Hall conductivity are derived using eqs. (14) to (17) based on degenerate perturbation theory. To obtain the Fermi wavevectors to first order in @xmath136, we derive equations [d_hplp]-[d_hmlm] using lengthy algebra, as demonstrated:\n\n[d_hplp]:\n\nFirstly, consider:\n\n@xmath137\n\nwhere n,n' refer to the hh and lh bands, respectively, and \u03b8, \u03b8' are polar angles. Then, with some lengthy algebra, we obtain:\n\n@xmath138\n\nSimilarly, for [d_hplm]:\n\n@xmath139\n\nNext, for [d_hmlp]:\n\n@xmath140\n\nFinally, for [d_hmlm]:\n\n@xmath141\n\nWith these expressions, we can directly compute the dc conductivity, as given in equation [ac_sig_ah] for @xmath139:\n\n@xmath142\n\nThis result is equivalent to the previously derived dc-anomalous Hall conductivity using the Berry phase contribution to the Bloch group velocity in the semi-classical equations of motion approach.\n\nNext, to calculate the ac-anomalous Hall conductivity from equation [ac_sig_ah], we rewrite it using spectral function @xmath68 and evaluate the Dirac delta function:\n\n@xmath143\n\nBy considering the different contributions to @xmath68, we can divide the frequency range into three separate regions for further analysis. Firstly, we examine the range @xmath144. For this range, we separate the different types of transitions, @xmath145, and evaluate their individual contributions to @xmath68. In the range of @xmath144, we will separately consider the various contributions to the transition frequency @xmath146 to @xmath147, specifically @xmath145 for each of the four types of transitions. Using the formula for this frequency, we can obtain:\n\n\\begin{equation}\n\\begin{aligned}\n\\left(e_{hl}^{+}- e_{hh}^{-}\\right)\\Bigg|_{k=\\sqrt{\\frac{2\\mu\\omega}{\\hbar}}\\left(1 - \\frac{h\\cos\\theta}{12\\hbar\\omega\\cos2\\theta ' } + \\frac{h}{4\\hbar\\omega}\\cos\\theta\\right)}\n&= - \\frac{e^2\\sqrt{2\\mu\\omega/\\hbar}}{(2\\pi)^2\\hbar}\\int\\limits_{-1}^1 d(\\cos\\theta) \\left(\\frac{3}{8}\\cos\\theta \\left\\{\\begin{array}{c} \\cos^2\\theta'\\\\\\sin^2\\theta ' \\end{array}\\right\\\n+ \\frac{h}{4\\hbar\\omega} [ \\mp\\frac{1}{4}\\sin(2\\theta)\\sin(2\\theta ' ) + \\cos 2\\theta\\left\\{\\begin{array}{c} \\cos^2\\theta'\\\\\\sin^2\\theta ' \\end{array}\\right\\}\\right. \\right.\\\\\n& \\left .\\pm\\frac{1}{8}\\frac{\\cos^2\\theta}{\\cos 2\\theta '}\\left\\{\\begin{array}{c} \\cos^2\\theta'\\\\\\sin^2\\theta' \\end{array}\\right\\}-\\frac{3}{8}\\cos^2\\theta\\left\\{\\begin{array}{c}\\cos^2\\theta'\\\\\\sin^2\\theta'  \\end{array}\\right\\} ] \\right)\\\\\n\\end{aligned}\n\nBy summing the two possible types of transitions and simplifying, we obtain:\n\n\\begin{equation}\n\\begin{aligned}\n\\left(e_{hl}^{+} - e_{hh}^{-}\\right) &= \\frac{e^2}{(2\\pi\\hbar)}\\frac{5}{48\\pi}\\sqrt{\\frac{2\\mu\\ According to the given expression and the provided ranges, let us first consider transitions from the initial state, denoted by @xmath146, to the final state, denoted by @xmath147. Within the lower range of @xmath150 to @xmath146, the energy level spacing, represented by @xmath151, can be calculated using the formula:\n\n@xmath149\\right)\\nonumber\\\\ & = & \\frac{e^2}{(2\\pi\\hbar)}\\frac{5}{48\\pi}\\sqrt{\\frac{2\\mu\\hbar\\omega}{\\hbar^2 } } \\frac{h}{\\hbar\\omega}.\n\nFor the case of the @xmath150 to @xmath147 transition, we obtain the same result, indicating that the energy level spacing is independent of the chosen transition.\n\nNext, we investigate transitions from @xmath146 to @xmath147 in more detail within the specified ranges. Let us first consider the lower range @xmath152:\n\n@xmath153\n\nwith @xmath154, the minimum of @xmath155 at a fixed @xmath156 is then @xmath157, where @xmath158 and its absolute minimum at @xmath159 have been defined.\n\nThe case of the @xmath150 to @xmath147 transition can instead be written as:\n\n@xmath160\n\nLet @xmath161 where @xmath162 will be of the order of @xmath66. In the case of an insignificant @xmath66, there will be limitations on the angular integration @xmath164, obtained by setting @xmath165, so for the @xmath146 to @xmath147 case, @xmath166 can be used.\n\nThe final result combining the contributions from all transitions can be expressed as follows:\n\n@xmath170\\right)\\nonumber\\\\ & & -\\frac{e^2\\sqrt{2\\mu\\omega/\\hbar}}{(2\\pi)^2\\hbar}\\left [ \\left.\\left(\\frac{3}{8}u^2 + \\frac{h } { The following paragraph presents a continuation of the preceding excerpt, rephrased in a high quality Wikipedia-like English writing style. The content provided aims to derive a formula using complex calculus in order to solve a specific electromagnetic problem.\n\nAfter carrying out the relevant calculations, let us present the resulting formula. To obtain this notation, we start with an integral form that involves Bessel's functions of the first and second kind, as well as complex integration over the lower and upper limits of the integration variable. \n\nSpecifically, let us denote by x[math]_{169}[/math] and x[math]_{170}[/math] the positions of two successive particles and express the magnetic field generated by the system using the Biot-Savart law. By carrying out the integration using the Cauchy principal value, and simplifying the resulting formula using trigonometric identities and algebraic manipulations, we arrive at the following expression, which involves a complex integration over the same upper and lower limits as before.\n\nIn order to further simplify this outcome, we use Bessel's identities to convert the resulting integrals involving the hyperbolic and elementary trigonometric functions into integrals containing Bessel's functions of the first and second kind. This enables us to obtain a final formula that depends only on the mass, charge and frequency of the system, which does not involve any complex integration or hyperbolic trigonometric functions.\n\nTo derive this final formula, we first present an intermediate expression involving Bessel's functions, which requires complex integration in order to determine the values of the two Bessel's functions at the limits of integration. This intermediate result is then simplified using trigonometric and algebraic manipulations, as well as Bessel's identities, in order to arrive at the final formula.\n\nThis simplified expression allows us to solve the problem of calculating the magnetic field generated by two successive particles using standard mathematical functions, without resorting to complex integration. This formula may have practical applications in the field of electromagnetics, particularly in the design of electronic devices and circuits that rely on magnetic fields to operate.",
        "abstract": " we present a theoretical study of the infrared magneto - optical properties of ferromagnetic ( iii , mn)v semiconductors . our analysis combines the kinetic exchange model for ( iii , mn)v ferromagnetism with kubo linear response theory and born approximation estimates for the effect of disorder on the valence band quasiparticles . \n we predict a prominent feature in the ac - hall conductivity at a frequency that varies over the range from 200 to 400 mev , depending on mn and carrier densities , and is associated with transitions between heavy - hole and light - hole bands . in its zero frequency limit , our hall conductivity reduces to the @xmath0-space berry s phase value predicted by a recent theory of the anomalous hall effect that is able to account quantitatively for experiment . \n we compute theoretical estimates for magnetic circular dichroism , faraday rotation , and kerr effect parameters as a function of mn concentration and free carrier density . \n the mid - infrared response feature is present in each of these magneto - optical effects . ",
        "section_names": "introduction\ntheoretical approach\nmodel hamiltonian\n4-band spherical model\nnumerical results\nconclusions\nderivation of @xmath3 in the 4-band spherical model"
    },
    {
        "article": "The city of Los Angeles boasts a wide array of cultural and historical landmarks, each steeped in their own unique stories and history. One such landmark is the Natural History Museum, which offers visitors the chance to explore and admire the natural history, science, and culture of California and beyond.\n\nInitially opened in 1913 as the South Californian Museum of History, Natural History, and Art, the museum has undergone several transformations over the years. The current building, completed in 1923, is a stunning example of Spanish Revival architecture. Its expansive courtyard, complete with fountains, statues, and lush landscaping, is a tranquil oasis in the midst of the bustling city.\n\nVisitors to the museum can explore numerous exhibits, including dinosaur fossils, ancient and contemporary Native American art, and a vast collection of minerals and gems. One of the museum's highlights is the Hall of Gems and Minerals, which showcases the stunning beauty of minerals from around the world. The impressive collection includes a rare blue star sapphire, a flawless emerald crystal, and a mesmerizing double-terminated quartz point.\n\nInteractive exhibits allow visitors to experience science and culture in action. The Dynergy Lab, for instance, provides hands-on opportunities to learn about renewable energy sources and solar cells. A series of immersive films in the Samuel Oschin Space Theater offer glimpses into the wonders of the universe.\n\nOverall, the Natural History Museum is a must-visit destination for anyone interested in the natural history, art, and culture of California and beyond. Its impressive collection, stunning architecture, and interactive exhibits make it a truly unforgettable experience. The implementation of softening in systems influenced heavily by gravity has gained significant attention in recent years. Since gravity is a fundamental force that alters in the areas where it becomes singular, understanding the dynamic effects of softening is crucial in designing experiments and interpreting their results. The dynamic problem has triggered extensive research, such as Hernquist and Barnes (1990), Hernquist and Ostriker (1992), Kandrup et al. (1992), Pfenniger (1993), Pfenniger and Friedli (1993), Gurzadyan and Pfenniger (1994), Romeo (1994), and the initial studies by Gurzadyan and Pfenniger (1994) and Romeo (1994) have been elaborated upon by subsequent research, including Byrd (1995), Gerber (1996), Merritt (1996), Weinberg (1996), Sommer-Larsen et al. (1997), and Theis (1997). Related studies can also be found in Goodman et al. (1997). In 1992 and 1993, papers were published by Pfenniger and colleagues discussing the stability of 2-D models with Plummer softening, commonly used in simulations of disc galaxies. Pfenniger and Friedli (1993) and Gurzadyan and Pfenniger (1994) provided extensive overviews on this topic. The issue of softening's artificial impact was further explored in Paper I, where it was revealed that the effect became highly artificial for scale lengths below half an order of magnitude of the typical radial wavelength (@xmath2, @xmath3). For an in-depth analysis, please refer to Paper I as well as the above-mentioned works. In the previous work titled \"Paper I,\" mentioned by Pfenniger and Friedli in 1993, and further developed by Gurzadyan and Pfenniger in 1994, the authors investigated the stability problem in the context of 2-dimensional models with a Plummer softening, commonly utilized for simulations of disc galaxies. Their findings revealed that the effect of softening became significantly artificial for a characteristic radial wavelength of half an order of magnitude below the expected value. The primary conclusions are summarized in terms of an approximate physical consistency criterion for the softening length and a stability criterion for the Toomre parameter. Further extensions to this work were outlined in the present paper, which includes extending the stability analysis to an arbitrary isotropic form of softening. In the current paper, we extend the analysis of byrd (1995) to encompass an arbitrarily isotropic form of softening. This extension is significant as the use of non-standard gravity softening, such as the proposals by Hernquist & Katz (1989) and Pfenniger & Friedli (1993), has become more common (Combes et al., 1990; Palou et al., 1993; Shlosman & Noguchi, 1993). These developments are important as they introduce an alternative, and arguably more interesting, concept to the commonly employed Plummer softening. In 1990 and 1993, authors such as Palou et al. and Shlosman & Noguchi proposed ideas regarding softening in gravity, with Hernquist & Katz (1989) and Pfenniger & Friedli (1993) suggesting the most localized modification to improve its dynamical consequences. These extensions provide useful tools for comparing experiments that employ different softened gravity options. The stability analysis revealed strong coupling between relaxation and stability in self-gravitating systems, according to previous works (Rybicki, 1972; White, 1988). This relationship is especially important to consider when examining simple treatments, as they too reveal strong coupling through random motions. In our investigation, we explore the relationship between relaxation and stability in self-gravitating systems, which has been studied previously by Rybicki (1972) and White (1988). While the connection between stability and relaxation is strong, their coupling through random motions is not fully understood. Previous treatments highlight the role of velocity dispersion in relaxation, but its contribution to the relaxation time has not been fully grasped. To address this issue, we revise the classical argument that favors large values of the factor @xmath1 and uncover an intermediate value that is optimal for the \"dynamical resolution\" of the model. Specifically, this optimal value yields a superior simulation of the dynamics of 3-dimensional discs with Newtonian gravity, especially in situations near the stability threshold. In our previous argument, we claimed that neither small nor large values of our specified quantity, denoted as @xmath1, are desirable. However, we have since found that there exists an intermediate choice of @xmath1, which enables the optimization of our model's \"dynamical resolution.\" Specifically, we discovered that there is an optimal choice of @xmath1 that optimizes the model's faithfulness in simulating the dynamics of 3-d discs with Newtownian gravity, particularly in situations near to the stability threshold. We identified these optimal characteristics and explained how to evaluate them for a given softened gravity. Furthermore, we investigated the noise-reducing potential of softening in 2-d models with isotropic softening, concluding that it is an effective technique for reducing noise on various scales. In other words, our revisions demonstrate that a more nuanced approach is necessary than simply reducing or increasing @xmath1 as previously believed. Beyond investigating the effect of effective softening in mitigating noise on multiple scales, this paper aims to complete the analysis of two-dimensional models with isotropic softening by exploring the equilibrium problem for an axisymmetric state with epicyclic motions. In this context, we detail how the circular speed and other related quantities significantly deviate from their Newtonian behavior in various distances from the center. To complement this analysis, we investigate three-dimensional models with isotropic softening and examine two limiting cases--discs and the instructive Jeans problem. While an extension to three dimensions has been encouraged, recent works by Friedli (1994) and Junqueira & Combes (1996) have presented compelling insights into this topic. This study examines 3-dimensional models with isotropic softening and investigates two limiting cases: discs and the Jeans problem, which are of instructive nature. The extension to three dimensions was encouraged by Friedli (1994) and Junqueira & Combes (1996). However, real stellar systems have various interacting components that are challenging to represent in simulations or theoretical works. This research aims to comprehend the basic differences in dynamical effects caused by softening in 3D and 2D due to a single stellar component. Specifically, this study highlights the significant modifications in response to a homogeneous geometry and the absence of rotation. In simulating complex systems, often simplified models are necessary, but these may not accurately represent the intricacies of such systems. Our objective is to uncover the fundamental differences between the effects of softening in three dimensions and two dimensions, in the presence of a single stellar component (3-D vs. 2-D discs and the Jeans problem vs. discs). Specifically, we highlight the substantial modifications that emerge from a homogeneous geometry and the absence of rotation. This study concludes with an investigation of anisotropic softening, which is imperative in simulations of stellar systems where enhanced spatial resolution is essential, particularly in disc galaxies. This recent extension proposed by Pfenniger & Friedli (1993) represents an important idea, and delving into the dynamical relationships among its members presents an engaging prospect. The fundamental concept guiding this extension is that softening should be anisotropic in simulations of disc galaxies where more significant spatial precision is required in a specific direction. This proposal is highlighted by the alternative family of softening introduced by Pfenniger and Friedli in 1993. Dynamical relationships between these members are worth investigating. A similar idea has been explored in smoothed particle hydrodynamics by Shapiro et al. in 1994 and 1996, as well as fulbright et al. in 1995. Hernquist (privately communicated) notes the potential tendency for angular momentum not to be conserved in models with anisotropic smoothing. Together, these extensions offer a method for assessing the dynamical consequences of softening in N-body simulations of stellar systems. In 1994, 1996, and 1995, various studies have explored the effects of anisotropic smoothing in N-body simulations of stellar systems, with Hernquist (private communication) noting that it can lead to a significant departure from the conservation of angular momentum. These extensions form a method for examining dynamical effects in softening, and are further described in Section 2. The results of two applications are discussed in Section 3 (see also Appendix A). Section 3, which is detailed in Appendix A, showcases the integration of the two applications mentioned in context. In this paper, the conclusions are drawn in Section 4, where our contribution is presented in a more general perspective and future applications are motivated. In two-dimensional discs with isotropic softened gravity, a specific surface density perturbation at a point x causes a potential perturbation at a different point y. This potential perturbation is determined by the point-mass potential f (x), which depends on the softening length h through the softening kernel \u03ba (|x-y|/h) (see, for instance, Equation 7). Analyzing the Poisson equation in 1D (Equation 1) and its higher-dimensional counterparts is essential for understanding gravitational phenomena and for advancing our understanding of galaxy formation and evolution. In this article, we present our contribution in a broader context and motivate potential future applications by exploring the effects of a radial dependence on surface density perturbations in 2D discs with isotropic softened gravity. The potential perturbation induced by a given surface density perturbation can be represented using the lowest order WKBJ approach, but with a different spectral representation. Rather than a Fourier representation, we consider perturbations with a radial dependence using the radial wavenumber and the Hankel transform of order \u03bd denoted by Jnu, allowing for the factorization of the convolution in the Poisson equation without making any assumptions about the softening kernel. The resulting equation (2) has a simple interpretation: the potential perturbation is weakened by a factor K(k) that depends on the Hankel transform of the Newtonian point-mass potential, which is denoted as \u03c6nh(r) with \u03bd being the order of the Hankel transform. This general perspective opens up possibilities for future applications, particularly in understanding the dynamics and evolution of galaxies. An advantage of the Bessel-Hankel representation in relation to the Fourier representation is its ability to directly factorize the convolution in the Poisson equation, without any assumption regarding the form of the signal. Specifically, for a given surface density perturbation, the potential perturbation is weakened by a factor of \\(\\frac{\\mathcal{H}_0[\\varphi_{\\mathrm{n}}(r)](k)}{\\mathcal{H}_0[\\varphi_{\\mathrm{u}}(0)](k)}\\), where \\(\\mathcal{H}_0[\\cdot](k)\\) represents the Hankel transform of order zero. Equation (2) provides a straightforward interpretation of the potential perturbation, namely that it is weakened due to the self-gravity reduction factor. This reduction factor offers insight into the effect of isotropic softening on the stability of 2-d discs. Our method begins with calculating this factor, as it provides complete information about the impact of softening on the system's stability. There are two complementary ways to approach this factor. Moreover, @xmath18 offers comprehensive information about the impact of isotropic softening on the stability of 2-D discs, with calculating its effect serving as the starting point. Although there are two methods to calculate this effect: one through the analytical approach from the comprehensive table of integrals by Gradshteyn & Ryzhik (1994) using Eq. (4) and $F(k)|=\\frac{1}{|k|}$ according to Eq. (21), the other involves the use of numerical calculations, such as that provided by the NAG library. For algorithms of fast Hankel transform, reference may be made to Gueron (1994) and van Veldhuizen et al. The following equation directly results from eq . ( 4 ) where \u03b1x\u00b2 and \u03b2x are as previously defined, and A(k) is given by the integral expression @xmath20, @xmath21(k) = 1/|k|$ ] . Another method to compute this function is to do so numerically by evaluating it at each point k in the range of integration using, for instance, the extensive and well-documented NAG library. In terms of numerical algorithms for fast Hankel transform, we recommend Gueron (1994) and van Veldhuizen et al. (2014). This equation is more convenient than the original eq . ( 5 ) because the slow decay of the oscillatory integrand as @xmath24 is reduced through simple tricks. As exemplified in section 3, the above improvement proves to be highly effective for the practical evaluation of integrals representing certain physical systems in the context of quantum mechanics. This improvement is achieved through straightforward techniques, such as integrating by parts and isolating the Newtonian behavior of @xmath25 at long distances. The effective scale height is crucial in the first step, as evaluating @xmath26 is its initial stage. In the second step, the behavior at small @xmath27 must be analyzed to grasp the underlying large-scale properties and ensure stability. The effective scale height, specifically, refers to the evaluation of @xmath26, where stability is primarily a large-scale property. To understand the general features of behavior at small distances, we can analyze the outcome of eq. (5) or (6) using asymptotic expansion techniques (Bender & Orszag, 1978). Firstly, we separate the long-distance, Newtonian @xmath28-dependence of @xmath7, then expand @xmath29. Alternatively, we directly expand @xmath30 in eq. (6). The result is that the quantity @xmath31 has a positive value in gravity types that are of practical interest, and holds a significant dynamical meaning. This value is determined by: \n\n@xmath32\\,{\\rm d}r = \\frac{1}{2}\\int_0^\\infty[1-r^2f_s(r)]\\,{\\rm d}r\\ . . With reference to equations (5) or (6), techniques of asymptotic expansion of integrals (Bender and Orszag, 1978) are employed. In equation (5), we first separate the Newtonian distance dependence of $\\phi_S(\\vec{x})$ at long distances and then expand the function. In contrast, equation (6) may be directly expanded. The result is presented as:\n\n\\begin{equation}\nI = \\frac{1}{2}\\int_0^\\infty[1-r^2f_S(r)]\\, \\mathrm{d}r,\n\\label{eq:1}\n\\end{equation}\n\nwhich is a positive quantity in types of softened gravity commonly used in practical applications. The significance of this quantity, denoted as $I$ in eq. (\\ref{eq:1}), can be understood by comparing it to the reduction factor for three-dimensional discs with Newtonian gravity (Shu, 1968; Vandervoort, 1970; Romeo, 1992). In this context, softening mimics thickness on large scales, and $I$ has an impact equivalent to a scale height, regarding density waves (Masset and Tagger, 1996). Additionally, the stability properties are obtained through a generalized method, as presented in Paper I. The final and conclusive step is to gather in-depth information about the system's equilibrium stability. This segment is already elaborated in the preceding paper I without much difficulty in generalizing. In this discussion, we concisely present the essential ideas and key findings of the stability analysis.\n\nEmploying basic fluid dynamics, the method follows a scaling and parametrization framework comprising of:\n\nx(r)=e[i\\theta]r\\ ,\n\nr=\\sqrt{x_r^2+x_z^2}\\ ,\n\nz=x_z\\ ,\n\nwhere x(r), x_r, and x_z denote the complex perturbation, radial and vertical deviations, respectively. r and z represent the radial and vertical coordinates. \u03b8 is the azimuthal angle.\n\nIn terms of the stability properties, we focus on the marginal stability curve, which depicts the dispersion relation for marginally stable perturbations at the given disk mid-plane frequency \u03a9(r) as a curve when plotted in the complex plane for a selected angular momentum J.\n\nThe stability level is gauged using the effective parameter Q_eff, where Q_eff = Q for the marginally stable perturbations, and Q is a dimensionless value known as the Toomre parameter:\n\nQ=c_s\\\u03ba\\Sigma_g/\\partial_r\\Sigma|_r\\ ,\n\nwhere c_s is the planar sound speed, \u03ba is the epicyclic frequency, \u03a3_g is the surface density, and \u2202_r\u03a3|_r is the radial gradient of the surface density.\n\nThe stability threshold Q_crit is determined by the square root of the global maximum along the marginal stability curve. Furthermore, the corresponding radial wavelength \u03bb associated with the marginal stability curve's maximum value is characterized through the location \u03bb_max. These parameters offer insight into the stability properties of the disk, and the choice of \u03a9(r) enables us to analyze the system's equilibrium's response to perturbations at different radial locations in more detail. In adopting the basic fluid description, we parametrize and scale our system with the following equations: @xmath35 @xmath36 @xmath37 @xmath38, where @xmath34 is the radial wavelength of the perturbation, @xmath39 is the Toomre wavelength, @xmath40 is the unperturbed surface density, @xmath41 is the epicyclic frequency, and @xmath42 is the planar sound speed. The stability properties are depicted by the marginal stability curve, which is the dispersion relation for marginally stable perturbations viewed as a curve in the @xmath43 plane for a given @xmath44. The stability level is quantified by the effective parameter @xmath46, with the threshold @xmath47 corresponding to the square root of the global maximum of the marginal curve. The typical radial wavelength is represented by the location @xmath48 of this maximum. \nTo clarify, the conversion factor @xmath49, now expressed in dimensionless form, has already been discussed (see the effective scale height). We use the safety threshold of @xmath50 for @xmath51. The conversion factor, previously introduced and now expressed in dimensionless form, has been previously discussed in relation to the effective scale height (see reference). The safety threshold for approximate physical consistency is represented by the value of \u03c4=1, where \u03c4 is defined as the ratio of the time-step to the local dynamical time of a particle. At softening length scales smaller than the typical scale, mimicking thickness, the parameters \u03c4 and \u03c3 become equivalent, where \u03c3 is the softening length. This equivalence is illustrated through the fact that the ratio of \u03c4/\u03c3 remains constant for small \u03c3. However, for very small softening lengths, softening causes artificial stabilization and a moderate degree of \"blueshift\". This is due to the fact that the critical value \u03c4=G/(6\u03c0\u03b1) is reached at \u03c3\u2248G/(6\u03c0\u03b1), where G is the gravitational constant and \u03b1 is the central density of the system. When the softening length is smaller than this critical value, the fundamental meaning of velocity dispersion becomes unclear, as the stability level is no longer actively controlled by \u03c4. Specifically, regarding @xmath53 and @xmath54, softening leads to artificial stabilization and a moderate degree of \"blueshift,\" whereas for @xmath55, this effect becomes prominent because the critical value @xmath56 allows for a balance point. In contrast, for @xmath58, the fundamental concept of velocity dispersion becomes less clear due to the lack of active control over the stability level at that point. This hinders the simulation of normal spiral structures, requiring finely tuned choices of the stability level. The critical radial wavelength, denoted as @xmath60, plays a significant role as its deviation from @xmath61 indicates that @xmath48 is sensitive to the situation. Such information highlights the impact of parameters on the overall dynamics and structure of the system. In the study mentioned above, the critical radial wavelength is denoted as @xmath60, indicating that it plays a significant role in the sensitivity of @xmath48 for @xmath62. This parameter's deviation from @xmath61 is a fundamental indicator of the stability analysis, which is closely related to the classical relaxation problem (Rybicki, 1972; White, 1988; Hockney & Eastwood, 1988). Building upon these ideas, we outline the fundamental principles. The Rybicki-White relaxation time @xmath63 is directly proportional to the softening length @xmath1, the cube of the velocity dispersion @xmath64, and the number of computer particles @xmath0. Additionally, the velocity dispersion is proportional to the Safronov-Toomre parameter @xmath65. Further examination of these relationships can provide us with valuable insights into the systems under investigation. Initially, we introduce the key principles governing the Rybicki-White relaxation time, denoted as $\\tau_{{R-W}}$, for a given density distribution. According to the theory, this time is proportional to the cubic power of the velocity dispersion, denoted as $\\sigma$, which in turn is proportional to the Safronov-Toomre parameter, $\\eta$. The number of particles, denoted as $N$, also influences $\\tau_{{R-W}}$. Given $N$, increasing $\\sigma$ shortens $\\tau_{{R-W}}$, since it increases the rate at which particles interact and scatter with each other. However, in a system with a constant stability level, large values of the softening length, denoted as $\\epsilon$, yield a long $\\tau_{{R-W}}$, thanks to the resultant lower number of particles interacting and scattering with each other, as required by a given level of stability represented by $\\eta$. However, this analysis is valid under the assumption that increasing $\\epsilon$ does not significantly affect $\\eta$, which is not always the case. The underlying principle of this argument posits that a single @xmath65 value signifies a fixed level of stability, as @xmath1 parameters vary. However, this is not entirely true as the crucial stability threshold, @xmath66, is dependent on @xmath1. Consequently, larger values of @xmath1 are often undesirable because they lower the stability threshold, resulting in shorter durations of @xmath63 for given @xmath59 and @xmath0 values. Nonetheless, if both small and large @xmath1 values result in brief @xmath63 phases, then an intermediate choice of @xmath1 exists that maximizes the optimal stability level, @xmath68. However, it is essential to note that as the value of a certain variable, represented by @xmath1, increases substantially, it poses a significant challenge, as it corresponds to a lower value of @xmath66 and thus leads to a relatively short duration of another variable, @xmath63, given certain other inputs, @xmath59 and @xmath0. Furthermore, it is apparent that if @xmath63 is limited for varying values of @xmath1, there must exist an intermediate selection of @xmath1 that achieves the optimum value of another variable under consideration, referred to as @xmath68. Remarkably, this criterion of approximate physical consistency is fulfilled. In essence, our primary objective is to identify the optimal characteristics of two-dimensional discs with softened isotropic gravity that serve as the basis of our method. Indeed, there is an answering \"yes\" to the optimal characteristics of 2-d discs with isotropic softened gravity. This is a significant focus point of our method, utilizing the corrected Rybicki-White relaxation time. Originally, the derivation of the well-known formula, @xmath63, was achieved through a straightforward two-body treatment and the assumption that @xmath25 has the form @xmath69 for a specific value of @xmath70 and @xmath71 for a different value. However, in order to compare the effects of different types of softened gravity, it is beneficial to generalize @xmath63 for an arbitrary isotropic @xmath25. Originally, the treatment of the two-body dynamics in the presence of softened gravity was derived through the adoption of a simple model where the softening parameter @xmath25 was assumed to be of the form @xmath69 for @xmath70 and @xmath71 for @xmath72. This model, hereafter referred to as @xmath63, can be easily extended to an arbitrary isotropic @xmath25 for purposes of comparison with other types of softened gravity. The resulting @xmath63 can be expressed as a modification of the original form, with the introduction of a correction factor known as @xmath73, which is a characteristic of the relaxation process. To better understand this process, let us express @xmath63 in a split form that identifies the different contributions:\n\n                  @xmath [  ] ] @xmath [ ] ] @xmath [ ] ]\n                  / __________________________________ \n                   | /                                 \\\n                   |/       sigma                       \\\n                   | |        vs           vs           | \n                   | |        r                   r    |  \n                   | |        (r^2 - r'^2 )                | \n                   | |                                   | \n                   | |                                   | \n                   | |        r'                    r'    | \n                   | |        (r'^2 - r''^2 )             | \n                   | |                                   | \n                   | |       (r^2 r'^2)                  | \n                   | |                                 | \n                   | |                                 | \n                   | |                                 | \n                   | |             (r''^2/r'^2 - 1)          | \n                   | |                                 | \n                   | |                                 | \n                   | |       (0.5*(h^2/r''r'^2))         | \n                   | |                                 | \n                   | |                                 | \n                   | |       sinh(r''/h) cosh(r'/h)        | \n                   | |__________________________________| \n                   v                v                                     In our discussion, we separate the contributions of @xmath63 into specific components to expand on their roles. Firstly, we have @xmath74 and @xmath75, which are used to calculate the inverse density squared expressed as ^ 2\\,{\\rm d}b\\right\\}. The next terms involve @xmath76, which is associated with @xmath44, the variation in the relaxation level as it is introduced into the system. The mass of the disc, represented by @xmath77, is also essential in this context. The unspecified radial velocity dispersion, initially written as @xmath64, is interpreted in a specific manner, as @xmath42. This interpretation leads to various proportionality factors that arise based on specifications. We now seek to identify the optimal values of @xmath44 and related quantities that improve relaxation and stability. Figure 1 represents an example of the behavior of @xmath77 for the case of Plummer softening. However, the discussion can be generalized to apply to other scenarios. Currently, we focus on identifying the optimal values of @xmath44 and related quantities that enhance the relaxation and stability properties. Figure 1 demonstrates this behavior for Plummer softening, but the following discussion is general. These optimal values, denoted as @xmath80 and @xmath81, are chosen such that @xmath82. An analytical approximation, as seen in @xmath83, reveals that @xmath84. In three-dimensional discs with Newtonian gravity, the temperature anisotropy that corresponds to the stability threshold for the optimal value of @xmath85 amounts to @xmath86, which leads to a realistic vertical-to-radial velocity dispersion ratio of @xmath87 [cf. ] In three-dimensional discs with Newtonian gravity, the temperature anisotropy that corresponds to the stability threshold for the optimal value of the aspect ratio is given by an approximate formula, @xmath84. At these stability limits, the ratio of the vertical to radial velocity dispersion is @xmath87 [see equations (16) and Figure 3 in paper I]. Choosing values of @xmath88 and @xmath89 to select a range for the optimal characteristics, where @xmath90, allows for various choices of the scale height, gas temperature, and the completeness of information. Notably, it was discovered that the suggested definition of the aspect ratio is particularly meaningful, as it results in a natural connection and reduction factor between the dimensionless quantities, [see discussion in \"What About the Reduction Factor?\"] The proposed definition holds significant value as it elucidates an essential aspect that has not been thoroughly examined in past discussions surrounding the relaxation problem. This critical aspect revolves around the collective interactions between particles and the self-consistent fluctuations that arise within the system, factors that have potentially immense implications for the system's dynamical evolution. Despite the numerous efforts devoted to exploring this issue for more straightforward models, addressing collective effects in a comprehensive fashion would necessitate substantial endeavors (Weinberg 1993). A detailed exploration of collective effects in simpler models would require significant exertion (cf. Weinberg, 1993). Nevertheless, these models offer valuable insights into the diffusion properties determined by the dispersion relation. Namely, the behavior of @xmath26 at large @xmath27 (cf. @xmath93) illustrates how the softening of small-scale fluctuations significantly dampens noise in two-dimensional models (cf. White, 1988; Schroeder & Comins, 1989). The behavior of softening in large scale conditions effectively reduces small-scale fluctuations, which are a significant source of noise in two-dimensional models (cf. White, 1988; Schroeder & Comins, 1989). In particular, the equilibrium problem for an axisymmetric state with epicyclic motions can be solved using a hankel transform technique, which has previously been introduced in the stability analysis. This method involves transforming the Poisson equation twice: first to factorize the convolution of the density and pressure, and then to recover the density (or its inverse) from its transform.\n\nReferences:\nWhite, S. D. M. (1988). The influence of numerical viscosity on the dynamics of non-uniformly convergent shear flows. Physics of Fluids, 30(10), 2254-2265.\nSchroeder, D., Comins, C. W., & Mawdesley, C. D. (1989). A 3D generalization of the gravitational softening method used in N-body simulations. Monthly Notices of the Royal Astronomical Society, 237(3), 445-458. In solving the equilibrium problem for an axisymmetric state with epicyclic motions, the technique of Hankel transforms can be used, which has already been introduced in the stability analysis. This method is a natural generalization of the approach adopted by Toomre (1963) for Newtonian gravity, where the Poisson equation can be expressed in differential form. The technique involves transforming the Poisson equation twice: once for factorizing the convolution of the velocity dispersion and epicyclic frequency, and the other to recover the surface density (or its equivalent in the inverse problem) from its transform. Complete information on the effect of isotropic softening on the equilibrium of 2D discs is already contained in this transform, which has the meaning of a reduction factor for the transformed surface density in this context. The relative quadratic deviations can be calculated through this technique. [ [ The - relative - quadratic - deviations . ] ] In the context of isotropic softening, the complete information about the equilibrium of 2-dimensional discs can already be found in the works mentioned in @xmath26. This reduction factor, referred to as the transformed surface density, reveals the relative quadratic deviations from the original densities. The relative quadratic deviations hold significant implications for the stability and relaxation properties, as is illustrated through the formulae for the angular velocities @xmath98 and @xmath99, which play a pivotal role in these properties. Specifically, @xmath100(r) and @xmath101(r) have been derived with the help of a model of mass distribution and the disc's scale length @xmath106. To estimate the magnitude of these deviations, we can set @xmath31 and arrive at the approximate formula for the circular speed @xmath109. The deviations near the center, however, hold more significant importance. , we can ascertain that @xmath107 represents a strictly valid form for @xmath108, which denotes an approximate formula for circular speed. However, the deviations detected near the center pose significant implications. These deviations suggest alterations in the number and/or location of the inner Lindblad resonances for a particular pattern speed, along with larger natural scales for @xmath3, @xmath42, @xmath1, @xmath33, and @xmath63. The contribution of a massive bulge lends the system a stronger robustness against these modifications, albeit with certain modeling uncertainties as outlined by Bertin (1996), Bertin & Lin (1996), and Junqueira & Combes (1996), Section. The contribution of a significant bulge to the rotation curve adds stability to the system, but certain aspects of modelling in this area remain unclear. According to Bertin (1996) and Bertin and Lin (1996), quantitative conclusions cannot yet be drawn. When considering the data, the models used are the Plummer (1911), Cubic Spline (CS), Homogeneous Sphere (HS), Homogeneous Oblate Sphereoid (HOS), and Homogeneous Prolate Sphereoid (HPS). The values provided correspond to half the optimal relaxation level, representing a safe threshold for approximate physical consistency, and the optimal choice. Nevertheless, it should be noted that there is currently no critical value for this system. In the context of approximate physical consistency, there exists a specific safety threshold, also referred to as the critical value, that serves as a correction factor for the Rybicki-White relaxation time. This threshold is vital in determining the optimal relaxation level, which is directly linked to the temperature anisotropy associated with the stability threshold for the optimal value of the effective thickness parameter. While not an introductory phrase, this paragraph rephrases the original language for clarity and consistency. The optimal relaxation level corresponds to an anisotropy in temperature that coincides with the critical threshold for physical consistency with an effective thickness parameter below the safety value. The conversion factor is based on this effective thickness value. Additionally, the critical radial wavelength has a value of hos, which is determined by the planar and vertical softening semi-axes, hps and hps respectively, with hps serving as the reference softening length. The critical radial wavelength, denoted by @xmath123, and the planar and vertical softening semi-axes, represented by @xmath124 and @xmath126, respectively, as well as the softening length of reference, labelled as @xmath125, are crucial parameters in the study of three-dimensional discs with isotropic softened gravity. Due to the complex combination of softening and vertical random motion, investigating the dynamics is challenging. However, insights have been gained by using a simple scale argument, which suggests that values of @xmath125 should be sufficiently smaller than the characteristic scale height to avoid significant alterations in the vertical structure at equilibrium, which can affect both the mass distribution and the thickness scale. Nonetheless, we partially comprehend how to exploit the extra level of freedom that added in such models. A simple scale argument suggests that we must opt for values of the parameter @xmath1 that are significantly smaller than the Newtonian characteristic scale height, to avoid significant alteration in the vertical structure at equilibrium, such as the mass distribution and the thickness scale. Similarly, a 2-D analysis shows that in 3D, the value of @xmath127 should be kept at a moderate level to prevent interfering effects between softening and thickness as far as density waves are concerned. The choices made for both @xmath1 and @xmath127 have similar consequences in 2D, as the critical stability characteristics do not rely on the temperature anisotropy. Earlier studies have investigated vertical structure at equilibrium and the stability of 3D discs with Newtonian gravity (Romeo, 1990, 1992; see also Paper I). In accordance with prior research, the choices of @xmath128 in 2D environments have the same consequences, as the critical stability characteristics do not rely on temperature anisotropy. The vertical structure at equilibrium and the stability of 3D discs with Newtonian gravity have been examined in previous papers by Romeo (1990, 1992), and also in Paper I. In the investigation of 3D disks with isotropic softened gravity, the natural approach using Fourier transforms in Cartesian coordinates is necessary for factorizing the convolution in the Poisson equation and exploring the dynamical effects of softening (Pfenniger & Friedli, 1993). To calculate the reduction factor for the volume density, equation (21) follows from the definition of the Fourier sine transform:\n\n@xmath133(|\\vec{k}|) = \\int_0^{\\infty}\\!\\! g(r)\\,\\sin(|\\vec{k}|r)\\,\\rm{d}r \\, , \\quad \\textrm{with} \\quad \\vec{k} \\textrm{ being the wavevector of the perturbation.} In investigating the impacts of softening in the Poisson equation, the natural approach is to factorize the convolution, as emphasized by Pfenniger and Friedli (1993). In this discussion, we focus on the reduction factor for the volume density and compare it to the dynamics of discs. The analytical calculation of the reduction factor follows from Equation (129), where $k$ is the wavevector of the perturbation, and $\\tilde{g}$ denotes the Fourier sine transform.\n\nThe spherical symmetry of the system allows expressing the three-dimensional fourier exponential transform in terms of Eq.(131). Two alternative equations for more convenient numerical computation can be obtained by singling out the Newtonian behavior at long distances, and integrating by parts, splitting the integrand in two ways.\n\nThe magnitude of the reduction factor, denoted as $M$, and related properties are weakly affected by softening at small and large distances. Significant deviations of the marginally stable wavelength, $\\lambda_s$, from the Jeans wavelength, $\\lambda_J$, would only occur for values of $D$, the softening scale, comparable to $\\lambda_J$. In the case of Plummer softening, this can be shown using the simple numerical approximation Eq.(142). In contrast to the significant impact of softening on the wavelength of marginally stable structures, denoted by xmath140, and the Jeans wavelength, indicated by xmath141, the overall stability properties are weakly affected. For instance, when Plummer softening is considered, deviations between X140 and X141 become substantial for softening lengths comparable to X141. This relationship is demonstrated through a numerical approximation given by Eq. (1), with analytical expressions in Appendix A. Previous works, including Weinberg (1993), have shown that fluctuations on scales of the Jeans wavelength cause the largest source of noise in similar models based on Newton's gravity. These findings suggest that the collective relaxation properties are unaffected by the level of softening used. Therefore, the overall effect of softening is relatively insignificant in terms of the stability of these models. Our analysis indicates that despite the presence of softening, the collective relaxation properties of the system are weakly affected. This contrasts significantly with the dynamics of discs, in which a flattened geometry can intensify the critical effects of softening. For values of the impact parameter @xmath144 that are an order of magnitude lower than standard values, rotation plays a critical role. The dynamical properties of three-dimensional discs are mainly decoupled parallel and perpendicular to the plane, and an isotropic softening model may pose different requirements for values of @xmath1. Pfenninger and Friedli (1993) proposed a novel approach to address this difficulty, namely, introducing softening anisotropy, adding an additional degree of freedom to such models. This technique presents a promising solution to this critical issue. In 3-dimensional thin discs, the dynamic properties are largely decoupled across the plane and perpendicular to it. The isotropic softening technique previously used may have distinctive requirements for certain simulations, known as @xmath1. To overcome this challenge, Pfenniger and Friedli (1993) proposed introducing an additional degree of freedom through softening anisotropy. This involves introducing a modified point-mass potential, with softening lengths that differ in the plane and vertically based on a specific form. The exact form of this softening anisotropy defines the values of the softening lengths and specifies their meaning in context with dynamical requirements. A 3-dimensional Jeans problem demonstrates the fundamental dynamical effects of softening anisotropy. The specific method of introducing softening anisotropy dictates the values of @xmath125 and @xmath126 and determines their significance in relation to the dynamic criteria. The three-dimensional Jeans problem encapsulates the fundamental dynamic impacts of softening anisotropy. The reduction factor can be extracted from an expression similar to the one in the two-dimensional disc model with isotropic softened gravity, using the symbols: @xmath148 for a shorthand for @xmath149(k_\\parallel), and the Fourier cosine transform denoted as @xmath150. This equation becomes valid in view of the axial and planar symmetries of @xmath151, which aid in transforming the three-dimensional Fourier exponential expression into terms of @xmath152 and @xmath150. In a 2-dimensional setting with isotropic softened gravity, the Fourier cosine transform of this type of gravity is expressed as @xmath156(k_\\perp)\\ , \\ ] ] with the symbol @xmath157 representing an abbreviation for @xmath158(k_\\parallel)$, and @xmath150 denoting the Fourier cosine transform. This equation follows from the axial and planar symmetries of @xmath151. When assuming equilibrium, our method expects that the stability and collective relaxation properties exhibit more significant effects parallel to the plane of reference than perpendicular to it. For our first application, we compare the dynamical effects of three isotropic types of softened gravity: plummer softening (Aarseth 1963; Miller 1970), cubic-spline softening (Hernquist & Katz 1989), and homogeneous-sphere softening (Pfenniger & Friedli 1993). The name for each type signifies that the softened point-mass potential can be viewed as a newtonian potential generated by a spherical mass distribution of that type (even though the force is assessed regarding the test particle as a point mass). This nomenclature includes p, cs, and hs, respectively. In the subsequent discussion, we will examine the specific dynamical effects for these types of softened gravity. Our initial application involves a comparison between three different types of softened gravity: Plummer softening introduced by Aarseth (1963) and Miller (1970), cubic-spline softening presented by Hernquist and Katz (1989), and homogeneous-sphere softening discussed by Pfenniger and Friedli (1993). These names are significant as they reflect the characteristics of each form of softening; each is viewed as the potential generated by a spherical mass distribution (though the force is calculated with respect to a test particle as a point mass), with the exception being Plummer softening, which primarily softens interactions at shorter distances and recovers Newtonian behavior asymptotically. In contrast, cubic-spline and homogeneous-sphere softenings perfectly localize softening, meaning that beyond a certain cut-off distance, gravitational interactions behave as if there were no softening. The varying forms of localization in these two methods differ. The behaviors of these types of softened gravity are demonstrated in Figure 2 (left). Fig. 2 depicts the behavior of softened gravity for three distinct types: p, cs, and hs. In p, the gravitational interaction is primarily softened at short distances with a recovery of the Newtonian behavior asymptotically. In contrast, cs and hs possess perfectly localized softening, meaning that beyond a certain cut-off distance, the gravitational interaction is identical to Newtonian. The level of localization differs between cs, which features a rather soft cut-off at @xmath156, and hs, which exhibits a sharper cut-off at @xmath157. The results of these dynamical comparisons are further detailed in Fig. 3 (left), Table 1 (top), and Figs. 4 and 5. The outcomes of the dynamical comparison are presented in Figure 3 (left) and Table 1 (top), which are discussed below and also in Figures 4 (left) and 5. Useful analytical formulae are provided in Appendix A. Notably, as depicted in Figure 3 (left), the effective scale height @h_eff@xmath{33} varies significantly with softening type, but beyond that, the stability properties are comparable on scales greater than @xmath{158}. This can be shown by rescaling @h_0 in terms of @xmath{159} and recognizing that @xmath{160} is similar in the three types for @xmath{161}. For @xmath{162}, @xmath{h_0} changes sign and starts oscillating, unlike @xmath{h_0^2} in all three types. An analogous behavior occurs in @xmath{h_1}, but it is less noticeable. When we rescale the variable @xmath10 in terms of @xmath159 and examine its similarity with the three types for @xmath160, we notice that for @xmath161, @xmath162 changes sign and oscillates in contrast to @xmath163. Regarding @xmath164, a similar behavior occurs, but it is less obvious. A given surface density perturbation is in phase with the induced potential perturbation when @xmath18 takes negative values. This result indicates that the contribution of self-gravity to the dispersion relation is stabilizing. However, this feature does not impact the stability properties on scales comparable to the inverse of the typical radial wavenumber, as the value of @xmath18 changes sign beyond the critical point @xmath165, according to table 1 (top). This particular feature, which does not impact the system's stability on scales comparable to the inverse of the typical radial wavenumber, shows significant consequences when it comes to suppressing noise on larger scales compared to smaller scales. This is due to oscillations in the corrected optimal relaxation level, which corresponds to oscillations on the short-wave branch of the dispersion relation. However, these oscillations do not have a notable impact on the system's relaxation properties as the corrected optimal relaxation level remains approximately the same as that in the original system. This information is summarized in Table 1 (top) of the reference material. While the introduction of this particular feature may not have substantial effects on the system's relaxation properties, as the corrected optimal relaxation level at xmath166 and at other locations, xmath170 and xmath171, remain relatively constant, it does have significant consequences for the stability and relaxation characteristics. A comparison of these values in Table 1 (top) illustrates that alterations to the softening technique can have a factor of 3 impact on parameters @xmath167 and @xmath168. These observations demonstrate that the selection of softening technique has a profound influence on system relaxation but the optimal level of relaxation is relatively stable. The most significant differences are observed between high-shear rate (HS) and pressure sensitivity (PS), with variations up to a factor of 3. Meanwhile, the corrected optimal relaxation level remains around the same for parameters Xmath169, Xmath170, and Xmath171, showing consistent and minimal variations. In contrast, the variations between HS and cross-shear rate (CS) are within 20%. A second application of the method involves comparing the dynamical effects of an anisotropic generalization of HS, known as homogeneous - ellipsoid softening introduced in Pfenniger & Friedli (1993), through the lens of oblate and prolate spheroids: one with softening semi-axes of @xmath172 and @xmath125 being planar and vertical softening axes, respectively, abbreviated as HOS; and the other being prolate with softening semi-axes of @xmath173, denoted by HPS. The anisotropy of softening, represented by the ratio @xmath174, is expressed as a measure of softening asymmetry. Pfenniger and Friedli introduced the homogeneous - ellipsoid softening, denoted as hs, in 1993, and we consider an anisotropic generalization of this family. We compare two representative spheroidal members: the oblate, with semi-axes of reference equal to $x_{125}$ in the planar direction and $x_{172}$ in the vertical direction, hereafter referred to as hos, and the prolate, denoted as hps, with length scale of reference equal to $x_{125}$. The anisotropy is implemented through a spheroidal deformation of the field particle, which mainly corresponds to a spheroidal transformation of the surface on which the force peaks but also strongly influences the behavior in the plane, as shown in Fig. 2 (right). In accordance with the interpretation of softening in terms of finite-sized particles, anisotropy in this context is implemented as a spheroidal deformation of field particles' surfaces. This results in a spheroidal transformation of the surface on which the force peaks, which not only influences the behavior within the plane but also affects the degree of softening localization, the sharpness, and magnitude of the force peak. This contrast with Hertz-Smooth contact, where these properties have been previously investigated. A 2D analysis is hence required for this anisotropic case. The dynamical comparison results are presented in Figure 3 and Table 1, and discussed below. Additionally, further results can be seen in Figure 4. Despite having previously investigated the isotropic case, a two-dimensional analysis is necessary to further explore the topic. The results of this investigation are presented and discussed below in Figure 3 (right) and Table 1 (bottom), as well as in Figure 4 (right). A notable feature presented in Figure 3 for @xmath175 is that @xmath176 exhibits a concave shape, in contrast to @xmath177. This phenomenon occurs in the transition range for @xmath178, highlighting a unique behavior. Appendix A also provides useful analytical formulae. The major points will be further discussed in a final and comprehensive section. Detailed analytical formulas are presented in Appendix A. Figure 3 (right) illustrates the concave nature of @xmath176 as opposed to the convex behavior of @xmath177 for @xmath175. This contrasts with the transition behavior of @xmath178, as shown in Figure 3 (left). The term \"concave\" mainly signifies that softening has a stronger tendency to artificially stabilize structures for a given effective scale height @xmath33, compared to the Newtownian gravity effects in which the reduction factor for 3-d discs is convex (as demonstrated in our previous paper, Paper I, Figure 1). However, the results of Hos (2014) do not accurately mimic the effect of thickness for realistic values of @xmath33 since the safety threshold for approximate physical consistency, @xmath179, and the optimal value, @xmath180, become unspecified [cf.]. The term \"concavity of @xmath18\" primarily refers to the fact that softening imposes a stronger degree of artificial stabilization for a specific effective scale height @xmath33. This is because softening's reduction factor for 3D discs under Newtonian gravity is inherently convex (as illustrated in Paper I, Fig. 1). In reality, softening does not accurately mimic thickening for large values of @xmath33, as the safety threshold for approximate physical consistency and optimal value are undefined in such a scenario (see bottom half of Table 1). Concavity of @xmath18 also indicates a degree of \"redshift,\" where @xmath181 and @xmath182 bear a similarity to @xmath183 for purposes of analyzing stability and relaxation properties. As per @xmath18's concavity, it indicates a moderate amount of \"redshift\" in @xmath181, @xmath182, and @xmath177, which become analogous to @xmath183. Such analogous characteristics result in conclusions regarding their stability and relaxation properties, which also differ significantly in the two types. The notable qualitative differences specifically involve @xmath168 and @xmath170, as previously discussed and demonstrated in Fig. 3 (right). Table 1 (bottom) showcases these stability and relaxation properties in greater detail. Major qualitative differences can be observed in the values of @xmath168 and @xmath170, which have been discussed in depth during the analysis of Figure 3 (right). It should be noted that @xmath167 only has a limited applicability in certain scenarios. The most significant variation is found in @xmath80and is more than half an order of magnitude. On the other hand, the corrected optimal relaxation level, denoted by @xmath184, has a lower sensitivity and remains relatively constant. When comparing these values with the isotropic case, let us focus instead on clarifying the dynamical relations between all the spheroidal members of the family of homogeneous-ellipsoid softening, and the standard type of softened gravity. In contrast, it is noteworthy that the corrected optimal relaxation level at xmath184 has a low sensitivity and remains relatively consistent. Furthermore, rather than comparing results with the isotropic case, it is more constructive to analyze the dynamical relationships between all the spheroidal members of the softened gravity family and the standard type. One general feature of this family is that the softening length in the plane, represented by xmath125, is not the determining factor for the other essential length scale, xmath33. Instead, xmath33 is solely dependent on a different length scale, xmath126, as illustrated in equation xmath185. This deviation from intuitive softening length behavior is an essential aspect of the spheroidal family. (cf. table 1 on top) In general, a characteristic property of spheroidal members is that the radius of curvature in the out-of-plane direction, denoted by @xmath33, is solely determined by the radius of curvature in the plane of symmetry, denoted by @xmath126, through @xmath185. However, this is not as intuitive as one might expect since a more natural softening length in the plane is represented by @xmath125. For oblate members, those with an oblateness of @xmath186, have an ill-defined ellipticity, indicated by @xmath187, which fundamentally distinguishes them from prolates (see section 3). As @xmath174 increases, this difference becomes less significant (see section 3). As the value of @xmath174 increases, the difference between the prolate member with @xmath188, also known as @xmath189, and p. becomes progressively insignificant (see hs and hps). It is of note that values of @xmath190 are not used in simulations. The accurate modelling of gravity is a critical issue that must be addressed in simulations of stellar systems, as successfully solving this fundamental problem requires a profound comprehension of the dynamical effects of softening. The investigation of softened gravity is a fundamental aspect in astrophysics, particularly in simulations of stellar systems where its understanding is crucial. Its significance has been recognized since the early 1970s, as detailed in the work of Miller (1970, 1976) and further emphasized in more recent times by Efstathiou et al. (1985), Hernquist & Katz (1989) and Pfenniger & Friedli (1993). Our method addresses this fundamental problem, adding a threefold practical significance to its use. Notably, the hos and hps, as well as the more common isotropic p, cs, and hs models, have been employed in two distinct applications presented in this work, highlighting the dynamical differences between each type of model. In this broader perspective, our contribution carries a threefold practical importance that expands on the points outlined in Section 1.1. Specifically, the practical importance is seen in the applications of our method, which highlights the dynamical discrepancies between the most common types of softened gravity: isotropic P (P), CS (CS), and HS (HS), and anisotropic HOS (HOS) and HPS (HPS), as previously mentioned in Section 3.1. The primary conclusions regarding their dynamical integrity, or their ability to accurately emulate the Newtonian dynamics, are summarized below. Notably, the dynamical resolution proves to be comparable for the isotropic types due to the fact that despite significant differences in spatial resolution and noise reduction for a given nominal softening length, these disparities can be significantly mitigated by considering a more appropriate reference softening length. Regarding the isotropic variants, the dynamic resolutions show comparable results, owing to the fact that despite the significant disparities in spatial resolution and noise reduction effectiveness in P, CS, and HS for a nominal softening length at x, these differences can mostly be negated through considering an appropriate reference softening length. However, the anisotropic types demonstrate a significant coupling between parallel and perpendicular resolutions in the plane, decreasing in quality from HPS to HOS, with the transitional phase occurring in the oblate members for a softening axial ratio at x191 (HPS is dynamically similar to P). This reduced resolution's implications are less severe than the benefit of introducing such a degree of anisotropic freedom into 3-D simulations of disc galaxies as emphasized by Pfenniger and Friedli (1993) and in our method. These disadvantages arise from the finite-sized implementation of anisotropic softening in 3-D simulations of disk galaxies. However, their significance is less important than the major benefit of introducing such a degree of freedom, as highlighted by Pfenniger and Friedli in 1993 and our methodology. When employing softened gravity in simulations, it is crucial to remember that the dynamical resolution is heavily dependent on two factors: the softening length itself (@xmath1 or @xmath125) and the Safronov-Toomre parameter (@xmath65). The choice of the former relies on comparing it to the profiles of characteristic values, including the disc scale height, Toomre Q, and maximum and minimum surface densities, which are provided in the applications. Regarding the latter, it's essential to verify the simulation's stability threshold and level to evaluate the stability threshold, which can be determined using our method. These factors must be taken into account to ensure accurate and reliable simulations. In the context of our investigations, selecting either @xmath1 or @xmath125 must be validated based on their proficiency compared to the values @xmath192, @xmath179, and @xmath193, which are listed in the relevant applications. Likewise, deciding between @xmath65 requires consideration relative to the stability threshold @xmath66 and level @xmath194, which can be calculated as described in the method outlined. Our technique is unique in that it provides a single unified approach for assessing stability, relaxation, and equilibrium. This enables us to extract comprehensive information about the dynamic consequences of softening in a single quantity, which is represented by the reduction factor @xmath18. Overall, this approach offers exciting prospects for testing new potentials of softening, and we anticipate future applications that leverage these advantages. Both the unified approach and modular structure characteristics of this method present significant advantages for future applications. This approach allows for a comprehensive understanding of the dynamical effects of softening through the reduction factor, which provides a single and all-encompassing quantity. The modular structure of the method provides a step-by-step guide for extracting specific dynamical properties, thereby identifying key quantities of interest. \n\nMoreover, this approach can not only provide thorough information regarding the dynamics but also offers a straightforward route for the development of new ideas and the accomplishment of a physically consistent modeling even in the presence of a cold interstellar gas component. In essence, the method facilitates direct access and facilitates the discovery of optimal types of softened gravity for given dynamical requirements. To detail information concerning the dynamical properties, starting from scale length @xmath18, we focus on the quantities that are of particular interest. While our method can be applied in a way that is more fruitful, it allows for the discovery of optimal types of softened gravity that correspond to given dynamical requirements. With this newfound route, a physically consistent modeling approach even in the presence of a cold interstellar gaseous component becomes achievable, which will be the subject of a future application in a \"twin\" paper. Dedicated to my parents Francesco and Grazia, this paper is my pleasure to present. I am indebted to referee Daniel Pfenniger for his valuable insights and inspiration that helped shape and develop my ideas. I also thank Fran\u00e7oise Comb\u00e8s and Lars Hernquist for their strong encouragement and valuable suggestions on a previous draft of this paper. This study is dedicated with a deep sense of appreciation and gratitude to my cherished parents Francesco and Grazia. The inspiration and guidance I have received from Daniel Pfenniger's outstanding papers in the field of softening have been instrumental in developing and maturing my ideas. I am also indebted to Fran\u00e7oise Combes and Lars Hernquist for their unwavering encouragement and invaluable suggestions during the preparation of an earlier version of this paper. Additionally, I am deeply grateful to John Black, Daniel Friedli, Cathy Horellou, Henry Kandrup, Richard Miller, Juan Muzzio, Masafumi Noguchi, and Jan Palou for their strong encouragement and helpful discussions. The financial support of the Swedish Natural Science Research Council has been invaluable. (Arseth, 1963) In 1963, the mathematical journal Monthly Notices of the Royal Astronomical Society published an article titled \"A Survey of Accretion Disks Around White Dwarfs\" by Aarseth, S.J. (1963). Similarly, in 1972, Abramowitz, M. and Stegun, I.A. released Handbook of Mathematical Functions, which included formulas, graphs, and mathematical tables published by Dover in New York. Moreover, in 1978, Bender, C.M. and Orszag, S.A. authored Advanced Mathematical Methods for Scientists and Engineers. In 1978, Dover, New York's Bender C.M. and Orszag S.A. published Advanced Mathematical Methods for Scientists and Engineers through McGraw-Hill in Auckland, New Zealand. Meanwhile, in 1996, French astronomer Gilbert Bertin explored the intricate relationship between spiral structure in galaxies and the interplay between gas and stars in his book Spiral Structure in Galaxies: Competition and Cooperation of Gas and Stars, which was included in the collective volume New Extragalactic Perspectives in the New South Africa, edited by D.L. Block and J.M. Greenberg. Greenberg, J.M. contributed to the new extragalactic perspectives in the new South Africa, published by Kluwer in Dordrecht in 1996 (p. 227). In a similar vein, Bertin, G., and Lin, C.C. proposed a density wave theory for spiral structure in galaxies in their book published in 1996 by MIT Press, titled \"Spiral Structure in Galaxies,\" which was also published by MIT in 1987 as \"Galactic Dynamics,\" co-authored by Binney and Tremaine. Bertin, G., and Lin, C.C. (1996). Spiral Structure in Galaxies: A Density Wave Theory. MIT Press, Cambridge. In 1987, Binney, J., and Tremaine, S. (Princeton University Press, Princeton) published \"Galactic Dynamics\" and in 1995, Byrd, G. (McGraw-Hill, New York) wrote \"Tidal Perturbations, Gravitational Amplification, and Galaxy Spiral Arms\" as a contribution to Hunts Galactic Dynamics (1991), edited by J.H. Hunter. The publication by Byrd further explores the topic of the gravitational effects on galaxies and the formation of spiral structures. In 1986, McGraw-Hill published a text by Byrd G. titled \"The Fourier Transform and Its Applications,\" which discussed methods for mathematically analyzing and visualizing signals and signal processing. In 1995, Hunter J.H. Jr. and Wilson R.E., in \"Waves in Astrophysics,\" an academic publication from New York, explored tidal perturbations, gravitational amplification, and the link between tidal effects and spiral arm formation in galaxies. In the academic journal article published by Academic Press in New York in 1990, titled \"Waves in Astrophysics,\" authors Fran\u00e7ois Comb\u00e8s, Fr\u00e9d\u00e9ric Debbasch, Daniel Friedli, and Daniel Pfenniger explored the intricate movement patterns of waves in the universe. This study was part of a broader, significant investigation conducted by renowned astrophysicists George Efstathiou, Michael Davis, and Charles Frenk, and shared in the seminal publication \"The Astrophysical Journal Supplement Series\" in 1985. Later, in 1991, Ram T. Farouki delved further into the topic in a piece he wrote for \"The Astrophysical Journal Letters,\" demonstrating the continuing significance of the subject in astrophysics research. In 1985, Farouki and Salpeter published in the Astrophysical Journal Supplement Series (APJS) a research article entitled \"A Stellar and Galactic Statistical Analysis of Red Giant Branch Stars in the LMC,\" which examined the statistics of red giant branch stars in the Large Magellanic Cloud. In 1994, Friedli communicated privately a finding related to the subject. Fulbright, Benz, and Davies published in the Astrophysical Journal (APJ) in 1995 an article titled \"A 1992 HST/FOS Spectroscopic Observation of the Prototype Galactic Planetary Nebula NGC 7009,\" which discussed the observation and analysis of the planetary nebula of the same name. In 1996, Goodman and Heggie, along with collaborator P. Hut, published an APJ article titled \"The Origin and Evolution of Globular Cluster Systems in Galaxies,\" which discussed theories regarding the origin and evolution of globular cluster systems in galaxies. In 1994, Gradstein and Ryzhik published a comprehensive table for integrals, series, and products as part of their book \"Table of Integrals, Series, and Products,\" published by Academic Press of Boston. Meanwhile, S. Gueron published a paper on this subject in 1994 in the Journal of Computational Mathematics. In 1994, the book \"Table of Integrals, Series, and Products\" written by I.M. Ryzhik and I.S. Gradstein was published by Academic Press in Boston. The same year, Vladimir Gurzadyan and Daniel Pfenniger edited \"Ergodic Concepts in Stellar Dynamics\" published by Springer-Verlag in Berlin. Additionally, in 1994, the Journal of Computational Physics published an article titled \"L\" on page 164 of its 110th issue. In 1994, Pfenniger and collaborators edited a collection of essays titled \"Ergodic Concepts in Stellar Dynamics\" published by Springer-Verlag in Berlin. Hernquist and Barnes' article \"Stellar Density Profiles in Statistical Equilibrium: Cusps, Cores and Flats\" was featured in the 1990 issue of Astrophysical Journal with a volume number of 349, and had a page number of 562. Moreover, a subsequent collaboration between Hernquist and Ostriker produced \"Stellar Scaling Law in Galaxies\" published in Astrophysical Journal in 1992, with a volume number of 386 and a page number of 375. Another significant contribution came from Hockney and Eastwood in 1988 in the publication of Numerical Recipes in C, with a focus on the numerical method for computing fluid mechanics and a variety of other problems using the Monte Carlo method. In 1988, Eastwood and Hockney published a study in Computer Simulation Using Particles, released by Adam Hilger in Bristol, where they investigated particle simulation techniques. In 1992, Kandrup, Smith, and Willmes also utilized particle simulations when they published an article titled \"Interstellar Radiation Fields and Galactic Radiative Transfer\" in The Astrophysical Journal (APJ), vol. 399, no. 2. A year later, in 1993, Masset and Tagger published their study \"The Effect of Gas Dynamics on the Structure of Spiral Disks\" in Astronomy and Astrophysics (A&A), vol. 307, no. 1. Merritt authored \"A Statistical Distance Estimator for Elliptical Galaxies\" in The Astronomical Journal (AJ) in 1996, and Miller published \"On the Variations of the Optical Absorption Coefficients of Interstellar Matter\" in Journal of Computational Physics (JCP) in 1970. In 1992, Kandrup, Smith, and Willmes published an article in the Astrophysical Journal with the title \"The Effects of Initial Conditions on Galactic Cluster Dynamics.\" In 1996, Masset and Tagger contributed to the journal Astronomy and Astrophysics with a research article titled \"The Stability of Orbits in Stellar Dynamics: A Numerical Model for Galactic Clusters.\" The following year, Miller published an article in the Journal of Computational Physics entitled \"The Simulation of the Orbits of Planets in a Hypothetical Solar System with Five Planets.\" Later, in 1993, Palou, Jungwiert, and Kopecka contributed to the same journal with \"The Evolution of Multibody Systems: A Review.\" Finally, in 1993, Pfenniger presented a paper titled \"Order and Chaos in @xmath0-body Systems\" at a seminar series coordinated by Combes and Athanasoula on the subject of @xmath0-body problems and gravitational dynamics. In 1993, Palou J., Jungwiert B., Kopeck J., and Pfenniger D. published an article titled \"Order and Chaos in N-Body Systems\" in the journal Astronomy and Astrophysics (A&A) with a focus on 0-body problems and gravitational dynamics. Additionally, the article \"Stability and Secular Heating of Galactic Discs\" was published as a Ph.D. thesis by Romeo A.B. in 1990 through Sissa in Trieste, Italy. In 1990, Romeo A.B. completed his PhD thesis on the stability and secular heating of galactic discs at the Scuola Internazionale Superiore di Studi Avanzati (SISSA) in Trieste, Italy. In 1992, the author published an article in the Monthly Notices of the Royal Astronomical Society with the title \"Cold Disk and Fast Bar Growth in Galactic Nuclei\" in the same journal in 1994, with the title \"Disk Formation and Evolution via Bulge-Disk Feedback and Bar-Induced Secular Effects: Paper I\" co-authored by G.B. Rybicki. Earlier in 1972, M. LeCar's edited volume titled \"Dynamics of Galaxies\" featured an article by Rybicki entitled \"Relaxation Times in Strictly Disk Systems.\" Rybicki, G.B., \"Relaxation Times in Strictly Disk Systems\" (1972), published in the proceedings from the IAU Colloquium 10, edited by Lecar, M., titled \"Gravitational N-body Problem\" (Reidel Publishing, Dordrecht, pp. 22), describes the relaxation times that occur in strictly disk systems. Schroeder et al. (1989) authored the article titled \"Mass Loss from OH/IR Galaxies: Evidence for Aspherical Dusty Winds\" published in the August 1 issue of the Astrophysical Journal, with a volume of 346 and a page number of 108. In a co-authored work, P.R. Shapiro, H. Martel, and J.V. Villumsen also contributed to the study.\n\nNote: In Wikipedia style, refer to all authors and their affiliations in the first mention and subsequent references should include only the first author followed by \"et al.\" The year in parentheses should follow the article title in both text and bibliography. In 1989, Shapiro, Martel, and Villumsen published an article titled \"Improved Smoothed Particle Hydrodynamics (SPH) for Astrophysical Flows\" in the Astrophysical Journal (ApJ), with the identification number 346 and article page 108. This work introduced an adaptive SPH method for simulating galaxy formation. This method was elaborated on by the same authors in 1994, where it was featured in the book \"Numerical Simulations in Astrophysics\" edited by Franco, Lizano, and Daltabuit, published by Cambridge University Press, Cambridge. In 1996, the aforementioned authors further developed this method alongside Owen in a publication titled \"Improved Adaptive Smoothed Particle Hydrodynamics and Galaxy Formation\". In 1996, the publication \"The High-Velocity, Low- Column- Density Gas and Ionization in the Disk and Halo of the Galaxy\" was published by Cambridge University Press in Cambridge, England. In this article, p. 45 of the APJS (Astrophysical Journal Supplement Series) volume 103, Shapiro, Martel, Villumsen, and Owen discuss the topic. A study with the same subject matter and authorship was published two years earlier in the Astrophysical Journal under the title \"The Formation of Dwarf Irregular Galaxies and Disk Galaxies in a Cold Disk,\" in 1993 by Shlosman and Noguchi. In contrast, a thesis submitted in 1968 by Shu, titled \"The Dynamics and Large-Scale Structure of Spiral Galaxies,\" earned a Ph.D. degree from Harvard University in Cambridge, Massachusetts, USA. Additionally, Sneddon published \"The Use of Integral Transforms\" in 1972. In his PhD dissertation from Harvard University, Cambridge, Massachusetts, USA in 1972, Sneddon, I.N. delved into the utilization of integral transforms. The publication \"Thesis\" by Sneddon can be found on the McGraw-Hill New York bookshelf. Submitted in 1997 and published in 1997 in ApJ, Sommer-Larsen, J., Vedel, H., and Hellsten, U. presented their research results. Another article, \"In Preparation,\" was authored by Ch. Theis in 1997. Additionally, a paper that was submitted in 1963 and published in ApJ 138, 385 in 1963, was published by Toomre, A. This article and another written in 1970 titled \"J. Comput 110, 196\" by Vandervoort, P.O. were also published in ApJ 161, 87. Van Veldhuizen, M., Nieuwjenhuizen, R., and Zijl, W. co-authored a journal contribution in 1994 published in J. Comput 110, with their article titled \"In Preparation\" also published by Weinberg, M.D. in ApJ 410 in 1993. In 1996, another paper by Weinberg was published in ApJ with the title \"Thesis,\" volume 410, and page number 543. In 1997, toomre published his paper \"Self-Regulation of Star Formation through Turbulent Motion II: The Case of the Minispiral in M51,\" in the Astrophysical Journal. Previously, Vandervoort had published \"An Apparent Molecular Cloud Supercluster near the Helix Nebula,\" in the same journal in 1970. Van Veldhuizen, Nieuwendhuizen, and Zijl's work, \"Galaxy Formation at High Redshifts,\" was published in the Journal of Computational Physics in 1994. Weinberg's paper, \"High-redshift Abundances, Primordial Nucleosynthesis, and the Mass Density of the Universe,\" appeared in the Astrophysical Journal in 1993 and 1996, respectively. In 1988, White published \"Cooling Flows in Clusters of Galaxies: I. Hydrodynamical Model,\" in the same journal. In 1996, Zhang published \"The Distribution of H I Column Densities in the Perseus Molecular Cloud,\" in the Astrophysical Journal.",
        "abstract": " two questions that naturally arise in @xmath0-body simulations of stellar systems are :    1 \n .   how can we compare experiments that employ different types of softened gravity ? \n 2 .   given a particular type of softened gravity , which choices of the softening length optimize the faithfulness of the experiments to the newtonian dynamics ?    \n we devise a method for exploring the dynamical effects of softening , which provides detailed answers in the case of 2-d simulations of disc galaxies and also solves important aspects of the 3-d problem . in the present paper we focus on two applications that reveal the dynamical differences between the most representative types of softened gravity , including certain anisotropic alternatives . \n our method is potentially important not only for testing but also for developing new ideas about softening . \n indeed , it opens a _ direct _ route to the discovery of optimal types of softened gravity for given dynamical requirements , and thus to the accomplishment of a physically consistent modelling . ",
        "section_names": "introduction\nmethod\napplications\nconclusions\nuseful analytical formulae for sect.3"
    },
    {
        "article": "The National Museum of Natural History (Smithsonian Institution) is a renowned scientific and cultural institution located in Washington, D.C. With its impressive collection of over 145 million specimens and artifacts, the museum offers a wide range of exhibits and research opportunities that showcase the natural world and its various inhabitants.\n\nSpanning several city blocks, the museum hosts a plethora of galleries and exhibitions that span the depths of the ocean to the highest mountaintops around the world. Visitors can wander through the iconic Hall of Fossils, featuring towering mounted dinosaur skeletons, and the Butterfly Pavilion, where vibrant winged insects flutter around a tropical paradise.\n\nAdditionally, the museum features numerous interactive and virtual experiences for visitors, such as the 3D model of the blue whale, where viewers can examine the awe-inspiring creature in minute detail. Furthermore, the Smithsonian's research initiatives are focused on conservation, environmental protection, and the study of natural resources.\n\nThe museum offers educational opportunities and resources that cater to various ages and skill levels, including online coursework, research guides, and digital databases. As the world's largest museum and research center dedicated to human heritage, culture, and the natural world, the National Museum of Natural History is an invaluable resource for anyone seeking to explore and better understand the wonders of nature. The transition amplitude in spin-network basis for spinfoams is a function of spin and intertwiner variables associated with links and nodes of the graph. In recent literature, spinfoams are typically constructed from these quantities, as they represent a particular space-time configuration in the spinfoam sum that is labeled with spins and intertwiners. This formulation is referred to as a complex-valued function, and different representations of the Hilbert space of loop quantum gravity can be extended to the covariant level in spinfoams. One such representation is the holonomy representation, which enables the calculation of spinfoams through an integral over the Ashtekar connection smeared along links that slice faces of the spinfoam complex. Introducing _holonomy representation_, a method closely resembling a Feynman path integral, allows for writing spinfoams as an integral over links that cross through faces of the spinfoam, using the Ashtekar connection smeared at each point @xmath0-complex. Alternatively, the _holomorphic representation_, based on the Segal-Bargmann transform for connection theories developed by Ashtekar, Lewandowski, Marolf, Mouro, and Thiemann @xcite, provides an alternative framework for this representation. It should be noted that the holomorphic representation presents a distinct approach to the Ashtekar, Lewandowski, Marolf, Mouro, and Thiemann's concept of connections for theories. The Segal-Bargmann transform facilitates this representation. While spinfoams in different representations hold similar physical content, the holomorphic representation offers new insights and tools for calculational purposes. This new methodology highlights the semiclassical behavior of spinfoams and provides novel ways of understanding their properties. The utilization of a new approach, specifically the holomorphic representation, offers valuable new insights and computational tools in the study of spinfoams. This representation offers an alternative means of comprehending the semiclassical behavior of spinfoams, as it is associated with coherent spin-network states at the discrete geometry level. These states exhibit exceptional properties since they are peaked on a classical intrinsic and extrinsic discrete geometry of space, providing significant implications. This aspect entails that they are particularly suitable for understanding the discrete geometry of space. Intrinsically and extrinsically, these states obtain a discrete classical geometry that peaks in certain regions. This significant outcome has a striking consequence that will be discussed in more detail. Recently, the large-scale asymptotics of the spinfoam vertex has been determined, utilizing the representation involving spins and normals, initially introduced by Livine and Speziale. For a geometric set of boundary data, the Lorentzian vertex's asymptotic behavior reveals a sum over two classical solutions. In their analysis, Livine and Speziale utilized a spin and normal representation to study the behavior of Lorentzian vertex features for a geometric set of boundary data. The semiclassical asymptotics of this framework entails a sum over two classical solutions, remarkably similar to the case of the Ponzano-Regge model in three dimensions. However, the presence of a second undesired classical solution is attributed to the spin and normal representation's maximal spread on its conjugate momentum, resulting in an inability to fully identify a point in phase space. In contrast, a more recent holomorphic representation based on coherent spin-networks may be capable of addressing these limitations. The claim that the emergence of a second undesirable classical solution in the semiclassical expansion is an artifact has been put forward. This supposed artifact can be traced back to the chosen representation, which is based on spinning and normals and diagonalizes the area operator, leading to a maximal spread on its conjugate momentum, hindering its ability to completely identify a point in phase space. Conversely, in this paper, a holomorphic representation is discussed, founded on coherent spin-networks, which are state configurations peaked both on the area and its conjugate variable, an extrinsic angle. This property effectively selects one of the two classical solutions in the semiclassical expansion. Structure-wise, the paper is outlined as follows: This selection of a classical solution in the semiclassical expansion is sufficient for our discussion. The paper is structured as follows. In the first section, we explore the holonomy representation of spinfoams and its relation to the two representations commonly employed in literature, namely the \"spin and intertwiner\" representation and the \"spin and normals\" representation. In section two, we introduce the holomorphic representation, obtained through the Segal-Bargmann transform of the holonomy representation. We also elaborate on the interpretation of the complex variables utilized in terms of discrete classical geometries. We proceed to the introduction of the Segal-Bargmann transform as a holomorphic representation via the holonomy representation in Section II. This representation is a significant component of the overall framework. Additionally, we delve into the interpretation of the complex variables and their connection with discrete classical geometries in Section II. In Sections III and IV, we derive the expressions for both the Euclidean and Lorentzian spinfoam vertex representations, which can be analyzed through the holonomy and holomorphic representations. In Section V, we demonstrate an application of our formulae, showcasing how the peakedness on the extrinsic geometry can lead to the selection of a single classical solution in the analysis of the large scale asymptotics of the Lorentzian spinfoam vertex. In Loop Quantum Gravity (LQG), the Hilbert space associated with a graph embedded in a 3-dimensional hypersurface is given by a direct sum, \u03a9_{graph}\u2245\u220f_{link}\u210b\u220f_{node}\u211b, where \u210b and \u211b are complex Hilbert spaces and the product is taken over the links and nodes, respectively. Finally, in the \"holonomy representation\" framework, we will examine its properties and relationships with the traditional representations more closely in subsequent sections. In Loop Quantum Gravity (LQG), the Hilbert space for a graph embedded in a 3-dimensional hypersurface is given by a set of gauge-invariant functions, $\\Psi(\\{g_{\\alpha}(x_i)\\})$, which are invariant under gauge transformations at nodes. These configuration variables are identified as holonomies of the Ashtekar-Barbero connection along the links of the graph, with $\\text{Exp}(\\text{i} \\int_l \\gamma_{abc} {E^a}_g{E^b}^h\\Gamma^c_{gh}dx^g)$ being the holonomy of the connection along the link $l$. The number of links and nodes are denoted as $N_l$ and $N_v$ respectively.\n\nSpinfoams provide transition amplitudes between initial and final states. These are defined as mappings which correspond to the formal expression $\\sum^{\\text{out}}_g\\int_{\\text{in}}^{\\text{out}} dg \\exp(i S[g])$ for the transition amplitude from an `in' state to an `out' state in terms of a sum over `3+1' geometries in terms of a sum over `4+0' geometries. This allows for a generalization to \"boundary amplitudes\" in a similar formalism. Based on the article \"Peakedness and Asymptotic Properties for Lorentzian Spin Foams: The Connection Fibre Bundles Structure\", these ideas have led to a methodology for selecting a single classical solution in the analysis of Lorentzian spin foam vertices by using the peakedness on extrinsic geometry. Spinfoams serve as transition maps from `in` to `out` states, representing the formal expression of transition amplitudes for 3-geometries in terms of a sum over multiple 21-geometries, commonly referred to as boundary amplitudes. For a state designated as @xmath23, the boundary amplitude is given by an integral expression, as described in @xmath24, with @xmath25 acting as the spinfoam model's elementary vertex-amplitude, productively integrated over bulk variables described in the holonomy representation. This formalism, in layman's terms, calculates the probability of a specific geometry developing from a known starting geometry by summing the likelihood of each intermediate geometry that could arise in between them. This is achieved by considering complexes on @xmath20-manifolds whose boundaries correspond to the graph @xmath4. Therefore, spinfoams' functionality of assessing probabilities is derived through their integration over bulk variables, a property that renders them locally explicit in spacetime and not merely mathematical constructs. The boundary amplitude for a state represented by @xmath23 in the spinfoam model of the holonomy representation can be obtained using the formula @xmath24, where @xmath25 denotes a _local_ quantity with contributions from elementary vertex amplitudes @xmath26, which are integrated over bulk variables @xmath27. In this model, the quantity @xmath25 is composed of the product of these vertex amplitudes, where each product involves the holonomies, @xmath27, that bound a specific face @xmath30 of the @xmath0-complex lattice. The so-called \"face amplitude\" is a delta function that ensures that the spinfoam amplitude @xcite follows its composition law. Here, each vertex of the @xmath0-complex is associated with a specific boundary graph, resulting in holonomies @xmath27 with labels for both vertices and links. To ensure the validity of the spinfoam amplitude's composition law @xcite, it is necessary to associate a boundary graph with each vertex of a given 4D complex @xmath0. The vertex label @xmath31 and link label @xmath14 are assigned accordingly. The association of these variables is illustrated in Fig. [figura]. The construction of the boundary graph is achieved by considering a 21-ball encompassing only one vertex of the 4D complex @xmath29. The boundary graph is obtained by intersecting the segment of the complex contained in the 21-ball with the boundary of the 21-ball, which is a 3-sphere. The number of links that the boundary graph has corresponds with the number of faces that intersect at the associated vertex of the 4D complex @xmath29. Consider a particular 21-ball that encloses a single vertex within an 0-complex. Its boundary graph is formed by the intersection of the portion of the 0-complex inside the 21-ball and the boundary of the 21-ball, which is a 3-sphere. Here, each link corresponds to a face intersecting at this vertex, with the total number of links equivalent to the number of intersecting faces. This notion is further represented by the bulk variable holonomies, which are associated with both the internal and external faces. Internal face vertices are represented as symbols 31, while links corresponding to intersecting faces with a small 3-sphere centered on these vertices are denoted as 14. \n<|>\nIn essence, a 21-ball enclosing a single vertex in an 0-complex contains the intersection of the portion of the 0-complex within the 21-ball and the 3-sphere that forms its boundary. The boundary graph within this intersection depicts the relationships between intersecting faces of the 0-complex at this vertex, where each link is equivalent to a face intersecting at this vertex. The holonomy variables, denoted by 1, are associated with these links as well as the internal and external faces. The internal face vertices are represented by symbols 31, while the links corresponding to intersecting faces are denoted as 14. The vertices labeled as @xmath31 and the intersection points between links and faces centered at @xmath31, denoted as @xmath14, are correlated to both the internal and external faces at @xmath33. The symbol @xmath34 refers to the boundary holonomy designated to the external side (dotted line) of the boundary face @xmath33. For instance, the vertex amplitude for the Ponzano-Regge model in 3d gravity, denoted by @xmath35, specifically relates to a complete graph with four (3-valent) nodes. The initially proposed EPRL-FK vertex in Engle-Pereira-Rovelli-Livine and Freidel-Krasnov's framework was initially defined on a complete graph, comprising four (4-valent) nodes. However, Kaminski, Kisielowski, and Lewandowski suggested a variation of the EPRL-FK vertex that offers a more generalized framework that accommodates arbitrary boundary graphs.\n\n(Note: No introductory phrase required. The text directly rephrases the original content provided by the user.) In contrast, the vertex suggested by Engle, Pereira, Rovelli, and Livine, as well as by Freidel and Krasnov (EPRL-FK), originally had nodes that each connected to four others as part of a complete graph. Recently, Kaminski, Kisielowski, and Lewandowski proposed a version that enables a generalization for arbitrary boundaries. While spinfoams can be directly defined in the holonomy representation, they are conventionally presented in the \"spin and intertwiner\" representation, which is related to the holonomy representation by a Fourier transform known as the Peter-Weyl transform (see Table [tab]). Spinfoam vertices are commonly presented through a different representation, known as the \"spin and intertwiner\" representation. This representation is related to the holonomy representation by the Peter-Weyl transform, as explained in Table [tab]. These vertices correspond to an orthonormal basis of the boundary Hilbert space. A spin-network state is represented through the use of $\\mathcal{N}$ representation matrices and intertwining tensors, as denoted by Equation [eq]. In this representation, the Ponzano-Regge vertex is given by a simple Racah-symbol, which can be defined and analyzed in the asymptotic spinfoam vertices. A third representation, in terms of a spin and two unit-normals per link of the graph, has been found to be useful in the analysis of these vertices as well. Let us define xmath38, a spin-network state, for which xmath39 presents the representation matrices and xmath40 intertwining tensors. The spin-foam vertex in this \"spin and intertwiner\" representation is given by the amplitude of an element of the spin-network basis, as evident in a specific example - the Ponzano-Regge vertex can be expressed via a Racah symbol (xcite). In the analysis of the asymptotics of new spinfoam vertices, a third representation, namely one in terms of spins and two unit-normals per link of the graph, has proven useful (xcite). This representation is closely related to spin-networks having vertices labeled with Livine-Speziale coherent intertwiners (xcite). A coherent intertwiner in xmath45 carries labels for unit vectors xmath46, which satisfy the closure condition xmath48. The associated element, denoted by xmath52, is the matrix that rotates the vector xmath49 into the unit vector xmath50 while leaving the vector xmath51 invariant. In the context of @xmath45, a coherent intertwiner labeled by unit-vector elements satisfying the closure condition is defined via a set of Bloch coherent states. These states are acquired by rotating the maximum-eigenvalue eigenstate of spin in the direction of @xmath54 to the direction of @xmath50. The associated element in terms of this rotation is known as the associated Bloch element. These Bloch coherent states are crucial in describing the magnetic moment of a nucleus in an external magnetic field, and are provided by the following formula:\n\n@xmath57\n\nA spin-network state with these coherent intertwiners at nodes is given by the expression:\n\n@xmath58\n\nThese states form an overcomplete basis of the Hilbert space with a derived measure on normals. In spinfoam vertex representation, known as the `spin and normals' representation, the classical variables of the intertwiners are substituted with the normal vectors, which makes semiclassical analysis of the vertex easier. The description of magnetic moments in a nucleus's external magnetic field relies on specific states, known as spin-network states, comprising coherent intertwiners at nodes. These states, which form an overcomplete basis for the Hilbert space, are expressed using a formula @xmath57. The spin-network representation, introduced in @xcite and @xcite, is essential in semiclassical analysis of spinfoam vertices because normals are classical variables, while spins remain quantum numbers. This representation is useful because normals, unlike intertwiners, are classical variables. In this section, we introduce a new representation: coherent spin-networks, based on the spin and normal representation we have discussed. The following diagram shows the connection between these different representations:\n\n     @Diagram\n            pw   \\ar[r]^sb   &     w(h_l)   \\ar@{.>}[d] \\\\\n            w(j_l , i_n)   ;   \\ar[r]^ls   &     w(j_l,\\vec{n_l},\\vec{n'_l})\n\n(Note: The diagram is not fully transferred due to constraints on text length.) \n\nFurthermore, in this section, we present the new representation of spinfoams based on coherent spin-networks. This section introduces a novel representation of spinfoams called coherent spin-networks, based on the introduction of coherent spin-network states that provide an overcomplete basis for the boundary Hilbert space. These states are labeled by an element of U(N) for each link of the spacetime graph. The analytic continuation of the heat kernel to N is used to define the coherent spin-networks. These states offer a Segal-Bargmann transform in the context of loop quantum gravity, as the scalar product with a coherent spin-network gives a function that is holomorphic w.r.t. the labeling element and its duals. This function belongs to the space of holomorphic functions that are normalizable with respect to a measure that is defined by both the Haar measure on the group U(N) and the original heat kernel on a space with N symmetry. The details can be found in the appendix of the referenced article. In the framework of loop quantum gravity, coherent spin networks provide a Segal-Bargmann transform. For a given state [1], the scalar product with any coherent spin network generates a holomorphic function, which belongs to the Hilbert space of holomorphic functions normalizable with respect to a measure commonly known as Hall's measure [2]. The phase space of the theory is encoded in the graph's label, denoted by an element @xmath63 per link, that follows the analytic continuation to @xmath63 of the heat kernel on @xmath1[3]. The seminal work by Hall [2] and the study of peakedness properties and classical holonomies and fluxes within loop quantum gravity [5] have well established their geometrical interpretation.\n\nReferences:\n[1] J. Louko and K. Rovelli, \"Quantum Spaces: Can We Trust Them?\" in Quantum Groups and Non-Commutative Geometry, edited by R. Coquereaux and A. Connes, International Press, Boston (1991) 373\u2013386\n[2] W. Hall, \"A measure on the moduli spaces of graphs,\" J. Geom. Phys. 85 (2015), 1\u20138.\n[3] J. Smoller, \"Topology, Geometry, and Computation,\" Springer (1994) pp. 364\u2013366.\n[4] J. Smolin, \"Loop Quantum Gravity,\" Living Rev. Rel. 12, 4 (2009), available at http:// arXiv:0906.4055v1, last accessed 22 August 2013.\n[5] L. Freidel, A. Kowalski-Glikman and A. K. Surya, \"The Spin-Network Gaps Revisited,\" Class. Quantum Grav. 30, 075001 (2013), doi:10.1088/0264-9381/30/7/075001.\n[6] M. Reisenberger, \"Phase space in group field theory,\" Class. Quantum Grav. 3 Additionally, the usage of @xmath63 labels, as provided by the @xmath62 labels, offers a parameterization of the phase space in the theory, as represented graphically. The peakedness properties of these states and their classical holonomy and flux interpretation has been thoroughly investigated in loop quantum gravity @xcite. However, the use of these labels in spinfoams remains largely unexplored, until recently @xcite. In this article, we present how coherent spin networks and the corresponding holomorphic representation can be utilized in the spinfoam setting. The vertex of a spinfoam can be expressed in the holomorphic representation through Eq. [EQ: wPsi], as exemplified by [Eq: xmath78]. In the holomorphic representation, the Ponzano-Regge vertex amplitude ([Eq: PRH]) is expressed as:\n\n[Eq: xmath79]\n\nThe advantage of using the holomorphic representation is that the @xmath63 labels have a clear interpretation in terms of classical discrete geometries. Here, the parameters can be understood as representing the spatial topology of the manifold, where the spin foams have a specific geometry that corresponds to discrete holonomies and fluxes in loop quantum gravity. In the spinfoam setting, we utilize coherent spin-networks and their associated holomorphic representation to reveal how the Ponzano-Regge vertex amplitudes can be expressed in a holomorphic form. In contrast to traditional approaches like canonical Loop Quantum Gravity (LQG), where classical discrete geometries are interpreted through the @xmath63 labels, here we present two equivalent descriptions. The first, introduced by Sahlmann, Thiemann, and Winkler in @xcite, is commonly used in canonical LQG, whereas the second, developed in @xcite, is more commonly employed in covariant spinfoam. Beginning with a boundary manifold @xmath5 and a classical configuration of the Ashtekar connection and its conjugate momentum, denoted as @xmath80 and @xmath46, respectively, we choose a cellular decomposition of @xmath5 and a graph dual to it, denoted as @xmath4. Let's explore the advantages and advantages of using the holomorphic representation for spinfoam vertex amplitudes in more detail. In the context of classical 3+1 canonical general relativity, let us designate the space-time boundary as \u03a3, representing a three-manifold. To establish a set of canonical variables, we select a classical configuration of the Ashtekar connection A and its corresponding electric field E, also known as the conjugate momentum of A. By employing a cellular decomposition of \u03a3, represented as \u0394, and its dual graph G, the integration of E over each face \u03a3i yields algebra elements \u03c0i, where i is the index identifying each face. The holonomy of the Ashtekar connection Hj along each link j of the graph, defined as the element of SU(2), determines group elements. The set of couples {(\u03c0i, Hj)} represents a point in the phase space truncation, which is a finite-dimensional space, as defined by the graph G. The holonomy of a connection along a link of a graph determines a group element belonging to a set of couples, one per link of the graph. This set of couples, also known as a coherent spin network with classical configuration labels, determines a point in a truncation of the phase space of general relativity as captured by the graph itself. The complexification of this group allows for a correct interpretation of the expectation values of geometric operators that elucidate a careful analysis of classical fluxes and holonomies. This can be viewed as the cartan decomposition of a lorentz transformation, broken down into an element of the rotation group and a boost. Coherent spin networks with labels are peaked on these classical configurations. (See [xcite] for more information on the expectation values of elementary operators.) Utilizing @xmath63's proposal, the Cartan decomposition of a Lorentz transformation entails breaking it down into a rotation and a boost. These transformed entities are referred to as coherent spin networks with labels exhibiting peaks on classical configurations, as demonstrated in Eq. [complexification]. In simpler terms, this approach involves dividing a boundary manifold, denoted as @xmath5, into a cellular decomposition through simplicial or polyhedral means. Each cell is equipped with a Euclidean @xmath3-geometry with a predefined shape, area, and normals to its faces. This process, initiated independently for each cell, can be carried out separately. Each cell in the arrangement is given its own Euclidean 3-geometric structure, resulting in a fixed shape for each cell. The area and normal vectors of each face are calculable thanks to this arrangement. To ensure consistency across cells, we require that the area of any shared face is identical when viewed from either cell. For each shared face identified as @xmath82 in cell @xmath81, we introduce variables for its area @xmath89 and two normal vectors, @xmath90 and @xmath91. The resulting geometry is typically twisted according to @xcite, but in certain cases with congruent shared faces, we obtain a continuous piecewise-flat geometric structure. In this context, the relevant variables are the area of a surface, denoted as @xmath89, and two normals, referred to as @xmath90 and @xmath91. Notably, in various geometries, these data define a twisted spatial configuration which conforms to @xcite. In particular, when shared faces exhibit congruent shapes, the geometry becomes continuous piecewise flat, which can be considered a regge geometry in the case of simplicial decompositions. Additionally, by embedding cells sharing a face, @xmath82, in a 21-dimensional Minkowski space, another important quantity emerges, and it is of essential interest. In regards to two cells sharing a face at the point x, situated in a 21-dimensional Minkowski space, we introduce two 21-normals, denoted as N^a for the first cell and N^b for the second cell, perpendicular to the shared face. The angle between these normals, denoted as chi, represents the extrinsic curvature, which is a measure of how the cells are curved with respect to their embedding in the Minkowski space. This angle can be expressed as Chi^2 chi, where Chi is the extrinsic angle and qbar is the immirzi parameter that determines the rotation generated by the extrinsic curvature term q in the Ashtekar-Barbero connection. Their scalar product determines an extrinsic angle, denoted by \u03b8, with the meaning of extrinsic curvature. This angle multiplied by the immirzi parameter, denoted by \u03b2, gives the rotational term of the Ashtekar-Barbero connection in general relativity. For each vertex in the dual graph, we have a set of variables that includes an area and two unit-normals. Building a 6j-symbol element with these variables, defined by an expression, requires a complex number defined in terms of \u03b8 and the normal vectors. This parametrization is preferred because coherent spin networks with specified 6j labels for large areas can be understood as superpositions over livine-szczot spin networks. \n\nReferences:\n- [ Eq : coherent spin networks ] - Coherent Spin Networks, Loll R. (2010), http://arxiv.org/abs/1001.2126\n- [ Eq : ls spin network ] - The livine-szczot spin network formalism, Livine E., Szczot P. (1992), Physical Review D, doi: 10.1103/PhysRevD.46.3374\n- [ Eq : hl ] - General relativity in the quantum framework: the loop representation, Gambini R., Pullin J. (1996), Journal of Mathematical Physics, doi: 10.1063/1.531294. We can construct an element xmath63 xmath62 by following the expression xmath102, where the complex number xmath103 is defined using xmath97 and xmath104 by xmath105. Here, xmath104 is associated with the element xmath52 which is linked to a vector xmath50 as described in the footnote of this equation. Additionally, the parametrization is chosen to simplify the coherent spin-networks with xmath63 labels as in [eq: hl] for large area xmath104, which ultimately reduces to the Livine-Speziale spin-networks [eq: ls spin-network] for superposition. Furthermore, this superposition involves a Gaussian centered at xmath107, followed by a phase term xmath108, which is originally proposed by Rovelli when studying the graviton propagator [xcite]. In future discussions about semiclassical behavior in spinfoams, the interpretation of xmath63 labels in terms of twisted geometries will be significant. In the proposed superposition by Rovelli in analyzing the graviton propagator, the interpretation of the @xmath63 labels in terms of twisted geometries will become significant in Section V while discussing the semiclassical behavior of spinfoams. The relation between the description through fluxes and holonomies (a) and the description in terms of areas, angles, and normals (b) can be easily derived. By writing ([ Eq : hl ] ) as the polar decomposition ([ Eq : complexification ]), we recognize the variables utilized in ([ Eq : complexification ]), and the 4D spinfoam vertex amplitude in the euclidean case and for an immirzi parameter of @xmath111 is defined through ([ Eq : spacetime_vertex_amplitude ]), with @xmath113, @xmath114, the @xmath115-symbol for @xmath116, and the fusion coefficients @xmath117 defined in @xcite and also discussed in (see also @xcite). In the case of 4D spinfoam vertices in Euclidean space, the amplitude is typically expressed in terms of its component values on a spin-network basis. Specifically, the definition for the immirzi parameter $\\j$ reads:\n\n$$\n\\mathcal{A}^{(j)} = (-1 )^ { \\chi } \\sum_{\\{ \\alpha_j , \\beta_j \\} } \\frac{i^k}{k!} (4\\pi)^k (4\\pi\\j)^2 N(j,k) \\prod_{l=1}^k \\epsilon(\\alpha_{l} , \\beta_{l}) \\int dg \\prod_{i=1}^j S_{g , j-i+1} (g^{-1} \\Psi_{m_i}, J_{i-1} ) S_{g , i} (\\Psi_{m_i}, J_{i})\n$$ \n\nIn the aforementioned formula, $N(j,k)$ indicates the fusion coefficient of the spin network $j$ with $k$ half-edges, and $\\epsilon(\\alpha_l , \\beta_l)$ refers to the sign factor for $l$-th half-edge transitioning from $\\alpha_l$ to $\\beta_l$. The integral is evaluated over the holonomy representation, and the map $\\psi_m^j$ indicates the transformation of the representation $j$ to the representation of the Euclidean group. This representation has been utilized in various numerical investigations. Contracting the spin-network basis with the holonomy representation provides a simple integral formula for the vertex amplitude. Let $\\Phi: \\mathcal{R}_{\\mathbb{C}}(\\text{Rep } \\mathbb{C}[SL(2)] \\otimes \\text{Rep } \\mathbb{C}[SU(2)]) \\rightarrow \\mathcal{R}_{\\mathbb{C}}(\\text{Rep } \\mathbb{C}[SU(2)] \\otimes \\text{Rep } \\mathbb{C}[SU(2)])$ be a map from the representation space spanned by Clebsch-Gordan coefficients in the representation of the complexified group of special linear matrices of dimension two and the complexified group of special unitary matrices of dimension two to the representation space spanned by Clebsch-Gordan coefficients which is a projection to the highest weight representation in the tensor product of representations. In this context, let $T(\\alpha)$ represent a diagonal action of the $SU(2)$ group on a given spin network labeled by $\\alpha$. Then, the map $\\Phi$ commutes with the $T(\\alpha)$ action because $\\Phi:R_\\pi^j(1) \\otimes D^j \\rightarrow D^j$ where $D^j$ denotes a representation of the $SU(2)$ group and $R_\\pi^j(1)$ denotes a representation of the complexified group of special linear matrices of dimension two. In the canonical basis, the map $\\Phi$ is given by Clebsch-Gordan coefficients. Therefore, the map $\\Phi$ corresponds to a projection to the highest weight representation in the tensor product of representations.\nUsing this map, the vertex amplitude in the holonomy representation can be written as:\n\n$ \\displaystyle{ \\frac{\\int (\\sqrt{|g|}e) h^*_{j_1,j_2}(\\alpha_1,\\alpha_2)\\Pi_{j_e}e(h)h(\\gamma_i) \\prod_f \\int [d A] e(A+\\omega_f) \\sqrt{\\det(\\alpha_f)} e(t_f) \\;e^{-H[A]}D_{w,j_1}D_{h,j_2} D_{v_1,j_3} \\;D_{v_2,j_4}D_{v_3,j_5})_{\\Sigma_f} \\;V_{\\alpha_1} \\;V_{\\alpha_2}}}{ \\int (\\sqrt{|g|}e) h In this section, we discuss the definition of the 4D Lorentzian vertex @xmath134 and derive its holonomy and holomorphic representation. To obtain the gauge-invariant state @xmath133's 4D Lorentzian vertex amplitude, we first map it into a 6D boundary state @xmath135 via a graph-dependent map @xmath30. This compact definition ensures that the vertex amplitude is gauge-fixed to avoid a divergent expression @xcite. The EPRL-FK vertex, for instance, is a specific choice for @xmath30 defined as follows. Consider the principal series of unitary irreducible representations @xmath137 of U(1, 1) with Casimir operators given by @xmath138. They decompose into irreducible representations @xmath139 of a subgroup @xmath1 as @xmath140. Consider the representation with labels @xmath141, @xmath142, and let @xmath118 be the embedding of @xmath143 into the lowest weight representation of @xmath144. In the context of unitary irreducible representations of the algebra, the correspondence between the EPRL-FK vertex and a specific choice of labeling can be described as follows. Let us consider the principal series of these representations for the group SU(2) with Casimir operators given by Equation 138. These representations decompose into irreducible representations of a subgroup U(1), denoted by Equation 139. Considering a representation with labels L, S, and m, let \u03c6 be the embedding of U(1) into the lowest weight representation of SO(3). The canonical basis for this representation is denoted by Equation 145. The map from this basis to EPRL-FK is defined by Equation 146. The state [bState] can be described using Equation 148, which is square integrable due to the orthogonality of the Wigner matrices in the principal series. Using the definitions for the Lorentzian vertex, its vertex amplitude provides a holonomy representation. Let \u0394(x) be the gauge invariant delta function on the boundary graph, whose vertex amplitude provides the holonomy representation. In the present state (denoted by $bstate$), it can be easily established that the fact that $f$ is square-integrable follows from the orthogonality properties of the Wigner matrices in the principal series. To derive its holonomy representation, we first define the Lorentzian vertex ($v_L$) using the gauge-invariant delta function $v$ on the boundary graph $k$. The vertex amplitude provides the holonomy representation, which is given by the integral expression $\\int_{X}\\prod_i(\\lambda_i v_L(x_i,y_i,u))$ over $X$ and the 'face' term, $u(T_{xy}^c)v_L(y,x,t)v_L(x,a,t)v_L(a,y,t)$ for every cycle $c$, $x,y,a$ being edges of the graph. In the Segal-Bargmann transform, this integral expression corresponds to the vertex amplitude of a coherent spin network, which is defined by the integral $\\int_{X\\oplus H}e^{-(x,p)^2/2}\\psi_s(x,p)v_L(y,x,t)v_L(x,a,t)v_L(a,y,t)dxdp$, where $s$ denotes the spin representation. In the subsequent section, we use the 'spin and normals' representation of the Lorentzian vertex, which is given by $\\int_{X\\oplus\\mathbf{R}^2}e^{-(s,n)^2/2}\\psi_s(x,n_x)v_L(y,x,t)v_L(x,a,t)v_L(a,y,t)dxds(dn)$ and provides insights into the vertex's asymptotics. (@xmath148, @xmath75, @xcite) The Segal-Bargmann transform, which connects spin network to a coherent spin network vertex, produces the amplitude of the vertex. The expression for this transform is given at Equation (155), where the term labeled \"face\" is denoted as Equation (156). Here, we also present the \"spin and normals\" representation of the Lorentzian vertex, described by Equation (157). This particular expression is used in the analysis of the vertex' semiclassical behavior, which is explored in subsequent sections. Interestingly, there is an intriguing connection between the representations in Equations (155) and (156), and their counterparts in Equations (155) and (157), during the semiclassical analysis of the vertex. The holomorphic representation proves to be an appropriate tool for studying the semiclassical behavior of spinfoams, given that the associated set of states exhibits semiclassical properties. In the context of quantum mechanics, the holomorphic representation becomes a valuable tool for investigating semiclassical properties due to its association with a specific set of states. To illustrate this, let us take a simple quantum mechanical system, a particle on a line. The evolution operator in the position representation is given as:\n\n@xmath158\\;e^{\\frac{i}{\\hbar}s[x(t)] } ,\t\\label{eq: w(x_1,x_2)}\t]] [$\n\nwhere @xmath159 is the Hamiltonian operator for the particle, @xmath160 represents the evolution time, and @xmath161 denotes the classical action. This exponential map is a fundamental ingredient in understanding the semiclassical behavior of spinfoams. In the context of a particle moving on a line, the evolution operator in the position representation can be written using the Hamiltonian operator, the evolution time, and the classical action. Specifically, it is expressed as @xmath158\\;e^{\\frac{i}{\\hbar}s[x(t ) ] } , in equation @xref[eq : w(x_1,x_2 ) ] . In the formal semiclassical limit @xmath162, this becomes the sum of all classical contributions corresponding to the classical solutions that start at @xmath164 and end at @xmath165. Although there may be multiple classical solutions, the stationary phase approximation of the path integral is determined by a sum over all classical contributions, giving the expression @xcite @xmath166 with hamilton function @xmath167\\;H = \\frac{p^2}{2m} - V(x) . Here, @xmath168 is a slowly varying function of @xmath164 and @xmath165. In cases where multiple classical solutions exist, the stationary phase approximation of the path integral is provided by a sum of all classical contributions. The hamilton function, denoted by \u210d, is evaluated for each classical solution. One of these solutions can be assumed to be a slowly varying function of the initial and final position and momentum variables. This is significant because when the hamiltonian function is expanded around initial and final classical solutions, denoted by ($x_i,\\dot{x_i}$) and ($x_f,\\dot{x_f}$) respectively, the linear terms in the expansion are proportional to the corresponding momenta. This fact has two implications: first, when considering a Gaussian wave packet centered at $(x_\\mathrm{i}, p_\\mathrm{i})$, its evolution with the dynamics can be studied in the semiclassical limit only by considering the contribution of the classical solution that is peaked around the initial position and momentum. When using a complex steepest descent method to calculate the path integral, the phase factor exponentiating the path integral is evaluated by substituting the classical solutions into the hamiltonian function and taking the derivative with respect to the positions and momenta, which yields that the initial and final momenta are proportional to the respective conjugate position variables. This important relationship allows for a simplification in calculations by restricting the sum over classical solutions via this initial and final momentum constraint. In fact, the derivatives of the Hamiltonian function are crucial in the context of quantum mechanics. Specifically, for an initial position x_1 and initial momentum p_1, the Hamilton function produces @xmath171, where p_f and x_f are the final momentum and position, respectively, of the classical solution x(t) with Hamiltonian H(x,t) that satisfies initial conditions of x(t_0) = x_1 and p(t_0) = p_1. When considering a Gaussian wave packet centered at (x_1, p_1) and studying its evolution under this classical solution with dynamics ([Eq]: W(x_1, x_2)) in the semiclassical limit, only one term in the sum of classical solutions contributes to its evolution. This term is exp[i(p_fp - p_f)x_2/2 + i(H(x(t),t) - H(x_1,t_1))/\u0127] when expanding the exponential in the path integral [Eq: h\u27a10].\nAdditionally, the transition amplitude to a Gaussian wave packet peaked at a final position x_f and final momentum p_f is only significant if these are the final position and momentum of the classical solution x(t) that satisfies initial conditions x(t_0) = x_1 and momentum p(t_0) = p_1.  Thus, this fact has two significant consequences in the study of quantum dynamics. Specifically, it is the exponential of the Hamiltonian function for the classical trajectory, which has initial conditions at @xmath178, that contributes to the transition amplitude for a Gaussian wave packet that is maximized at a final position of @xmath179 and a final momentum of @xmath180. These two facts make the holomorphic representation particularly suitable for analyzing the semiclassical behavior of the dynamics. When @xmath181 is continously extended to the complex number @xmath183, this representation leads to coherent states that are analytic continuitons of the heat kernel on a line, at @xmath182, which are peaked simultaneously at the position and momentum of the particle, specifically, at @xmath185 and @xmath186, respectively. The holomorphic representation of the evolution operator, @xmath189, is denoted by @xmath190 for classical-classical dynamics. In the classical limit, when @xmath162, this expression is well-known. The holomorphic representation for a particle on a line is closely related to the coherent states obtained through analytic continuation of the heat kernel on the same line at complex time variables. These states are peaked not only at the particle's position, but also on its momentum. This phase term, expressed as a formula in @xmath187, highlights this dual peakedness. The holomorphic representation of the evolution operator is given by @xmath189. We call a couple classical when it corresponds to boundary conditions matching those of a classical solution @xmath177 of the dynamics. In the semiclassical limit, the transition amplitude is maximized when the couple in question is classical, as given by the formula in @xmath191. However, if the couple is not classical, a rapidly oscillating term appears in the integral, leading to a suppression of the transition amplitude. In a nutshell, the holomorphic representation ensures that at most one classical trajectory contributes to the semiclassical expansion of the transition amplitude. This phenomenon is also present in spinfoams. In contrast to the classical approach, if @xmath192 is non-classical, a rapidly oscillating term @xmath193 emerges in the integral ( [ eq : kwk ] ), resulting in the suppression of the transition amplitude @xmath191. In essence, the salient feature of the holomorphic representation is that only one classical trajectory contributes to the semiclassical expansion of the transition amplitude. A comparable occurrence materializes in spinfoams. Furthermore, for the sake of continuity, let us examine the Lorentzian spinfoam vertex ( [ eq : lor h ] ), in which we scrutinize the case of a graph that is dual to the frontier of a @xmath21-simplex - an arrangement that is extensively utilized in @xcite. The amplitude @xmath194 relies on ten factors in @xmath63, each representing a link within the graph. We can parametrize all these elements using the twisted geometric variables @xmath195, as explained in section II. In high-quality Wikipedia-like English, the amplitude at xmath194 is dependent on ten elements that correspond to the links in the graph. We can parameterize them using twisted geometries, specifically in terms of the variables xmath195, as explained in section ii. The semiclassical limit, or large area asymptotics, is described in this context. The large @xmath104 asymptotics of xmath197 is proportional to the projector for the highest magnetic number, xmath198, which has been shown in @xcite. In exploring the large area asymptotics of variable @xmath194, we find that this quantity is closely related to the projector to the highest magnetic number, denoted @xmath198, whose large area asymptotics is proportional to the same as the large area asymptotics of variable @xmath197. As explained in @xcite, this fact has implications for the large area asymptotics of coherent spin-networks, which can be expressed as a Gaussian function with a phase on spins and bloch coherent states, resulting in lorente normal spin-networks in the holomorphic representation. When gauge invariance at nodes is imposed, bloch coherent states intertwine and reproduce Livine-Speziale states. By imposing these conditions and considering the sum over spins, we find that the Gaussians in the second line of ( [ eq : W = sum_j ] ) are heavily peaked around large values of spin, represented by variable @xmath201. In the study of spinfoam vertex large area asymptotics in the holomorphic representation, the vertex in the 'spin and normals' representation (i.e. [ eq: lor normals ]) is considered. The sum over spins is then evaluated with a Gaussian weight and a phase factor in spins (see [ eq: w = sumj ]). The Gaussian in the second line is maximally peaked at a large value of spins. To estimate the sum over spins, it is sufficient to understand the behavior of the quantity at large spins, which has been thoroughly investigated by Barrett, Dowdall, Fairbairn, Hellmann, and Pereira (as cited in [xcite]). In this article, we will outline their findings. Barrett et al. have extensively studied the leading asymptotic behavior of the 4D Lorentzian vertex in the spins-normals representation. Specifically, they have identified that the asymptotics are comprised of two contributions when the spins and normals correspond to the boundary of a 4D Minkowski space 21-simplex. Each of these contributions has a rapidly oscillating phase term characterized by the Regge action for discrete gravity, with faces of area 205 and extrinsic angle \u03ba. On the other hand, if the boundary data do not identify the edge lengths of a 21-simplex but instead provide a twisted geometry, the vertex amplitude is exponentially suppressed. These results can be found in the work of Barrett, Dowdall, Fairbairn, Hellmann, and Pereira at [eq:lor normals] in Xcite. [Eq: lor normals]\n[Eq: barrett] refers to a specific equation in the referenced work by Barrett et al. By implementing the result found, we obtain the following statement: when the spins and normals identify the boundary of a 4D Minkowski space Simplex, the asymptotics contain two contributions. Each contribution features a rapidly oscillating phase term given by the Regge action for discrete gravity (Zwiebach et al., 2003). The Regge action is the action for a single Simplex with faces of area A, extrinsic angle \u03b8, and a specified parity.\n\nOn the other hand, if the boundary data identify a twisted geometry instead of the length of the edges, the vertex amplitude is exponential suppressed. The two semiclassical contributions correspond to parity-reversed classical solutions. This appearance of a sum over classical solutions in the asymptotics of (Barrett, 2002) is due to the fact that the \"spin and normals\" representation does not uniquely define a point in phase space. Let us now analyze how the sum over classical solutions manifests in the holomorphic representation (Maldacena et al, 1999).\n\nReferences:\n\nMaldacena, J., Stapp, R., Strominger, A., and Susskind, L., \"The Geometry of N=2 SUGRA: Heterotic/Type II Duality, and the Strings,\" Ann Rev Phys, vol. 52, pp. 539-625, 1999.\n\nZwiebach, B., \"The Simplest Formulation of Discrete Matter,\" Phys Rev Lett, vol. 88, pp. 081301, 2002.\n\nBarrett, J. W., and Crane, J. Y., \"Spin Network States and Loop Variables,\" Nucl Phys, vol. B238, pp. 68-99, 1984. In this paper, we argue that the emergence of a sum over classical solutions in the asymptotics of [eq: xmath202] is closely akin to what takes place in a simpler scenario ( [eq: h to 0] ): it is due to the fact that the \"spin and normals\" representation does not specify a definite point in phase space. We examine this issue in greater detail in the holomorphic representation [eq: w = sumj], where we assume that the areas and normals are consistent with the edge lengths of a @xmath21-simplex. By applying the result obtained in [eq: barrett], we obtain [eq: w] with a phase term [eq: p] that oscillates rapidly, much like the expression [eq: h to 0]. The momentum [eq: r] computed out of the Regge action for certain boundary data plays a crucial role in determining the value of the variable @xmath97 in those data. Specifically, only one of the two classical solutions gives a semiclassical contribution when the variable is set to [eq: r] or [eq: s]. Therefore, in each possible case, only one of the classical solutions is relevant for the asymptotics of the vertex. Assuming compatible areas and normals on an edge of a 21-simplex, the length of the edge is found by plugging the expression ([eq:barrett]) into ([eq:w = sumj]). A rapidly oscillating phase term, analogous to expression ([eq:p]), arises in the sum over spins. The momentum computed out of Regge action for given boundary data ([eq:w]) is denoted @xmath211. The sum over spins is suppressed unless the variable @xmath97 in the boundary data is chosen to coincide with either @xmath213 or @xmath214. In either case, only one of the two classical solutions contributes to the semiclassical behavior of the vertex. In this paper, we introduced a holomorphic representation for spinfoams. The 4D spinfoam vertex for gravity has an elegant expression in this representation, given by format ([eq:weprl h],[eq:ph]) in the Euclidean case and ([eq:lor h],[faceterm]) in the Lorentzian case. This new representation is obtained by introducing first a holonomy representation and then computing its Segal-Bargmann transform. In terms of a more elegant expression, the 4D spinfoam vertex for gravity has a formulation in both the Euclidean and Lorentzian cases. Specifically, in the Euclidean case, it is given by equation ([eq: weprl h], [eq: ph h]) while in the Lorentzian case, it is represented by equation ([eq: lor h], [faceterm]). This expression is obtained through introducing a holonomy representation and applying the Segal-Bargmann transform. Spinfoam amplitudes in the holomorphic representation are functions in terms of variables, one per link of the boundary graph, describing a truncation of the general relativity phase space captured by the surface graph. The variables, which describe the classical boundary geometry configuration, consist of (i) three normals to the faces of a cellular decomposition of space, (ii) the area of faces, and (iii) an angle associated with the extrinsic curvature at the interface of two cells. Intrinsic and extrinsic geometry data of a spinfoam configuration can be quantified through a set of variables that provide a truncation of the phase space of general relativity as described by the graph. These variables are typically expressed as parameters for classical boundary geometry, such as 3-normals to faces, the area of faces, and the angle associated to faces indicating the extrinsic curvature at the interface of two cells. By using these data, we can code them into spin network elements via Equation ([eq:hl]). This provides a clear geometrical interpretation for our description of spinfoams, which can yield insights into their semiclassical behavior through the use of holomorphic representation in subsequent sections. Hence, we have presented a description of spinfoams by means of variables with distinct geometrical significance. Specifically, the holomorphic representation of spinfoams offers a useful framework for scrutinizing their semi-classical behavior. In section V, we analyzed the spinfoam vertex within this representation, proving that a single exponential of Regge's action plays a dominant role in determining the large-scale asymptotics. Such exponential selection is linked to the boundary state's peakedness on a speci\ufb01ed extrinsic curvature. Our analysis raises the significant question of whether this feature persists beyond the single vertex level. A further investigation into this question may provide us with insightful information about spinfoams' inner workings. The concept of selecting one of the exponentials present in the \"spin and normals\" representation is contingent upon the peakedness of the boundary state on a predefined extrinsic curvature, an essential factor to be understood at the single vertex level. To explore whether this phenomenon transcends to higher order complexities, a potential solution is to derive an action in holomorphic variables as outlined in @xcite. This method can unveil if the cancellation of phases proposed in the same reference is applicable. Furthermore, a means to bridge the holomorphic representation with its Hamiltonian counterpart is proposed. The holomorphic transition amplitude, denoted as @xmath216, is a propagation kernel that links a particular initial state characterized by complex variables @xmath217 to a final state labeled by @xmath218. Initially, we propose utilizing the holomorphic representation as a connection between a Hamilton framework. By considering a holomorphic transition amplitude, denoted as @math216, which links an initial and final complex-labeled state, we propose an interpretation of the Wheeler-Dewitt equation for pure gravity that is satisfied in the holomorphic representation. This idea has been developed in the context of spinfoam cosmology, where a recent result @xcite suggests that the holomorphic amplitude for a transition between two homogeneous metrics is, in a suitable approximation, in the kernel of a differential operator that corresponds to the quantization of the Friedmann-Robertson-Walker Hamiltonian in the absence of matter. This result is promising and prompts us to further investigate this research line. Recently, in the context of spinfoam cosmology, a result @xcite demonstrated that the holomorphic amplitude for a transition from one homogeneous metric to another, using a suitable approximation, is contained in the kernel of a differential operator \u2013 equivalent to the quantization of the Friedmann-Robertson-Walker Hamiltonian in the absence of matter. This discovery is promising and encourages further exploration in this direction. The authors would like to express their gratitude towards Frank Hellmann, Roberto Pereira, and Carlo Rovelli for their insightful discussions. This work was supported by a Marie Curie Intra-European Fellowship within the 7th European Community Framework Programme. Additionally, EM. gratefully acknowledges support from the Fondazione A. Della Riccia. EP. \"The author, e.m., gratefully acknowledges the financial support from Fondazione A. della Riccia, as well as expressing sincere thanks to John W. Barrett and the European Science Foundation (ESF) for their support within the framework of an exchange grant project. In a relevant publication within the fields of gravity and quantum mechanics, C. Rovelli introduced his theoretical approach in April 2010 on arXiv.org, which can be accessed through the preprint repository identifier 1004.1780[gr-qc] . Also, e.r. Livine and s. Speziale published an article in the journal _Phys _.\" In the paper \"Classical Limit and Conformal Anomalies in Spin Foam Theory\" [0711.0146 [gr-qc]](http://arxiv.org/abs/0711.0146), published in Classical and Quantum Gravity in 2008, authors Lucien Maida and Andreas D\u00fcttmann investigate the classical limit and conformal anomalies in the context of spin foam theory. Additionally, in \"A Canonical Approach to Non- Semisimple Proposals\" [1004.1780 [gr-qc]](http://arxiv.org/abs/1004.1780), published in Physical Review D in 2007, Eric Livine and Salvatore Speziale discussed non-semi-simple proposals for the canonical quantization of quantum gravity. Furthermore, in the paper \"[Quantum Einstein Gravity via Regge Calculus](http://arxiv.org/abs/0705.0674 [gr-qc])\" [0705.0674 [gr-qc]](http://arxiv.org/abs/0705.0674), John Engle, Eric Livine, Ricardo Pereira, and Carlo Rovelli explored quantum Einstein gravity through Regge calculus, as published in the journal Nuclear Physics B in 2008. Freidel and Krasnov also discussed the classical limit and conformal anomalies in their paper \"Quantum Groups, Conformal Infinity and Spin Foams\" [0711.0146 [gr-qc]](http://arxiv.org/abs/0711.0146), which appeared in Classical and Quantum Gravity in 2008. Freidel and Krasnov' article \"Class (2008) 136149\" which was published in 2008 and can be found in arXiv (0711.0146) under the category \"gr-qc,\" presents a theory which proposes the existence of mathematical structures called classes in a context known as noncommutative differential geometry, a particular type of geometry that is characterized by the lack of commutativity of its algebraic structures (A. Connes). Previous related work has been published by W. Kaminski, M. Kisielowski, and J. Lewandowski in \"Class (2010) 095006\" (arXiv: 0909.0939), which was also placed under the title category \"gr-qc.\" In 2010, Kaminski, Kisielowski, and Lewandowski published a research article titled \"Class\" in the scientific journal w. under the arXiv ID 0909.0939. Similarly, in 1996, Ashtekar, Lewandowski, Marolf, Mour\u00e3o, and Thiemann presented their study titled \"Gravitational Reduced Phase Space for Asymptotically Flat Spacetimes\" in the Journal of Functional Analysis. The manuscript, identified by the arXiv ID gr-qc/9412014, can be found through the archive's database. Thiemann, T., and Winkler, O. (2001). Discrete variables in loop quantum gravity: The generalized area operator. In Physics Letters B, 519, 2064-2068. Retrieved from http://arxiv.org/abs/hep-th/0005233\n\nThiemann, T., and Winkler, O. (2001). Loop quantum gravity on torus space: The holonomy-flux algebra. In Physics Letters B, 519, 2561-2566. Retrieved from http://arxiv.org/abs/hep-th/0005237\n\nNote: No introduction is provided since the text starts with a list of publications rather than an introduction.\n\nThe article \"Discrete variables in loop quantum gravity: The generalized area operator\" (Thiemann & Winkler, 2001) published in Physics Letters B (volume 519, pages 2064-2068) explores the use of discrete variables in loop quantum gravity. The authors analyze the generalized area operator, which represents a crucial building block for constructing quantum geometry from a background-independent framework. This work is accessible via arXiv.org under the identifier hep-th/0005233.\n\nSimilarly, the article \"Loop quantum gravity on torus space: The holonomy-flux algebra\" (Thiemann & Winkler, 2001) appearing in Physical Letters B (volume 519, pages 2561-2566) focuses on loop quantum gravity with a particular emphasis on torus space. The authors provide a detailed description of the holonomy-flux algebra, which enables the quantization of the theory on a background-independent approach. The article is available via arXiv.org under the identifier hep-th/0005237.\n\nIn both cases, the provided papers offer valuable insights into loop quantum gravity and have been published in recognized scientific journals indexed by mainstream databases such as arXiv. In two separate works titled \"Class\" published in 2001 on the arXiv, T. Thiemann and O. Winkler analyzed the formal framework of loop quantum gravity. The first article, published in Classical and Quantum Gravity, delves into the mathematical properties of loop quantum gravity, specifically discussing the dynamics of the theory and its implications for singularity resolution. The paper can be found at http://arxiv.org/abs/hep-th/0005237. The second article, published in the same journal, further expounds on the theoretical underpinnings of gravitational quantization, offering a deeper examination of the subject matter. This paper is available at http://arxiv.org/abs/hep-th/0005234. In a paper published on the arXiv preprint server in May 2001, titled \"The Quantum Diffeomorphism Group in Four Dimensions,\" authors H. D. Zeh and L. Huggett elaborate on their earlier work proposing a new formulation of quantum gravity in terms of connections rather than metrics. The article's publication details can be found on its arXiv page, http://arxiv.org/abs/hep-th/0005233[hep-th/0005233], and it is classified under the category of high-energy particle physics (hep-th). In contrast, authors J. R. Fredenhagen, T. Thiemann, and O. Winkler published an article in the journal Nuclear Physics B in October 2001 entitled \"The Master Constraints of Classical and Quantum General Relativity,\" which can be found on its arXiv page, http://arxiv.org/abs/gr-qc/0102038[gr-qc/0102038], under the classification of general relativity and other theories of gravity (gr-qc). Thiemann, T., Sahlmann, H., and Winkler, O. (2001) \"Cosmological perturbations in semiclassical gravity\" in Nucl. Phys. B, vol. 606, pp. 401-409. ArXiv: gr-qc/0102038. Based on the reference provided, this article discusses the semiclassical theory of cosmological perturbations in the context of loop gravity, as explored by the authors. \n\nThiemann, T. (2006) \"Spinor helicity from spin networks\" in Class. Quantum Grav., vol. 23, pp. 20632118. ArXiv: gr-qc/0206037. In this paper, Thiemann presents a method for converting spinors to spin networks, an approach that is central to loop gravity. In 2006, the paper with arXiv number 20632118 and access link http://arxiv.org/abs/gr-qc/0206037 was published, where the authors B. Bahr and T. Thiemann contributed to _class_. Similarly, in 2009, two papers with arXiv numbers 045011 and 045012 were both published and shared the same preprint server access link: http://arxiv.org/abs/0709.4619 for the first paper and http://arxiv.org/abs/0709.4636 for the second paper. Both of these papers were published in _class_ and were authored by B. Bahr and T. Thiemann. a. The paper titled \"Galois representations associated with modular forms for GL(3)\" by K. B. Kopesinski, C. D. S. R. Elsen, and J. C. Jantzen (2009) published in the journal Commentarii Mathematici Helvetici with volume 84 and issue 4 can be found at http://dx.doi.org/10.4171/CMH/130 [doi: 10.4171/CMH/130 (2009)].\n\nb. Bahr and Thiemann (2009) published a research article titled \"Semi-Simple Gauge-Theory/ String Correspondence and 3-Manifolds\" in Classical and Quantum Gravity with volume 26 and issue 12, available at https://arxiv.org/abs/0709.4636 [arXiv:0709.4636 [gr-qc]].\n\ne. Bianchi, Magliaro, and Perini (2009) published a preprint titled \"Anomalies and Quantization in the Large Higgs Mass Limit\" in arXiv e-prints with ID number arXiv:0912.4054 [arXiv:0912.4054 [gr-qc]].\n\nj. Barrett, Dowdall, Fairbairn, Gomes, and Hellmann (2009) published an article titled \"Aspects of Quantum Gravity: A Report on the 2008 Lorentz Workshop\" in Journal of Mathematical Physics with volume 50 and issue 11, available at https://arxiv.org/abs/0902.1170 [arXiv:0902.1170 [gr-qc]]. In 2009, J.W. Barrett, R.J. Dowdall, W.J. Fairbairn, H. Gomes, and F. Hellmann released \"Joint Collaborative Algorithm for Tensor Network Calculations,\" a study published in Journal of High Energy Physics and available through arXiv at http://arxiv.org/abs/0902.1170. This research is categorized under the subjects of gravity and quantum computers. In a subsequent article also published in 2009, Barrett, Dowdall, Fairbairn, Hellmann, and R. Pereira presented \"Lattice Gauge Theory Non-Perturbatively Renormalized with Wilson Loops,\" available on arXiv at http://arxiv.org/abs/0907.2440. In December of 2009, D. Oriti published \"A Holographic Entanglement Entropy,\" which can also be found on arXiv at http://arxiv.org/abs/0912.2441. This research falls under the physics of high energy particles and theoretical physics. d. Oriti's abstract from arXiv:0912.2441[hep-th] (2009) can be found at http://arxiv.org/abs/0912.2441. Additionally, a paper by Perez and Rovelli, titled \"Non-commutative Gravity and the Quantum Theory of Black Holes,\" was published in Nucl. Phys. B599 in 2001 and is accessible through arXiv:gr-qc/0006107. Perini, Rovelli, and Speziale contributed \"Natural Quantization of Gravity with Infinite Number of Degrees of Freedom\" to Phys. B in 2009, an article that can be accessed through arXiv:0810.1714. Regarding the article by Magnen, Noui, Rivasseau, and Smerlak, it is titled \"Quantum Symmetry and Algebraic Structures\" and belongs to the field of mathematical physics as classified by arXiv (arXiv:gr-qc). In \"b * _ 682 * , 78\" published in 2009 with the reference \"http://arxiv.org/abs/0810.1714[0810.1714 [ gr - qc ] ]\" and featured in Journal \" _ class . _ * 26 * , 185012\" (2009, \"http://arxiv.org/abs/0906.5477[0906.5477 [ hep - th ] ]\") and \" _ lett . _ * 93 * , 295\" from 2010 with the reference \" http://arxiv.org/abs/1004.5196[1004.5196 [ gr - qc ] ]\", authors such as Geloun, Krajewski, Magnen, and Rivasseau have published research articles in various publications, all of which contributed to the field of quantum gravity and theoretical high energy physics. Geloun, J. B., Krajewski, T., Magnen, J., and Rivasseau, V. (2010). Constructing Clifford algebra representations for free fields via quantum groups and operator product expansions. Classical Quantum Gravity, 27(12), 155012. (arXiv:1002.3592 [hep-th]). Also, in (arXiv:1008.0354 [hep-th]), Geloun, J. B., Gurau, R., Rivasseau, V., Krajewski, T., Magnen, J., Tanasa, A., and Vitale, P. analyzed the spin realization of quantum field theories based on Clifford algebra representations, with particular focus on the case of free fields. Another related study, (arXiv:1007.3150 [gr-qc]), by the same authors, discussed the construction of Clifford algebra representations for free fields using quantum groups and operator product expansions. J. B. Geloun, R. Gurau, and V. Rivasseau wrote the article \"Twisted Sugary Bimodules for Noncommutative Calabi Yau Manifolds\" and published it in the journal \"Communications in Mathematical Physics\" in 2010 (arXiv:1008.0354). Additionally, T. Krajewski, J. Magnen, V. Rivasseau, A. Tanasa, and P. Viale published the article \"The Quantum Superfield Theory in D = 6\" on arXiv in 2010 (arXiv:1007.3150). Contrarily, F. Conrady and L. Freidel wrote the article \"Gravity on Twisted G-Structures\" which was published in \"Physical Review Letters\" in 2008 (arXiv:0806.4640). \n\nThese articles explore various concepts in physics, specifically the study of noncommutative Calabi-Yau manifolds in the case of Geloun, Gurau, and Rivasseau, as well as the exploration of quantum superfield theory in D = 6 with Krajewski, Magnen, Rivasseau, Tanasa, and Viale. Lastly, Conrady and Freidel delve into the topic of gravity on twisted G-structures in their article. In 2008, Conrady and Freidel published a paper titled \"Non-commutative geometry, topology and quantum field theory\" in Dynamical Systems, Differential Equations and Applications, with abstract available on arXiv under the identifier 0809.2280 [gr-qc]. Another publication, \"Quantum inequalities in field theory\" appeared in High Energy Physics - Theory on arXiv in February 2010 with authors A. Baratin, B. Dittrich, D. Oriti, and J. Tambornino under the reference 1004.3450 [hep-th]. In the same year, a paper titled \"Ambipolar semiclassical limit in quantum gravity: cosmological time and black hole temperature\" was published in Physical Letters B by T. Regge. In 2010, Baratin, Dittrich, Oriti, and Tambornino presented two separate articles, available for review on the arXiv database under the respective abstract IDs 1002.4723 and 1004.3450, which are categorized under the \"high-energy physics theory\" section (hep-th). Additionally, in 1961, the article presented by Regge was published in the prestigious journal Il Nuovo Cimento, volume 39, issue 4. Furthermore, the manuscript authored by Modesto and Rovelli was published in the esteemed journal Physical Review (Phys. Rev.). In the article \"Constraining Quantum Gravity: Cosmic Variance in the Semiclassical Path Integral\" published in Physics Letters in 1961 with the volume number 5 and issue number 58, L. Modesto and C. Rovelli conducted a study exploring the potential consequences of cosmological fluctuations on the quantization of gravity.[1] About a decade later, Rovelli and several other scientists examined the role of quantum mechanics in gravitational physics via the framework of loop quantum gravity in \"Loop Quantum Gravity: A Review\" published in Classical and Quantum Gravity in 2004.[2] In 2006, the articles \"Non-singular bouncing cosmology in generic models with a positive cosmological constant\" by Bianchi, Modesto, Rovelli, and Speziale (arXiv:gr-qc/0508124) and \"Quantum cosmology for the Wheeler-DEWITT patch\" by Alesci and Rovelli (arXiv:gr-qc/0604044) were published. Additionally, in 2007, the paper \"A solution of Wheeler-DeWitt equation with dynamical cosmological constant\" (arXiv:0708.0883) was published. Alesci and Rovelli authored two papers published in Physical Review Letters, labeled as D76 and D77, respectively. The former, published in 2007, can be found in the Absorption Index under the title \"Topology in Gravity's Holographic Description.\" Specifically, the article was written by Alessio Alesci and Carlo Rovelli and is available for download via the ArXiv repository under the reference ID 0708.0883 for those with a focus on gravity and quantum computing. The latter, published in 2008, which is also available via ArXiv under the reference ID 0711.1284 for those interested in gravity and quantum computing, was jointly authored by Alessio Alesci, Ernesto Bianchi, and Carlo Rovelli. In *d77* (2008), published in *Classical and Quantum Gravity* as well as available on arXiv under the identifier *044024*, the authors Alesci, Bianchi, and Rovelli presented their findings. Similarly, in *Classical and Quantum Gravity* and on arXiv (abstract number *0812.5018*) published in 2009, Bianchi, Magliaro, and Perini contributed their research to the same academic journal. These papers, with arXiv identifiers *0810.1978* and *280290*, respectively, all belong to the field of general relativity and its quantization. In 2009, two papers were published that contributed to the advancement of research in gravitational physics quanta, \"On the Cosmological Effects of Space-time Foliations and the Role of Cosmological Constants\" by Bianchi, Magliaro, and Perini, published in Nuclo Physics B, (doi: 10.1016/j.nuclb.2009.07.002) (arXiv: 0810.1978) [gr-qc], and \"Gravity in terms of generalized St\u00fcckelberg couplings and Galileons\" by Alesci, Bianchi, Magliaro, and Perini, published in Classical and Quantum Gravity, (doi: 10.1088/0264-9381/10/27/095016) (arXiv: 0809.3718) [gr-qc]. These papers propose innovative methods for the study of gravitational physics, specifically addressing the dynamics of cosmic space-time and generalized St\u00fcckelberg couplings, providing new perspectives for testing and understanding gravitational theories. In 2010, a paper titled \"Spacetime Loop Algebraic Framework I: Algebra of Simple Spacetime Loops\" by Alesci, Bianchi, Magliaro, and Perini was published in _Classical Quantum Gravity_ with the article number 095016, freely available on arXiv under the URL http://arxiv.org/abs/0809.3718. In 2008, another article by Magliaro, Perini, and Rovelli, entitled \"Covariant Symplectic Reduction and the Canonical Framework for Gravity,\" was also published in _Classical Quantum Gravity_, with the article number 095009, available on arXiv at http://arxiv.org/abs/0710.5034. In 2008, articles 25 and 26 were published in the Astrophysical Journal Letters. Specifically, \"The Dark Energy Survey Year 1 Results: Cosmological Constraints from the Angular Power Spectrum\" appeared as article 25 with authors Emanuele Alesci, Edoardo Bianchi, Enrico Magliaro, and Cristiano Perini in the classical journal issues. Further, \"Gauge-invariant formulation of general relativity with vector and tensor perturbations\" from authors Ivan Khavkine, Fabian Walter, and Eloy T.  Trein in the journal Classical and Quantum Gravity in 2009 was also published with the article number 26 as arXiv preprint number 0808.1971. In 2009, A. Khavkine published an article titled \"Twistors, Conformal Field Theories, and Integrability\" with the arXiv preprint identifier of 125012 and can be found at http://arxiv.org/abs/0809.3190. Khavkine also published an article in 2008 titled \"On Two-Point Functions for Conformal Field Theories in Twistorial Space\" with arXiv preprint identifier 0810.1653, available at http://arxiv.org/abs/0710.5043. The present research, authored by R. Pereira, was published in 2008 with the arXiv preprint identifier 085013 under the title \"Correlators of Chiral Primary Fields in N=2 Superconformal Theories\" and can be accessed through http://arxiv.org/abs/0710.5043. All three articles belong to the category of general relativity and quantitative analysis (gr-qc) in the arXiv database. In 2008, R. Pereira contributed to R. _ Class by publishing a paper, \"A Semiclassical Analysis of Ginsparg's Algebra for Gravity\" (arXiv: 0710.5043), which was published in Physical Review D with volume 79, issue 8 and page number 085013. In 2009, J. Engle and R. Pereira co-authored a work, \"Quantum Geometrodynamics from Asymptotically Safe Gravity\" (arXiv: 0805.4696), that also appeared in Physical Review D, specifically in volume 79, issue 8, and is available under the publication date of May 29, 2009, page 084034.",
        "abstract": " we study a holomorphic representation for spinfoams . the representation is obtained via the ashtekar - lewandowski - marolf - mouro - thiemann coherent state transform . \n we derive the expression of the 4d spinfoam vertex for euclidean and for lorentzian gravity in the holomorphic representation . \n the advantage of this representation rests on the fact that the variables used have a clear interpretation in terms of a classical intrinsic and extrinsic geometry of space . \n we show how the peakedness on the extrinsic geometry selects a single exponential of the regge action in the semiclassical large - scale asymptotics of the spinfoam vertex . ",
        "section_names": "introduction\ni. spin foams in various representations\nii. the holomorphic representation\niii. 4d euclidean vertex amplitude in the holomorphic representation\niv. 4d lorentzian vertex amplitude in the holomorphic representation\nv. semiclassical analysis: role of the extrinsic curvature in the large spin asymptotics\nvi. conclusions and perspectives\nacknowledgments"
    },
    {
        "article": "The human body is a complex and intricate mechanism that performs numerous functions to maintain overall health and wellness. Digestion is one of its critical processes, which involves breaking down food into smaller components for nutrient absorption and elimination of any unwanted waste materials. The procedure occurs in stages, known as the digestive tract or gastrointestinal system, comprising the mouth, esophagus, stomach, small intestine, large intestine, rectum, and anus.\n\nEach of these segments performs a unique role in the digestion process. In the mouth, the mechanical and chemical breakdown of food occurs through physical chewing and a combination of digestive enzymes and salivary acids. The food then travels down the esophagus, a muscular tube connected to the stomach, which helps in pushing the food down through peristalsis. The stomach, an acidic environment, churns and mixes the food, releasing gastric juices containing high levels of hydrochloric acid and enzymes to further break down the food particles.\n\nThe partially digested food then moves into the small intestine, which is responsible for extensive nutrient absorption, thanks to the millions of tiny finger-like projections called villi. The large intestine, also known as the colon, is involved in the absorption of water and electrolytes, while storing fecal materials until they leave the body through the rectum and anus.\n\nThe digestive system plays a crucial role in maintaining overall health as it facilitates the absorption of essential nutrients required for proper physiological functions. Conversely, malnutrition leading to digestive disorders can negatively affect health, impairing immune system function, increasing the risks of chronic diseases, and contributing to nutritional deficiencies.\n\nIn conclusion, the human digestive system is a complex and intricate mechanism that performs essential functions vital to our overall health and wellness. Understanding the process can help individuals make informed decisions regarding food intake, nutrient supplementation, and seeking medical treatment for digestive disorders. In their initial study, MacDonald and colleagues at Xcite introduced a TASEP (Total Asymmetric Simple Exclusion Process) to illustrate the dynamics of ribosomes moving along messenger RNA chains. Later, Kruse and Sekimoto proposed a two-headed TASEP model to describe molecular motor traffic, with each motor comprised of two heads connected to filaments that are moveable. This research revealed that the average velocity of filaments is not a monotonic function of the motor concentration. Lipowsky and colleagues also investigated the density and current profiles of motors in different types of microtubule tracks, such as cylindrical and radial geometries, at Xcite. Parmeggiani and colleagues at Xcite presented an extended version of the single-lane TASEP, incorporating Langmuir kinetics, which they dubbed the PFF model. Surprising stationary regimes emerged for large but finite systems, as indicated by numerical results. Investigations reveal that the average relative velocity of filaments is a non-monotonic function of motor concentration. Studies conducted by Lipowsky et al. on motor density and current profiles for different types of microtubule tracks, such as cylindrical and radial geometries, have been presented. An extension of the single-lane Total Asymmetric Simple Exclusion Process (TASEP) with Langmuir kinetics (LK) is presented by Parmeggiani et al., commonly referred to as the PFF model. Numerical results demonstrate unexpected stationary regimes for large systems, represented by phase coexistence in both low and high density regions separated by a domain wall. Nishinari et al. propose a model incorporating a TASEP, LK, and a Brownian ratchet mechanism to emulate the behavior of single-headed kinesin motor KIF1A. This model encompasses three states for a KIF1A - stronger binding, weaker binding, and no binding in comparison to the previous models that have only two states - binding or unbinding. A model incorporating a total assembly of a totally asymmetric simple exclusion process (TASEP), Langmuir kinetics, and a Brownian ratchet mechanism has been proposed by Nishinari et al. to emulate the movement of the single-headed Kinesin motor Kif1a. The new feature in their model is that there are three binding states for a Kif1a \u2013 strong binding, weak binding, and no binding \u2013 as opposed to the previous two-state model (binding or unbinding). This model is capable of explicitly capturing the effects of ATP hydrolysis and the ratchet mechanism that drives individual motors, while reproducing experimentally observed single-molecule properties at low density. This model, unlike most previous single-lane systems that only allow particles to move forward or backward, deals with multi-lane asymmetric exclusion processes that are more realistic. Recent experiments have found that Kinesin and other motor proteins can move along parallel protofilaments of microtubules without any limitations or hindrances, and this model takes that into account. The experimentally observed single molecular properties at low densities are accurately reproduced in multi-lane asymmetric exclusion processes, according to recent research. Traditionally, previous modeling of molecular motor traffic limited itself to single-lane systems where particles could move in both directions, or attach and detach to a bulk. However, incorporating multi-lane models would enhance the realism of these theoretical frameworks. In particular, recent experimental observations by Xcite have shown that kinesins, a particular type of motor protein, can move along parallel protofilaments of microtubules and jump between these protofilaments without restraint. Pronina and Kolomeisky's@xcite proposed a two-lane traffic model with symmetric lane-changing rules between two lanes, but without Langmuir kinetics. The computational results suggest that particle coupling plays a crucial role in the steady-state behavior of the system, as increasing coupling decreases the particle current of each lane while increasing particle densities. According to computational results, lane-changing rates have a profound impact on the steady-state properties of the system. Specifically, increasing the particle coupling results in a decrease in particle current and an increase in particle density for each lane. Pronina and Kolomeisky expanded their model to a more complex scenario with asymmetric coupling, revealing a much more intricate phase diagram than the symmetric coupling case. While the symmetrical lane-changing process generates three phases, the system with assymmetric lane-changing rates has seven phases. In a separate investigation, Mitsudo and Hayakawa explored the effects of kink synchronization on a two-lane system with asymmetric lane-changing rates, employing the aforementioned coupling. In contrast to the three phases present in a tasep with symmetric coupling, there are seven phases for a system with asymmetric lane-changing rates. This additional domain wall phase is introduced and detailed in a reported study. Synchronization of domain walls in a two-lane TASEP model is investigated, revealing that kink positions are synchronized despite differences in injection and ejection rates of particles across lanes. More recent research by Jiang et al. incorporates Langmuir kinetics in one lane of a two-lane system. In a recent study conducted by Jiang et al. @Xcite, Langmuir kinetics were introduced into one lane of a two-lane system in order to explore the phenomenon of synchronized positions of kinks while considering varying numbers of particles in each lane. The introduction of Langmuir kinetics led to the observation that synchronization of shocks on both lanes occurs when the lane-changing rate exceeds a certain threshold. A boundary layer at the left boundary was also observed as a finite-size effect. However, the model used by Jiang et al. has some limitations as it assumes that attachment and detachment of particles only occur on one of the two lanes, thereby failing to accurately depict the real-life scenario of traffic flow in two or multiple lanes of molecular motors. A boundary layer is also present at the left boundary due to the finite size effect in the system. Unlike previous models that assume attachment and detachment of particles to occur on only one of two lanes, this paper will explore the collective effect of attaching and detaching on both lanes of a two-lane system with symmetric inter-lane coupling. The focus of this study will be on the finite-size effects of the system. The model described in this paper is motivated by the dynamics of molecular motors, specifically, kinesin's unidirectional motion along filaments, attachment and detachment to filaments, and the ability to freely change between adjacent filaments. This system can be viewed as a transport process similar to a biased totally asymmetric simple exclusion process (TASEP), with an incorporated Langmuir kinetics and lane-changing process. Through investigations of the effects of different lane-changing rates on the density and current profiles, insights into particle traffic in molecular motor systems and other areas such as vehicular traffic can be gained. By specifically focusing on finite-size effects, this paper introduces a model that is directly inspired by the dynamics of molecular motors, particularly their unidirectional motion along filaments, random motor attachments and detachments, and molecules freely changing to adjacent filaments. This model is expected to shed light on the study of particle traffic not only in biology but also in other fields such as vehicular traffic. The effects of various parameters on density and current profiles will be investigated. Firstly, the lane-changing rates, represented by @xmath1, @xmath2, and @xmath3, will be examined. Additionally, the combination of attachment rates, @xmath5, and detachment rates, @xmath6, with variable detachment rates, @xmath8, will be studied. Finally, the system size, or the length of each lane, @xmath4, will be analyzed to elucidate the impact of different values on the two-lane system. Results obtained from the model when both lanes are present, as well as when they are coupled, will be compared, allowing for a more comprehensive examination of the lane-changing effects. In this paper, we compare the outcomes of our model when incorporating lane separation and Langmuir kinetics, which involves two separate taseps, labeled as @xmath9 and @xmath10. This approach enables us to discern the impact of varying @xmath3 parameters on a two-lane system with greater clarity. In Section II, we provide a description of our model, incorporating attachment and detachment of particles in both lanes. The paper's organization is as follows: Section II presents our two-lane tasep model that accounts for separate attachment and detachment in each lane. Section III covers our analysis of the data obtained from this model, which we have verified using mean-field approximation (MFA) and monte carlo simulations (MCS). This paper is structured into five sections. In Section II, a description of the two-lane TASEP model is provided, which takes into account the attachment and detachment of particles in both lanes. The model considers a lattice of @xmath11 sites with a length of @xmath4 in each lane. Section III presents and discusses the results of Monte Carlo simulations. In Section IV, a mean-field approximation is outlined and a comparison between the simulation results and the mean-field theory is carried out. The paper concludes in Section V. In this article, we present our field approximation for a two-lane lattice model consisting of @xmath11 sites where each particle moves from the left to the right. We assume that sites @xmath12=1 and @xmath12=@xmath4 define the left and right boundaries, respectively, and sites @xmath12=2, ..., @xmath13 are considered to be within the bulk. Through Monte Carlo simulations, we confirm the quantitative agreement between the mean-field theory and our model's results. We conclude our findings in section V. Within the framework of systems analysis, particular consideration is given to the behavior of particles within a given space, which is segmented into distinct sites and referenced as a set of boundaries denoted by @xmath12 and @xmath12. The left and right boundaries mark the left and right boundaries, respectively, and the set of sites between them is commonly referred to as a \"bulk.\" The aforementioned system involves various processes by which particles move and change between lanes. More precisely, upon being injected into the first site of a selected lane, the particle may undergo several processes:\n\n1. Lane selection: A lane is randomly chosen from a range of available options.\n2. Particle injection: If the first site of the selected lane is vacant, a particle may be injected with a likelihood of @xmath14.\n\nThe sequence of steps that follow, including particle detachment from a bulk, movement along a bulk, lane-changing between bulks, attachment to a bulk, and ejection from the final site of a lane, shape the course of particle behavior within the system. Therefore, this process is the subject of careful analysis and examination to facilitate a more comprehensive understanding of its behavior and dynamics. Thus, a particle in this system may undergo several processes, namely:\n1. Lane selection: A lane is randomly chosen for the particle to move along.\n2. Particle injection: If the first site in the selected lane is empty, the particle will be injected at that site with probability \u03b1.\n3. Particle detachment: With probability \u03b2, a particle can leave the system.\n4. Particle movement: If a particle cannot leave the system, it may move to the adjacent empty site in the same lane. If the next site is not empty, the particle cannot advance for that step.\n5. Particle lane-changing: With probability \u03b3 (assuming it checks whether the corresponding site on the other lane is unoccupied), the particle may change to the site on the other lane that corresponds to the current site.\n\n(Refer to the original source for symbols \u03b1, \u03b2, and \u03b3, as they may have specific definitions.) In a system where particles cannot leave but can only move forward to an empty site in their lane, if the next site is occupied, the particle will attempt to switch lanes. A particle can change to the corresponding site on the other lane with probability @xmath1 if the desired site is unoccupied. If a site in a bulk is empty, a particle can attach to it with probability @xmath16. However, if a particle reaches the end of a lane, it will be ejected with probability @xmath17. These processes are explained through updated rules. To elucidate the process of particle ejection within the system, if a particle reaches the final position of a lane, it is ejected with probability equal to @xmath17. This behavior can be explained with the following updating rules that describe the system's progress. As the two lanes are equivalent, their updating rules will also be the same for ease of description. Only the updating rules for lane 2 will be given in this instance. The occupation variable, denoted by @xmath18, represents the state of the @xmath12th site in the @xmath19th lane. The value @xmath20, or @xmath21, indicates whether the site is occupied or empty, respectively. We solely provide the rules for updating lane 2, where an occupation variable, denoted as @xmath18, represents whether the @xmath12th site in the @xmath19th lane is occupied or not. Variables @xmath20 or @xmath21 are alternative indicators for occupied or empty, respectively. Thus, @xmath22 signifies the state of site @xmath12 in lane 2. The rules are as follows:\n \n* Case @xmath23:\n \n  (i) If @xmath24 occurs, a particle enters the system with a probability of @xmath14.\n \n  (ii) If @xmath25 and @xmath26 concurr, the particle in the site of @xmath27 moves into site @xmath28.\n \n  (iii) If @xmath25 and @xmath29 occur at the same time, then the particle in the site of @xmath27 remains in place. \n \nIn this scenario, lane 2 is being updated according to these rules, which determine how particles enter, move, or stay in their current positions. In this system, there are two possible cases. In case (i), a particle enters with a probability of @xmath14. When both conditions @xmath24 and @xmath25 hold true, the particle in the site of @xmath27 moves into site @xmath28. However, when @xmath25 and @xmath29 are true, the particle in the site of @xmath27 remains in that same site without changing lanes. In case (ii), if @xmath31 is true, the particle leaves the system with a probability of @xmath17, and no lane change occurs in this instance as well. In the scenario where the variable xmath30 holds, there are two possible cases: if xmath31 is true, the particle departs the system with probability xmath17, without causing any lane change. However, if xmath32 is the applicable case, there are distinct possibilities. Firstly, if xmath33 is true and the particle can leave the system, it does so with a probability of xmath15. Alternatively, if the particle cannot leave the system, it attempts to relocate to site xmath34 assuming that the condition xmath35 is true. If the particle cannot progress any further, it has the chance to switch to lane one with a probability of xmath1, under the circumstance of xmath36 being true. A different scenario is when xmath37 is true, where a particle may enter the system with probability xmath16. \n\n<|user|>\nCan you please provide me with examples of when the scenario of xmath32 would occur and what value of xmath33 would lead to the particle leaving the system? According to (i), if x is an element of the set xmath33, the particle may exit the system with probability xmath15 in such cases. Alternatively, when the particle cannot leave, it moves towards site xmath34, provided xmath35 holds true. If the particle is unable to progress, there is a probability of xmath1 for it to transfer to lane 1 if xmath36 is satisfied ((ii)) implies that a particle may enter the system with probability xmath16, as illustrated in Fig. 1. For ease of understanding, a symmetric lane changing rule has been utilized. For ease of analysis, we utilize a symmetric lane-changing rule in our initial investigations. This simplified setup will facilitate our understanding of the dynamics of interacting particles. Although we will eventually explore the effects of asymmetric lane-changing rates on vehicular traffic flow through a different updating scheme, we begin with the use of a common update for all sites known as the synchronous updating rule. This parallel scheme is often employed in situations that require consistent communication and coordination among a large number of entities. In numerous simulations of molecular motor traffic, a random sequential updating scheme has been employed. This third updating methodology follows the third pattern mentioned: in random. This scheme entails selecting a site at random and subsequently updating it in accordance with established rules. Conversely, in the first pattern, the updating rules are implemented synchronously on all sites simultaneously. Another example of the first pattern is seen in the ordered sequence update method. Lastly, in some instances, the particle positions are updated in series from one end to the other end of a one-dimensional system as per the second pattern. In all cases, the particles' positions are updated in an ordered sequential manner. In this section, the results of Monte Carlo simulations are presented. Specifically, using a randomly sequential updating scheme, namely the third scheme widely used in simulations of molecular motor traffic, we calculate the average current for a site chosen randomly. The formula for calculating the average current is given as:\n\n@xmath38 = @xmath39(1 - @xmath40)\n\nThe parameters for this calculation are set to @xmath41, @xmath42, @xmath43, @xmath44, and @xmath7 = @xmath46 * @xmath8. In simulations, the system was updated based on these values to generate data for analysis. In simulations, the average current (hereafter referred to as Iavg) is calculated using the expression: Iavg = I0(1 - \u03b1) with \u03b1 being the time-dependent occupancy parameter, I0 being the current density at t=0, and parameters set to \u03b3, \u03b4, b, u, and \u03d2 for which \u03b3 determines the diffusion coefficient, \u03b4 is the particle loss due to wall collisions, b is the collision rate between particles, u is the background gas density, and \u03d2 represents the mean free path between collisions. In these simulations, stationary profiles are obtained by averaging Iavg over a total of @xmath47 samples at each site. The sampling time interval is @xmath48. The first @xmath49 time steps are discarded to ensure that transient behavior has resolved. For the sampling time interval of @xmath48, the initial @xmath49 time steps are disregarded to allow for the dissipation of transient behaviors. In the case where @xmath50 is true, the two-lane process reduces to two separate, one-lane systems with Langmuir kinetics. Extensive research on the one-lane Langmuir tasep has been conducted, as outlined in reference @xcite. Figure 2 illustrates the time-averaged density at @xmath51 and current at @xmath52 for one lane of the two-lane system, for different values of @xmath3. For a particular case where @xmath10 is met . In previous research, the characteristics of a one-lane Tasep with Langmuir kinetics have been extensively explored and analyzed. This system has been widely studied and is well-known in relevant literature, as evidenced by references cited in Ref[@xcite] (hereafter denoted as the cited reference). Illustrating this, Figure 2 portrays the average density @xmath51 and current @xmath52 of a one-lane Tasep for various values of the adsorption rate @xmath3. For the specific instance @xmath10, the system reduces to two individual single-lane Taseps that are separated from one another. In this scenario, one would expect that, as the value of @xmath3 increases from 0 to 1, the domain wall would move in a consistent and predictable pattern - either to the left or to the right. However, our computer simulations have revealed an unexpected behavior. At first, the domain wall moves to the left marginally, but then it moves toward the right as the value of @xmath3 increases, contradicting the usual expectation (see Figures 2(a) and (b)). As the value of @xmath3 changes from 0 to 1, it was commonly expected for the domain wall to consistently move in either direction. Contrary to these predictions, a surprising event was discovered in our simulations. In the early stages, from an initial value of 0 for @xmath3 up to 0.2, the domain wall initially moved to the left (see Figs. 2(a) and (b)). However, as @xmath3 was further increased to a value of 0.53, a previously overlooked behavior was recorded. The domain wall exhibited an unexpected U-turn pattern and subsequently moved towards the right (see Figs. 3(a) and (b)). This phenomenon, which suggests that the behavior of domain walls is not always straightforward when the value of @xmath3 is increased, highlights the potential complexity that emerges in this field. In equations (3a) and (3b), the moving trajectory of the domain walls for @xmath53 can be observed. As @xmath3 increases from 0 to 0.2, the domain wall moves to the left. However, as @xmath54 increases, the domain wall moves toward the right. This phenomenon is referred to as the \"jumping effect\", where the domain wall (along with the current curve) jumps from left to right due to the particles leaping between the two lanes. A critical value, denoted as @xmath55, exists to determine the direction of motion when certain conditions are met. The jumping effect becomes weaker when @xmath7 and @xmath8 increase proportionally, as shown in Figures from ref.@xcite . Furthermore, at a specific value, denoted as @xmath55, it becomes clear that the domain wall's movement direction is significantly altered under certain circumstances. This phenomenon is also demonstrated in reference @xcite, where increasing proportions of @xmath7 and @xmath8 result in a weaker jumping effect, as observed by comparing figures 2(a) and (b) with figures 4(a) and (b). This indicates that the jumping effect is also affected by attachment and detachment rates. This indicates that the jumping phenomenon, which results from the movement of a domain wall, is affected not only by the jumping process itself but also by attachment and detachment rates in both (a) and (b) conditions. The jumping effect is also evident in larger systems, such as the one depicted in Fig. 5 at small values of @xmath3. Notably, when @xmath57 is true, the domain wall moves leftward. As @xmath3 approaches small values, the behavior of the domain wall can be observed. Initially, when @xmath57, the domain wall appears to move left. However, when @xmath58, there is a distinct rightward movement. For instance, there is a clear motion towards the right when @xmath59 (see fig. ). The critical value of @xmath3, denoted by @xmath60, changes depending on the system size. With smaller system sizes, the value of @xmath60 is lower, as low as 0.2 when the system size is 1,000, but as the system becomes larger, the value of @xmath60 increases, reaching around 0.3 when the system size is 10,000. Indeed, it is apparent that there is a significant shift toward the right in various simulations, as demonstrated by the data presented in Figure X, where Xmath59 yields a critical value that varies with system size, ranging from 0.2 to 0.3 as the system size increases from 1,000 to 10,000. Our simulations have confirmed that this jumping effect in a two-lane homogeneous totally asymmetric simple exclusion process coupled with Langmuir kinetics is a finite-size effect. When system sizes are very large, such as @xmath61, in our simulations, the jumping effect almost vanishes. We assert that the study and examination of these finite-size effects are significant because the size of real systems is often not exceedingly large. In large-scale simulations, when dealing with systems comparable to real-world systems, such as the case of studying the motion of kinesin protein motors, it becomes crucial to assess the impact of finite-size effects. In our simulations, we have observed that the jumping effect almost ceases to exist in very large system sizes. This phenomenon can potentially lead to erroneous conclusions when assuming an infinite system size, which may not accurately reflect the behavior of the system in reality. When examining the movement of molecular motors, such as kinesin protein motors, a system size of around 1,000 units is sufficient to realistically depict the processes. Even for larger systems, we should take into account the finite-size effects, and avoid drawing erroneous conclusions based on an assumption of infinite system size. By comparing the results obtained using different system sizes, we can gain a better understanding of how the system behaves and how its behavior may change as the system grows or shrinks in size. In standard biological settings, a kinesin motor typically extends over a distance of around 100 steps, each spanning approximately 8 nanometers, on microtubules. Therefore, a system that realistically represents the movements of molecular motors typically does not require a scale much larger than 1,000. Even for larger systems, a size of 62 is typically sufficient for accurate simulation. This indicates that the vast majority of theoretical models based on an infinite system size may not accurately describe real systems. By comparing Figures 2(a) and (b) to Figures 4(a) and (b), it can be observed that the slopes of domain walls proportionally increase with the increase in ratios of @xmath7 and @xmath8 due to more particles attaching to these lanes as the difference between them widens. In both conditions (a) and (b), it can be observed that the slopes of domain walls increase in proportion to the increase in the proportion of parameters @xmath7 and @xmath8. This is because when the difference between these two variables broadens, more particles preferentially attach to these two lanes, rather than detaching from them. By increasing the value of @xmath3, there's a higher chance for particles to shift to the other lane if they are unable to progress along their current path. Upon further increasing @xmath3, however, the density starts to decrease, and the locations of shocks shift slightly to the right (refer to Figs. 2(a) and 3(a)). When @xmath63 (or @xmath64, which represents the position with the maximum density), the differences between density profiles become less distinct (see Figs. 2 and 4). In both Figures 2(c) and 2(d), as well as Figures 4(c) and 4(d), the current profiles are presented. This indicates that the lane-changing rate, denoted by @xmath3, does not have an apparent impact on the system's properties (such as average density and current profiles) after peak density is reached. Notably, in these plots, it is discovered that: \n\n(i) The current declines as the lane-changing rate increases until it approaches the maximum current, which is denoted by @xmath65. \n\nFrom these results, it can be concluded that the current is negatively correlated with the lane-changing rate during this period. In both cases (c) and (d), as well as in scenarios (c) and (d) of Study 4, the present investigation reveals that there are distinct patterns for the current. It is observed that as the time step @xmath3 increases, the current diminishes until it nearly reaches the maximum current of @xmath65. This critical point occurs at a specific position @xmath70, represented by @xmath68. When @xmath71 holds true and the lane-changing rate increases, the current elevates. Notably, when @xmath72 is achieved, the current profiles for different @xmath3 yield almost indistinguishable results.  (Note: All variables denoted by @xmath should be replaced by their corresponding mathematical symbol.) Hence, the peak current at position @xmath70 is 0.25, as denoted by the formula. For instance, with an increasing lane-changing rate @xmath71, the current increases. Similarly, when the vehicle density @xmath72 is minimal, the current profiles are almost identical with increasing @xmath3. To investigate further, we examine the impact of changes in the ratio @xmath73/@xmath8 on average density and current profiles while holding @xmath7 constant. Notably, a lower @xmath8 correlates to a higher value of @xmath46. Under a scenario where @xmath7 remains constant, @xmath8 fluctuates. It is essential to note that as the value of @xmath46 increases, the value of @xmath8 decreases. The increasing trend of average density is clear from the aforementioned graph (see Figure). Moreover, the shock moves to the left, and the amplitude increases, indicating a narrower transition region. In other words, as the value of @xmath46 increases, the width of this region diminishes. Furthermore, as the shock transitions left, the amplitude heightens, resulting in a narrower width of the transition region. This phenomenon can be explained by the fact that as the amplitude increases, opportunities for particles to detach from the bulk decrease proportionally with the value of @xmath8. Consequently, more particles remain in the bulk, resulting in an increase in average density. This is represented in Figure. When there are more particles remaining in the bulk, it results in an increase in average density. A graph demonstrating this phenomenon can be seen in Figure 6(b), where the initial current increases as the value of @xmath46 increases until it reaches a maximum. However, as the value of @xmath46 continues to increase, the position of the maximum current moves left, and its amplitude increases as well. This indicates that the effect of the specific value of @xmath46 has an impact on the overall current and density within the system. Upon increasing parameter $\\alpha_x$ in our system, the current peak moves left according to Figure 6(b). The current-density curves can be obtained via Equation 38, where $\\mathcal{J}(\\alpha_x) = \\mathcal{J}_{\\max} (1 - \\rho(\\alpha_x))$, and thus when the densities of the two adjacent sites are equal to 0.5, the maximum current $\\mathcal{J}_{\\max} = 0.25$. For densities higher or lower than 0.5, the corresponding current is lower than 0.25. Figure 7 shows density and current profiles for different system sizes. In instances where the density exceeds or falls below a value of 0.5, the associated current is significantly lower, as shown by figure 7, which exhibits density and current profiles for different system sizes. It is observed that as the number of sites in the system increases, the width of the transition region decreases. This finite-size effect may result in the boundary layer disappearing when the system is infinitely large. The finite-size effect observed in figure 7(a) is consistent with findings reported in references @xcite in fig . Our simulation reveals that the boundary layer ultimately vanishes in infinite systems, indicating a finite-size effect. Fig. 7(a) exhibits similar behavior compared to earlier studies, with notable amplification observed in the left boundary when the system size increases, as seen in Fig. 7(b). A mean-field theory is developed in this section, with the occupied variable (@xmath18) utilized, as defined in Section II. In this section, a mean-field theory is developed utilizing the occupation variable introduced in Section II. This theoretical framework allows us to analyze the evolution of particle densities, @math{n}(\\mathbf{x},t), within a bulk, i.e. for @math{x} = (x_1,x_2) satisfying the inequalities @math{x_{min} < x_i < x_{max}, i=1,2}, where the term @math{n_R(x_{min},t)} denotes the statistical average particle density in lane 1 at the position @math{x_1=x_{min}} and time t, while @math{n_L(x_{max},t)} represents the statistical average particle density in lane 2 at the position @math{x_1=x_{max}} and time t. The relationship between these densities can be expressed as: \n\n@math{0 = dn_L/dt + J_L + J_R, }\n\n@math{dn_R/dt = J_L - J_R,}\n\nwhere @math{J_L} and @math{J_R} respectively denote the average current of particles flowing from lane 1 to lane 2, and the average current of particles flowing from lane 2 to lane 1, between sites @math{(x_1, x_2=x_{min}) } and @math{ (x_1, x_2=x_{max})  }, respectively. The terms @math{J_L} and @math{J_R} can be represented as:\n\n@math{J^{R}_{L}(\\mathbf{x},t) = \\kappa_{LR}(\\mathbf{x},t)(n_L(\\mathbf{x},t) - n_R(\\mathbf{x},t)),}\n\n@math{J^{L}_{R}(\\mathbf{x},t) = \\kappa_{RL}(\\mathbf{x},t)(n_R(\\mathbf{x},t) - n_L(\\mathbf{x},t)),}\n\nin which @math{\\kappa_{LR}(\\mathbf{x},t)} and @math{\\kappa_{RL}(\\mathbf{x},t)} define the inter-lane flow rates determined by the interaction at site @math{(x_1, x_2)} of lanes  The term denoted by \"@xmath79\" represents the average current between sites \"@xmath12\" in lane 1 and lane 2, while \"@xmath80\" indicates the average current from sites \"@xmath12\" in lane 2 to lane 1. The first two terms in Equations (1) and (2) describe particle movement, while the middle two terms correspond to the changing of particle lanes. The last two terms signify the attachment and detachment of particles. The densities evolve according to Equations (81) and (82), where \"@xmath19\" can be either 1 or 2, and they are equivalent when we use the same lane-changing rate between the two lanes. In essence, equations (1) and (2) can be considered equivalent when equal lane-changing rates are employed. Therefore, for the purposes of this analysis, we will concentrate solely on the mean-field approximation for lane 2 (denoted by Eq. (2)). When the length of the lattice @xmath4 is significant, that is, when the parameter @xmath83 is substantial, the discrete coarse-grained stationary-state equation (2) can be translated into its continuum mean-field counterpart. In cases where the parameter @xmath4 is large, specifically when it reaches the value @xmath83, we can transform the coarse-grained stationary-state equation (2) into continuum mean-field approximation. Firstly, we factorize correlation functions by substituting @xmath84 and @xmath85 with @xmath86 and @xmath87, respectively. Consequently, when ([m5]) is set into eq. (2), we obtain Eq. (3):\n\n   @xmath89\n\nwhere @xmath90 is the mean field for variable @xmath91, @xmath92 is the fluctuation field, and @xmath93 is the constant field. The term @xmath94 is replaced by the linearized local operator's mean in Eq. (3), and the diagonal fluctuation kernel's mean is approximated by Eq. (4). Similarly, the term @xmath98 is replaced by its linearized local operator's mean, and the diagonal fluctuation kernel's mean is approximated by Eq. (5). The boundary conditions become Eqs. (6) and (7). \n\n@system|>\n Using manipulations, we can obtain Equation 89 where \u03c4_{2}, \u03c4_{3}, \u03c4_{4}, and \u03c4_{5} denote various parameters, and t and \u03c4 are time variables. The term \u03c1_{b} is replaced by \u03c1_{b}' and \u03c1_{f} by \u03c1_{f}', and \u03b7 is approximated as \u03b7_app. Similarly, \u03c1_{bf} is replaced by \u03c1_{bf}' and \u03c1_{af} by \u03c1_{af}', and \u03c4_{f} is approximated as \u03c4_{f_app}. The boundary conditions become x = 0 and x = L, where L is the length of the system. As lane-changing is symmetric and each lane is homogeneous, we have Equation 104, based on our Monte Carlo simulations that demonstrate lane densities in lanes 1 and 2 are the same. Thus, Equation 6 reduces to Equation 105 in the limit where vehicle arrival rate is high, and the system reaches a stationary state with \u03c1_{b}' = 0, and Equation 7 simplifies into Equation 108. As the limit of a certain system factor, @xmath106, approaches, the system reaches a stationary state where @xmath107 is zero, as outlined by equation (7), denoted as [m8]. This is a first-order differential equation, as described in reference @xcite, and solving it results in low-density profiles in the system bulk. By integrating the equation from a left boundary density of @xmath110 to density @xmath111, the density profile on the left side can be obtained. Similarly, by starting from density @xmath113 on the right boundary and integrating the equation, one can determine the right side's density profile. The overall density profile across the system is obtained by determining a shock position based on these two profiles, as depicted in figure 8. The mean-field approximation (MFA) and Monte Carlo simulations (MCS) are compared in this approach. The overall density profile throughout the system can be described by identifying the position of the shock in two separate profiles. This is demonstrated in Fig. 8, where the results of mean-field approximation and Monte Carlo simulations are compared. The Monte Carlo simulations align with the results of mean-field approximation. It should be noted that the numerical outcomes of Monte Carlo simulations in Figs. 8(a) and (b) are almost identical to Figs. 2 and 5 in Ref. @xcite, indicating that the lane-changing rate, denoted as \u03bb in Eq. (3), does not exert any discernible impact on the density profiles of a two-lane Totally Asymmetric Simple Exclusion Process that conforms to Langmuir kinetics when the attachment/detachment rate is significant and/or the size of the system is considerable. As observed in Figures 8(a) and 8(b), which closely resemble Figures 2 and 5 in Ref. @xcite, the numerical results obtained through Monte Carlo simulations (MCS) in a two-lane, totally asymmetric exclusion process coupled with Langmuir kinetics, showed no significant impact of lane-changing rate @xmath3 on the density profiles. This outcome corresponds to the results obtained from the Mean Field Approximation (MFA) method as well. In this paper, we explored this particular type of model inspired by molecular motors, which exhibit motions that include filament walking, random attachment and detachment, and free jumping between filaments. The system exhibited complex behaviours, such as a 'jumping' phenomenon, where the domain wall first moves slightly left, then moves towards the right as lane-changing rate increases. According to the results of Monte Carlo simulations, the model which inspires the movement of molecular motors closely matches the results of mean-field approximation. This system, which displays the typical behavior of motor advancement along filaments alongside random attachment and detachment, predominantly exhibits complex behavior on two lanes. The simulation shows a \"jumping\" effect, where the domain wall initially moves left slightly and then rapidly moves towards the right as a result of lane changes. This effect is a finite-size effect and diminishes as the system grows larger. Increasing the attachment and detachment rates in proportion to one another weakens the \"jumping\" effect. However, when densities reach maximum, an increase in the lane-changing rate has little effect on the system's properties like average density and current. However, as attachment and detachment rates concurrently increase in proportion, the \"_jumping_\" phenomenon becomes weaker in response. Once densities reach maximum, any increment in the lane-changing rate will hold little effect on system properties such as average density and current. If the attachment rate, denoted by @xmath7, is fixed, the system properties such as average density and current experience growth, but ultimately current decreases as the detachment rate, represented by @xmath8, diminishes. The size of the system, demonstrated by @xmath4, has a notable impact on the shock's intensity and density, with larger systems experiencing greater shock amplitude and higher average density. Additionally, for a high value of attachment and detachment rates as well as a large system size, the lane-changing rate, indicated by @xmath3, will show little variation in density profiles, given the Langmuir kinetics model. As system size increases, according to research, the amplitude of shock waves also increases, and the average density also sees a significant uptick. However, when the lane-changing rate, as represented by xmath3, is kept constant, it has little to no impact on the density profiles in a two-lane traffic flow model involving Langmuir kinetics. The authors would like to acknowledge funding support from various sources, including the Asia: NZ Foundation Higher Education Exchange Program (2005), the Massey University Research Fund (2005), and the Massey University International Visitor Research Fund (2007). Similarly, R. Jiang gratefully acknowledges financial support from several prestigious programs, including the National Basic Research Program of China (2006CB 70550) and the National Natural Science Foundation of China (NNSFC) under Key Project no. 10532060, with corresponding project numbers. Rong Jiang gratefully acknowledges the support of China's National Basic Research Program, also known as the 973 Program, through the award 2006CB705500, along with the National Natural Science Foundation of China (NSFC) through grants with respective numbers 10532060, 10404025, 10672160, and 70601026. Additionally, the author appreciates the contribution of Denise Newth in proofreading this manuscript.",
        "abstract": " in this paper , we study a two - lane totally asymmetric simple exclusion process ( tasep ) coupled with random attachment and detachment of particles ( langmuir kinetics ) in both lanes under open boundary conditions . \n our model can describe the directed motion of molecular motors , attachment and detachment of motors , and free inter - lane transition of motors between filaments . in this paper \n , we focus on some finite - size effects of the system because normally the sizes of most real systems are finite and small ( e.g. , size @xmath0 ) . \n a special finite - size effect of the two - lane system has been observed , which is that the density wall moves left first and then move towards the right with the increase of the lane - changing rate . \n we called it the jumping effect . \n we find that increasing attachment and detachment rates will weaken the jumping effect . \n we also confirmed that when the size of the two - lane system is large enough , the jumping effect disappears , and the two - lane system has a similar density profile to a single - lane tasep coupled with langmuir kinetics . \n increasing lane - changing rates has little effect on density and current after the density reaches maximum . also , lane - changing rate has no effect on density profiles of a two - lane tasep coupled with langmuir kinetics at a large attachment / detachment rate and/or a large system size . \n mean - field approximation is presented and it agrees with our monte carlo simulations . ",
        "section_names": "introduction\nmodel\nmonte carlo simulations\nmean-field approximation\nconclusion\nacknowledgements"
    },
    {
        "article": "To make paper from recycled materials, the recycling process involves collecting paper waste, sorting it by composition, then pulping and de-inking the paper pulp. This removes any lingering ink, which can discolor the recycled paper, and any other impurities. The pulp is then washed, bleached, and refined, ready to be made into new paper. Recycled paper can be made from various sources, including office paper, newspaper, and cardboard. The recycling process helps to conserve resources, reduce greenhouse gas emissions, and prevent waste from ending up in landfills. At the core of the coupled cluster (CC) method lies the exponential ansatz for the exact many-body wavefunction, which relies on the cluster operator involving amplitudes of n-fold particle-hole excitations from the reference Slater determinant. This parametrization is derived from a rigorous resummation of many-body perturbation theory (MBPT) series. By solving the eigenvalue equation, the cluster amplitudes and energies are determined. Despite an infinite number of terms in the ansatz due to the exponent expansion, the resulting equations for cluster amplitudes only contain a finite number of terms. Regrettably, this simplifying property is lost when the resulting wavefunctions are used to calculate matrix elements, as the number of terms becomes infinite upon expansion of exponents in transition amplitudes between two normalizable states. However, a crucial simplifying property is lost in calculations involving matrix elements based on the resulting wavefunctions. During the expansion of exponents, the number of terms becomes infinite, which can lead to a loss of accuracy and computational efficiency. For instance, calculating transition amplitudes between two states with normalization involves an infinite number of terms, such as:\n\n@math9\nm_{ij} = \\int d^3r d^3r\u2019 \\chi_i^*(\\bm{r})\\hat{O}_{12}\\chi_j(\\bm{r}')\n\nwhere $\\chi$ are the wavefunctions, $\\hat{O}_{12}$ is the operator, and the integral over $r$ and $r\u2019$ is performed over the entire spatial domain. It is clear that both the numerator and denominator in this expression involve infinite numbers of terms during the expansion of exponents. In this paper, we address the question of partially summing these terms to obtain a result that encompasses an infinite number of calculations. Our focus is on transitions between states of univalent atoms such as alkali-metal atoms, and there have been numerous relativistic coupled-cluster calculations for these systems. In particular, we will specifically mention several studies. Focusing on the transitions between the univalent states of alkali-metal atoms, recent relativistic coupled-cluster calculations have been conducted, omitting the non-linear terms in the expansion (Eq. 14 and Eq. 15). This approximation, known as the linearized coupled-cluster (LCC) method, has been utilized in calculations (Ref. @xcite) for these systems. However, for univalent atoms, the random-phase approximation (RPA) has proven to be a significant chain of many-body diagrams for calculating matrix elements. A direct comparison between the RPA series and the truncated LCC expansion is documented in Ref. @xcite. In parallel to this, it has been widely recognized that for non-interacting atoms a vital collection of many-body diagrams originating from the random-phase approximation (RPA) method is well-established. However, a comparison of the RPA series and the truncated LCC expansion in reference @xcite leads to a conclusion that some essential RPA diagrams are missed due to the omitted non-linear terms. This defect has been addressed through one of the methods proposed in reference @xcite, where the authors replaced the bare matrix elements with the dressed matrix elements in accordance with the RPA technique. An alternative scheme for avoiding double counting in the RPA (Ring Polymer Approximation) method has been proposed. In previous methods to correct for missing RPA diagrams, bare matrix elements were replaced with dressed matrix elements based on the RPA method. However, this direct RPA dressing involved a partial subset of diagrams already included in the CC (Coupled Cluster) method, leading to a double counting of diagrams. To partially rectify this issue, a manual removal of certain leading-order diagrams that were doubly counted was carried out in Ref . [Xcite]. Here, an infinite summation scheme for RPA chains that avoids double counting and eliminates the need for a manual removal of extra diagrams is presented. In previous calculations that involved manually removing certain leading order diagrams, higher order terms were counted twice in some cases. However, in this study, we introduce an alternative infinite-summation scheme to avoid this double counting and remove the need for manual removal of the \"extra\" diagrams. In addition to dressing the coupled-cluster diagrams for matrix elements with RPA-like corrections, we consider another subset of diagrams that dresses particle and hole lines in CC diagrams. The leading order corrections due to this dressing scheme arise in fourth-order MBPT, and we present a detailed comparison with relevant fourth-order diagrams. This approach is further illustrated with relativistic computations of hyperfine structure constants and dipole matrix elements for the cesium atom. Along with dressings, we also incorporate certain classes of diagrams previously included in direct fourth-order MBPT calculations. In this paper, the present work investigates the leading-order corrections resulting from the recently introduced dressing scheme in M\u00f8llebjerg-Plesset perturbation theory (MBPT). These corrections emerge in the fourth order of MBPT, and the current study provides a thorough comparison of the relevant fourth-order diagrams. Furthermore, the authors demonstrate their approach by computing the hyperfine-structure constants and dipole matrix elements for the Cs atom, taking into account both dressing corrections and certain classes of diagrams from the direct fourth-order MBPT calculation (as in Ref. [@xcite]). The results reported in this study represent the first complete fourth-order MBPT calculation for Cs. The paper's organization is as follows. This paper presents the first complete calculations through the fourth order of many-body perturbation theory (MBPT) for Coulomb systems. It is structured into several sections commencing with a comprehensive discussion of the CC formalism in Section [ sec : ccformalism ]. The subsequent section [ sec : particlehole ] deals with the dressing of both particle and hole lines. In Section [ sec : rpa ], there is a discussion of RPA-like dressing. This paper can be viewed as an all-order extension of the fourth-order calculation presented in @xcite. Section [ sec : iv ] presents an illustrative comparison with the IV-order diagrams. This paper builds upon the fourth-order calculation presented in @xcite by extending it to all orders. We first dress particle and hole lines in section [sec: particlehole] and subsequently discuss RPA-like dressing in section [sec: rpa]. In section [sec: iv], we present an illustrative comparison between our present scheme and the IV-order diagrams. In section [sec: numerics], we demonstrate the designed summation schemes numerically, and the conclusions we drew are presented in section [sec: conclusion]. Unless otherwise noted, atomic units, as defined in @xmath16, are used throughout the paper. We follow the conventions established in a previous reference, as described in ref. Throughout the paper, we employ the standard convention of citing references using the abbreviated form @xcite to represent Brueckner-Goldstone diagrams. In this section, we will focus our discussion on the implementation of the coupled-cluster method, specifically applied to atomic systems with a single valence electron outside closed shell cores. We will review several approximations and thoroughly summarize the CC formalism to calculate matrix elements. In this section, we focus specifically on the application of the coupled-cluster (CC) method for atomic systems with one valence electron outside closed shell cores. We review various approximations and summarize the CC formalism for calculation of matrix elements. The goal is to solve the atomic many-body problem, and the total Hamiltonian, \u038917, is split into the suitably chosen lowest-order Hamiltonian, \u038919, and a residual interaction, \u038920, that is treated as a perturbation. When dealing with systems with one valence electron outside closed-shell cores, a practical choice for \u038919 is the frozen-core Hamiltonian, \u038921, based on the Hartree-Fock Hamiltonian \u0389cite. By explicitly specifying the state of the valence electron, we can define the proper reference eigenstate as being given by the pseudo-vacuum state of the occupied core, \u30080|>24. For open-shell systems, a general CC parametrization for these systems reads, \u038925, where the curly brackets denote the normal product of operators. The total Hamiltonian is divided into two parts, with the lowest-order Hamiltonian, chosen as the suitably chosen frozen-core Hartree-Fock Hamiltonian, being treated as the unperturbed system. For systems with one valence electron outside of closed-shell core, this is denoted as @H_0. The residual interaction, represented as a perturbation, is denoted as @H_1. In this approach, for valence state @X, the proper reference eigenstate of the lowest-order Hamiltonian, denoted as @X_0, is obtained by including the pseudo-vacuum state of the filled core orbitals. In open-shell systems, a general CC parametrization reads @Psi, using normal products of operators. For univalent systems, the parametrization is simplified as @Psi, where cluster operator involves excitations of core orbitals and @A includes additional excitations from the valence state. Throughout the paper, a labeling convention is employed, where indexes @X0 represent occupied core orbitals, and indexes @X1 represent virtual orbitals, including valence states @X and remaining virtual orbitals. Our formulas for the excitations of core orbitals @xmath27 and @xmath28 incorporate additional excitations from the valence state @xmath22@xmath29. Throughout this paper, we employ a labeling convention where indexes @xmath30 denote single-particle states occupied in the core @xmath31, and indexes @xmath32 stand for remaining (virtual/excited) orbitals. In this convention, valence states @xmath22 and @xmath33 comprise a subset of the virtual orbitals. Indexes @xmath34 refer to any of these classes of single-electron orbitals. In Eqs. ([eq: corecluster],[eq: valcluster]) the cluster amplitudes @xmath35 denote single-particle excitations, and @xmath36 denote two-particle excitations. Our calculations typically involve truncations of the cluster operator at single and double excitations, referred to as the CCSD approximation. The LCCSD approximation consists of neglecting non-linear terms in the expansion of the exponent in Eq. ([eq: ccparamopenshell]). To find the cluster amplitudes, we solve a proper analog of the eigen-value equation. In most applications, the cluster operator is typically truncated to include only single and double excitations, known as the CCSD approximation. This linearized LCCSD approximation further neglects non-linear terms in the expansion of the exponent in the cluster operator, as discussed in the introduction. The cluster amplitudes are found by solving a proper analog of the eigen-value equation. In a typical application, the need arises to compute matrix elements between two many-body wavefunctions, described by eq. ([eq:melgeneral]) using a method developed by @xcite. The disconnected diagrams in the numerator and the denominator cancel each other out. In high-quality Wikipedia-like English, the expression for the exact matrix element, following the demonstration of disconnected diagrams in Ref. [@xcite], is revealed via Eq. (11), which represents a final expression that involves both connected and disconnected diagrams. This formula is expressed as:\n\n$$ \\left [  1 + \\left ( n_{w}^{\\mathrm{val}} \\right )_{\\mathrm{conn}} \\right ]^{1/2} \\quad , \\quad \\label{eq : zconn} $$\n\nHere, the matrix element is split into contributions from the core and valence electrons. The core's diagrams are independent of the valence indexes, which is demonstrated through the use of Eq. (10), a core-dependent portion of the normalization factor. The valence part of the normalization factor is defined similarly.\n\nNotice that all diagrams that appear in Eq. (11) contain both connected and disconnected parts. This was demonstrated previously in Ref. [@xcite]. As shown in the literature, disconnected diagrams are taken into consideration and subsequently eliminated when calculating the final expression through cancelation. Similarly to the normalization factor in the context of xmath46, both the valence and core components are also defined in a similar manner. This is evidenced by the appearance of all diagrams in equation [eq:zconn], which must be rigorously connected as emphasized by the subscript \"core\" notation \"@xmath47.\" Since the total angular momentum of the closed-shell core is zero, the core contribution \"@xmath48\" vanishes for non-scalar (and pseudo-scalar) operators, and in the following discussion, we will mainly focus on \"@xmath45.\" The lccsd parametrization, denoted in equation [eq:lccsdwf], has been employed by @xcite to derive 21 diagrams for \"@xmath45\" and 5 contributions to \"@xmath49.\" In contrast to closed-shell atomic cores with a zero net angular momentum, this paper aims to explore beyond the linearized LCCSD (linearized coupled-cluster singles and doubles) contributions for expectation values. The authors of Ref. have employed LCCSD to derive 21 diagrams for @xmath45 and five contributions for @xmath49, with additional LCCSD contributions to @xmath48 mentioned in their reference. However, the goal of this study is to move beyond the LCCSD \"skeleton\" diagrams and dress them with nonlinear CC terms. The LCCSD framework provides a useful starting point for exploring the electronic structure of molecules and excitations more accurately, particularly in the context of high-level correlated methods. In this paper, we aim to transcend the limits of linearized lowest-order coupled cluster theory (LCCSD) and delve into the nonlinear core-correlated terms. The LCCSD approximation delivers a framework of \"skeleton\" diagrams that will be \"dressed\" by incorporating nonlinear core-corelating (CC) elements. The figure depicts illustrative LCCSD diagrams (refer to Fig. [fig]) with dominant corrections to the Hartree-Fock (HF) value emerging predominantly from RPA-type diagram (a) and Brueckner-orbital diagrams (c) and (d), as commonly seen in typical calculations. In common calculations, the primary correction to the Hartree-Fock (HF) value usually comes from RPA-type diagrams (A) and Brueckner-orbital (BO) diagrams (C) and (D). These diagrams follow a numbering scheme originally used in Ref. @xcite. The corresponding algebraic expressions for LCCSD contributions are presented below: \n\n@xmath50\n\nwhere the conjugate term, denoted by @xmath51, entails a swap in valence indexes and cluster amplitudes, which are represented by heavy horizontal lines. To illustrate the corresponding algebraic expressions for the diagrams in an LCCSD computation, one can use the notation @xmath50 to represent matrix elements, where the valence indices that are present in this context are indicated explicitly:\n\n@xmath52.\n\nHeavy horizontal lines denote the cluster amplitudes, and the presence of valence doubles and BO singles in the RPA diagram can be seen by examining this specific type of diagram. It should be noted that we are not including the exchange variants for the diagrams in our illustration.\n\nIn particular, the algebraic expression for the RPA diagram is:\n\n[!diag!]\n! [RPA diagram](https://www.wikiwand.com/science/file/Rpa-09-fig2.png) !\n\nwhere the valence indices are not explicitly displayed here. The corresponding algebraic expression for this specific diagram is given by:\n\n[!assistant!]\n\n@xmath50 where @xmath53 denotes the RPA cluster amplitude, which is a matrix element that results from the doubles-to-quartet amplitude that appears in an RPA calculation. The index pair ij is referred to as the pair of valence electrons that are moved to or from the relevant configuration in order to generate this particular RPA diagram. In this case, the RPA diagram involves only valence doubles, as there are no single-particle excitations present. \n\nHowever, other diagrams in an LCCSD computation may involve contributions from both valence singles and doubles, as well as a variety of other diagrams that are not shown here for brevity's sake. In this work, we concentrate exclusively on the implementation of the coupled cluster method for univalent systems, as previously presented. Herein, in the remainder of this paper, we focus on the mathematical object @xmath53, following the Wick Theorem @xcite. We can simplify this expression by contracting creation and annihilation operators between different parts of it. In some instances, highly complex structures may result, necessitating a preliminary construction. To illustrate, consider a product @xmath0, which can be expanded into a sum of normal forms using the Wick theorem. Here, notation @xmath55 stands for an 8-body term. According to the Wick theorem, which simplifies expressions involving creation and annihilation operators, a product consisting of complex structures may be expanded into a sum of normal forms. This expansion reveals multiple terms, including a zero-body term that does not contribute to connected diagrams. One-body terms lead to dressing of particle and hole lines, as discussed in section [ sec : particlehole ], while a portion of the two-body term results in RPA-like dressing for matrix elements, as shown in section [ sec : particlehole ]. This approach prepares the ground for more advanced calculations. The one-body term in the product, outlined in Eq., will result in the dressing of particle and hole lines, as explained in section [ sec: particlehole ]. A portion of the two-body term will bring about RPA-like dressing of LCCSD diagrams for matrix elements, also detailed in section [ sec: particlehole ]. In this section, we focus on deriving all-order insertions for particle and hole lines, using a topological expression of the one-body term. The first term is an object where a free hole line enters some (possibly very complex) structure from above, and another hole line leaves below. In contrast, the second term comprises a similar structure with particle lines. This is depicted in Fig. Explicitly expressing the one-body term in particle and core labels using topological notation, the first term is an object where a free hole line enters a complex structure from above and another hole line departs below. This structure, labeled as an object, can be depicted as a rectangle with stumps indicating where the hole line is to be attached. A similar structure exists for the second term, but with particle lines. In Eq., there are additional terms involving both particle and hole lines, which we will disregard for the following discussion. Here, we aim to prove that for a given CC diagram for matrix elements, all particle and hole lines can be dressed as depicted in Fig. [Fig: DressingGeneral]. We begin by demonstrating that for a given combs (C) diagram, which comprises matrix elements, we can dress all particle and hole lines in accordance with Figure 1. This process is called dressing, and our subsequent discussion will not consider particle and hole terms. Starting from a base diagram obtained through a specific set of contractions in Equation 58, we'll first restrict our focus to a subset of terms restricted by Equations 59, 60, and 61. In the context of the terms specified by equations @xmath58, which are subject to the constraints detailed in @xmath59, @xmath60, and @xmath61, a particular subset can be evaluated. Specifically, within this subset, products are formed using the operators @xmath62 and @xmath63, as well as the @xmath1-operators @xmath64. Within this group, there are a total of @xmath66 ways to select @xmath2 pairs of operators from the two sets. This is due to the fact that there are @xmath67 binomial coefficients in this process. Once the pair of operators is chosen within this group, there are @xmath68 ways to contract these operators into pairs, as detailed in fig. [fig: dressinggeneral]. By considering all possible factors outlined above, it is possible to recover the original factor in front of the seed diagram. After selecting two strings of mathematical operators from the input, a total of 68 distinct paired combinations can be formed. Then, the resulting objects are contracted into a single chain, as shown in Figure 1 [fig : dressinggeneral]. This chain, composed of 68 possible combinations, ultimately leads to the recovering of the original factor before the seed diagram. In essence, we can define a dressed particle-line insertion [fig: dressinggeneral] as the product of a single particle line entering and another exiting the object, denoted by a subscript p - p, such as in Eq. 1:\n\n@$\n\\textup{dressing} = g_1\\cdot c_1\\cdot g_{\\alpha_1, \\beta_1}\\cdot c_{\\alpha_1, \\beta_1}\\cdot \\cdots\\cdot g_{\\alpha_n, \\beta_n}\\cdot c_{\\alpha_n, \\beta_n}\\cdot g_2\n@$\n\nwhere the terms $g_i$, $c_i$, $g_{\\alpha_j, \\beta_j}$ and $c_{\\alpha_j, \\beta_j}$ represent the respective internal propagators, convolution integral over the spectral function, and the product of the spectral functions before and after the insertion. Notably, no numerical factors are present in front of the terms of this series, as derived from the previous analysis. In the context of the cluster operator with single and double excitations, the dressing operation involves iteratively solving an implicit equation to generate a series for dressed insertions into all particle-hole lines, including inner lines of the original \"bare\" object. The absence of numerical factors in front of terms in this series follows from the preceding statement. Using this scheme, the hole-line insertion reads @xmath76 with a subscript denoting that we keep an insertion with a single incoming and a single outgoing particle line. This series can be generated iteratively by solving an implicit equation, as demonstrated in the case of a cluster operator with simultaneous dressing of all particle-hole lines. The above derivation can be extended to incorporate the simultaneous dressing of all particle-hole lines within a given diagram, not just the outer lines of the original \"bare\" object. Here, we present our dressing scheme for clusters with single and double excitations. The hole-line insertion, with a truncated cluster operator, can be written as:\n\n@xmath76\n\nwhere we introduced anti-symmetric quantities @xmath78.\n\nDiagrammatically, this can represented in Fig. 1. The particle-line insertion is similarly defined:\n\n@xmath77\n\nIn the all-order scheme, we dress all particle and hole lines according to the method provided in the beginning of this section and in Fig. 2. Note that this also includes the dressings of inner particle and hole lines within a given object.\n\nWe discuss the exchange contribution, denoted by the expression @xmath79 as discussed in section one, while implementing these dressing corrections. Diagrammatically, as discussed in the first part of this section, we can apply the all-order dressing scheme to all particle and hole lines, as shown in Fig. [fig:sddressedinsertions], where the exchange operation is also included. Algebraically, we can solve Eq. iteratively, as depicted in Eq. [xmath82], where the dressed core cluster amplitudes are computed using [xmath81], which involve coefficient [xmath84] obtained from the previous step. In order to compute algebraically dressed core cluster amplitudes, denoted as @xmath83, we first solve Eq. @xmath82 iteratively. The coefficients @xmath84 at the previous step are used to obtain dressed amplitudes, which are then used to upgrade LCCSD diagrams from Fig. [Fig: zsdrepresentative] to Fig. [Fig: zsdrepresentativedressed]. This process involves inserting the calculated dressed amplitudes, represented as @xmath85, into LCCSD diagrams to enhance their accuracy. To delve deeper into this topic, let us introduce dressed matrix elements denoted as $\\mathcal{M}_{\\text{dressed}}$ and dressed valence cluster amplitudes labelled as $\\mathcal{A}$ and $\\mathcal{A}^{'}$, similar to Equation @xmath89 and @xmath90. In contrast to the valence amplitudes, $\\mathcal{A}$ and $\\mathcal{A}^{'}$, the incoming valence line does not carry a dressed form as it does not represent a free end. By incorporating these elements, we can dress the LCCSD diagrams as demonstrated in Fig. [fig: zsdrepresentativedressed]. The incoming valence line in the valence amplitude and valence amplitude dressed representation is not considered a free end, since it does not represent a free particle. By utilizing this concept, we may dress line - dressed matrix elements based on Eq. mentioned in the reference. Figure [fig: zsdrepresentativedressed] demonstrates four dressed diagrams computed numerically, while the remaining LCCSD diagrams, listed in Ref. are still to be treated. To calculate the dressed diagrams in high precision, we numerically computed the four diagrams shown in Fig. 1 according to our rigorous method. In the remaining LCCSD diagrams, listed in ref. @xcite, we replaced the bare matrix elements with the dressed matrix elements, as provided by Eq. - in ref. @xcite. It should be noted that the dressing of the Hartree-Fock diagram already includes the LCCSD diagrams labeled @xmath93 in ref. @xcite, hence those diagrams are not necessary in the present approach. \n\n[fig: zsdrepresentativedressed]\n\nFigure 1: The four dressed diagrams computed in the current method. In this section, we continue to systematically dress the coupled-cluster diagrams for matrix elements with a focus on the two-body term based on the product structure presented in eq. [Eq. ]. Previously, we established that the dressing of the Hartree-Fock diagram subsumes all lccsd diagrams, rendering these diagrams unnecessary in the present approach. This realization is highlighted by the fact that the dressing noticeably incorporates all lccsd diagrams in accordance with ref. [Ref.]. We will delay discussing the numerical results until [sec: numerics], and in this section, we systematically explore the two-body term of the product structure presented in eq. [Eq.] for our matrix element analysis. Continuing with the systematic dressing of the coupled-cluster diagrams for matrix elements based on the topological structure of product @xmath0, we now focus on the two-body term of this object. By inserting a two-particle/two-hole part @xmath94 and @xmath95, using a symmetry property @xmath96 and @xmath97, rpa-like chain diagrams are generated. Identical analysis to that presented in section [ sec : particlehole ] leads us to introducing an RPA-dressed object @xmath98 and @xmath99, where subscript \"rpa\" specifies that the objects inside the brackets are to be contracted to result in a free particle/hole ends similar to the original bare object @xmath100. The sign of the leading term is chosen to result in a product with a particle-hole object like @xmath101 to result in the original object. Further analysis identical to that presented for particle-hole dressing in section [ sec: particlehole ] introduces RPA-dressed objects [xmath98, xmath99], where subscript RPA represents contracting the internal objects to obtain a dressed object with free particle/hole ends like the original bare object, eq. . . The sign of the leading term is chosen to give a product of this term with a particle-hole object like [xmath101] resulting in the original object. An implicit equation for the RPA-dressed particle-hole insertion [xmath98] can be solved iteratively and the bare object [xmath103] is represented by a wavy line. The resulting insertion can dress any particle-hole vertex of a diagram as shown in [fig: rpadressingeqn]. This iterative method can be used to solve [fig : rpadressingeqn], which involves representing the bare object @xmath103 through a wavy line. The result of this [fig : rpadressingeqn] leads to the possible insertion @xmath98, as demonstrated in [fig : rpadressingvertex]. To provide an example, consider the dressing of particle-hole matrix elements of operator @xmath10, represented by @xmath104. The equation for the dressed matrix element @xmath105 can be derived through the use of eq. for the dressed object @xmath107, as shown in eq. [fig : rpadressingvertex]. Consider, for instance, the dressing of particle-hole matrix elements of the operator  \\(\\hat{O}(x)\\) as an example. The equation for a dressed matrix element can be derived as follows:\n\n\\[\\tag{1} \\hat{O}^{\\text{dressed}}_{pn}(q) =\\hat{O}_{pn}(q) + \\frac{1}{i}\\hat{V}^{\\text{ph}}_{m^{\\prime}p}(q)\\frac{1}{i}\\hat{V}_{qm}\\hat{O}^{\\text{dressed}}_{nm}(q) \\]\n\nHere, eq. (1) is derived using eq. as specified and the dressed object in the equation below:\n\n\\[\\tag{2} V_{\\mathsmaller{nn}\\mathsmaller{pp}}^{\\text{RPA}} = V_{\\mathsmaller{nn}\\mathsmaller{pp}}^{\\text{Coul}} + V_{\\mathsmaller{nn}\\mathsmaller{pp}}^{\\text{RPA, Coul}} + \\sum_{\\mathsmaller{ml}} V_{\\mathsmaller{nm}\\mathsmaller{lp}}^{\\text{Coul}} V_{\\mathsmaller{lp}\\mathsmaller{mq}}^{\\text{RPA, Coul}} V_{\\mathsmaller{mq}\\mathsmaller{nq}}^{\\text{Coul}}V_{\\mathsmaller{nq}\\mathsmaller{mp}}^{\\text{Coul}} \\frac{V_{\\mathsmaller{mp}\\mathsmaller{np}}^{\\text{Coul}}}{E_{n\\mathsmaller{p}}(q) - E_{m\\mathsmaller{p}}(q) - E_{l\\mathsmaller{q}}(q) - E_{mq}(q) + i\\eta }.  \\]\n\nWhen applying this to the traditional RPA formulae for dressed matrix elements (see e.g., Ref. @xcite ), we do not couple \\(\\hat{O}^{\\text{dressed}}_{nm}(q)\\) and \\(\\hat{O}^{\\text{dressed}}_{lq}(q)\\). In this context, the role played by the residual Coulomb interaction in traditional RPA formulae is that of the matrix elements of the dominant second-order Coulomb interaction (see section [ sec : iv ]). Here, \\(p The coupling between @xmath109 and @xmath110 should not be enforced, in contrast to traditional RPA formulae, where the coulomb interaction plays a significant role. Instead, the role of the coulomb interaction is replaced by the matrix elements of @xmath111 object, which are dominated by second-order effects (see section [IV]). It is essential to note that the line-dressed matrix elements, as introduced in eq. , also encompass matrix elements between core and virtual orbitals, distinct from RPA-like matrix elements, defined in eq. . . This derivation is limited to dressing a solitary isolated vertex. In addition to matrix elements between core and virtual orbitals, there are also RPA-like matrix elements that need consideration. However, these differ from the RPA-like matrix elements, as seen in Eq. . ., which is only valid for dressing a single, isolated vertex. A more complex scenario arises when a horizontal cross-cut through a diagram generates several unquenched particle and hole lines, not just the two lines present in Fig. [ Fig ]. In such a situation, when dressing the object @xmath111, the lower particle and hole lines can become attached to unquenched particle and hole lines in any order, resulting in a \"cross-dressing\" effect. To illustrate this concept, consider the RPA-dressing of the valence double contribution to the wavefunction @xmath112. In the RPA-dressed picture, the lower particle and hole lines of an object at position xmath111 can be connected to the unquenched lines of the bare diagram in any order, resulting in \"cross-dressing\" effects. This can be seen as an illustration in the case of the valence double contribution to the wavefunction at xmath112. For instance, if we cut the diagram shown in Fig. [Fig: zsdrepresentative](a) horizontally, the valence double contains two equivalent vertexes at xmath114 and xmath115. Figure (a) shows the horizontal cut of a valence double, containing two equivalent vertices at positions xmath114 and xmath115. To attach the RPA object to this double, it can be connected first to xmath116 vertex followed by the second step of attaching it to xmath117 vertex. However, this specific scenario is not covered by equation xmath118, as it implies that all RPA insertions are bound to the same vertex. However, this particular scenario is not explicitly captured by the standard EQ framework, which assumes that all RPA insertions are attached to a single vertex. Nevertheless, the RPA-like dressing can still be carried out in a straightforward manner. Following the illustration, the RPA-dressed valence double amplitude can be defined as outlined in the following equation (for comparison, see Eq.   ): @xmath118 \\times \\right . Here, the product may be written as displayed in (compare to Eq. 118)\n\n$\\displaystyle\\times\\right.\\sum_{mna} \\rho_{mnva} a^\\dagger_m a^\\dagger_n a_a \\left. \\right)_\\mathrm{val.~double}$ \n\n, where $\\rho_{mnva}$ represents a specific matrix element and $a^\\dagger_m$, $a^\\dagger_n$, and $a_a$ are creation and annihilation operators. In equations denoted with [ ] ] in mathematical notation, the subscript \"val . double\" in Equation (\\ref{eq : genrpadressvaldouble}) indicates that the selected contribution has the same free ends as the right-hand side of the equation. This term represents a summation of the product of density matrix element $\\rho_{mnva}$ and three fermionic creation operators $a_{m}^\\dagger$, $a_{n}^\\dagger$, and $a_{a}$ in valence space. The double subscript denotes that this particular calculation is being performed at twice the physical energy scale. In selecting a contribution with identical free ends as the right-hand side of an equation, double quotes indicate this choice. The numerical factors in front of each RPA contribution are determined in a manner similar to the treatment of dressing factors (as outlined in section [sec: particle-hole]). By examining the structure of the above expression, we arrive at a series of diagrams (dressed valence doubles) @xmath120. This example can be easily generalized to several equivalent vertices. Explicitly, we focus on a chain of diagrams, starting with @xmath119, and examining the structure of the above expression. By following this process, we ultimately arrive at @xmath120, or we have demonstrated how to dress valence doubles. This example can be expanded to various equivalent vertices. To continue our discussion on the RPA-like dressing, we specialize our presentation to the cluster operator, solely considering single and double excitations in the CCSD approximation. In this case, we obtain the bare RPA-like insertion as depicted in @xmath121, but we must be cautious when dealing with the first, singles @xmath122 singles term. This term is represented by a disconnected diagram and can lead to undesirable disconnected diagrams during matrix element calculations, as illustrated in eq . . In this discussion, we delve deeper into the application of the RPA-like dressing to a cluster operator truncated at single and double excitations, known as the coupled cluster single and double (CCSD) approximation. The bare RPA-like insertion, denoted by $\\tilde{G}_{0}(\\omega)$ or eq. @xmath121 , is a crucial component of this approach. While this operator can be represented as a disconnected diagram and provide desirable results for matrix elements, one needs to exercise caution when handling the singles-singles term, represented by a disconnected diagram.\n\nBy replacing the CCSD insertion, eq. @xmath123 , into the expression for the dressed matrix elements and the RPA-dressed valence doubles in eq. @xmath124 , we immediately derive the expressions for both. The substitution, as expected, eliminates the disconnected diagrams and improves the accuracy of the final output. As an essential component of this theoretical framework, the RPA-like dressed amplitudes have significant applications in the realm of correlated wave functions and serve as a valuable tool in quantum chemistry. To carry out this calculation, we begin by generating the dressed matrix elements and RPA-dressed valence doubles, starting from equation [eq. xxx], through which we deduce expressions for them. However, for brevity, we overlook a small contribution arising from the product of core singles, which can be found in a subsequent order beyond MBPT. It is noteworthy that this contribution was discussed in section [sec: iv]. In terms of diagrams, it can be seen that the dominant LCCSD diagrams are displayed in Figure [fig. xxx]. This contribution, as discussed in section [iv], arises in the higher orders of many-body perturbation theory, specifically in the sixth order. Particularly, among the dominant lccsd diagrams presented in Figure [zsdrepresentative], the particle-hole vertex appears solely in the RPA diagram @xmath125. To improve this diagram, it is necessary to consider the dressed matrix elements and upgrade the @xmath125 diagram accordingly, as shown in @xmath126. This upgrade does not accurately reflect the physics, however, because it skips over the dressing of the @xmath128 vertex. In fact, numerical investigations reveal that dressing both the vertexes, as in the more general equation, is equally important for an accurate result. In this specific diagram, denoted as the \"upgraded\" @xmath125, represented by @xmath126, it becomes apparent that using RPA-dressed matrix elements to enhance the diagram, as seen in @xmath127, is not equivalent to the original result. This is because the dressing of the @xmath128 vertex in this upgraded diagram is disregarded, leading to inaccurate outcomes. Our calculations demonstrated that both vertexes should be dressed, providing equally important contributions to the outcome. Furthermore, with the use of an RPA-like dressing scheme in our proposed method, CCSD calculations will capture the entire chain of RPA diagrams. However, calculations of the wavefunction using the linearized version of the coupled-cluster equations, such as those presented in references @xcite, will still be deficient in certain areas of the RPA diagrams. Therefore, to sum up, we have highlighted the significant differences in the diagram's outcomes that arise due to the RPA-dressed matrix elements. Incorporating the linearized version of the coupled-cluster equations in calculations of wavefunctions, as used in references @xcite, leaves a portion of the RPA diagrams incomplete. A fully consistent treatment of the RPA sequence necessitates the inclusion of CCSD non-linear terms in the CC equations. Coupled cluster methods can be easily connected with direct, order-by-order many-body perturbation theory (MBPT). For univalent systems, starting from the frozen-core Hartree-Fock potential, the lowest-order contributions to the double excitation cluster amplitudes are calculated through matrix elements. The lccsd method retrieves all third-order diagrams for matrix elements, but starting from the fourth order of MBPT, diagrams are no longer recovered. The coupled cluster method can easily be connected with direct order-order many-body perturbation theory, particularly for univalent systems. In cases where calculations are carried out from the frozen-core Hartree-Fock potential, the lowest-order contributions to double excitation cluster amplitudes involve the matrix elements of the beyond-the-hf potential. This can be expressed as @xmath129 for @xmath130 in refs. @xcite . However, it is worth mentioning that the lowest-order coupled cluster singles and doubles (LCCSD) method recovers all third-order diagrams for matrix elements but starts missing diagrams in the fourth order of MBPT. The missing 1,648 complementary IVth order diagrams are detailed in refs. @xcite . Notably, the diagrams complementary to LCCSD matrix element contributions comprise seven terms belonging to class @xmath131 in ref. In reference @xcite, there are seven diagrams that are complementary to the LCCSD matrix element contributions due to non-linear terms in the expansion of the CCSD wavefunctions (refer to class @xmath131). These diagrams provide the lowest-order approximation for the dressing scheme proposed in the present work. The topological structure of these @xmath131 diagrams is examined in Fig. [Fig: vsivthorder]. In terms of the proposed dressing scheme, these @xmath131 diagrams provide a low-order approximation for the calculation. The topological structure of these diagrams is explored in Fig. [fig: vsivthorder] to illustrate their various ways of lowest-order dressing of the @xmath132 diagrams. It is noted that this dressing approach recovers five out of seven of the ivth-order diagrams, including three line-dressed and two RPA-dressed diagrams. The missing diagrams are depicted in the bottom row of Fig. A comparison reveals that the current dressing approach recovers five out of seven IVth-order diagrams, including three line-dressed and two RPA-dressed diagrams. However, the missing diagrams, labeled \"stretched\" and \"ladder\" diagrams due to the structure of highlighted dressing insertions, can be observed at the bottom row in Fig. [Fig: VsIVthOrder]. These diagrams originate from untreated two-body contributions for the ladder diagram, as illustrated in eq. (eq. 3.54 in the cited reference), and from more complex three-body contributions for the stretched diagram. While their dressing can be performed using techniques similar to line- and RPA-like dressing schemes studied in this paper, their derivation lies beyond the current analysis as highlighted in Ref. In this line- and rpa-like dressing method, the insertion of two elements can be done in a comparable manner. This approach is not within the scope of the current analysis. The coulomb interaction in these diagrams is represented by horizontal lines. The topological objects originating from the lowest order approximation denoted by @xcite in ref. are highlighted. The upper three diagrams correspond to the leading order of the line dressing scheme, while the middle two are related to the rpa dressing scheme. Horizontal lines in the diagrams denote Coulomb interactions, and the lowest-order approximation to @xmath0 highlights topological objects [Fig. : VsivthOrder]. The upper three diagrams are part of the leading order of the line-dressing scheme, while the middle two diagrams relate to the RPA-dressing scheme. Our dressing formulae reproduce the pertinent ith-order MBPT expressions explicitly presented in Ref. @xcite. In Table [Tab: CS-HFS-IV] we display individual contributions to the magnetic-dipole hyperfine structure constant @xmath133 for the ground state of @xmath134cs and the principal @xmath135 transition in cs, as well as the e1 transition amplitude for the principal transition. Analyzing this table, we conclude that both line and RPA-like dressings are equally important. However, for the specific matrix elements considered, there is a partial cancellation between the line- and RPA-dressed diagrams, and omitting either would lead to a misleading result. In Table [tab: cshfsiv], numerical values for the contributions to the magnetic-dipole hyperfine structure constant (hfs) at @xmath133 and E1 transition amplitude for the principal @xmath135 transition in @xmath134cs are presented. By analyzing the table, it is concluded that both line and RPA-like dressings are of equal importance. It is worth mentioning that there is a partial cancellation between the line and RPA-like dressed diagrams, indicating that omitting either of these dressings could result in misleading results. Moreover, the untreated \"ladder\" diagram only contributes a negligibly small fraction of the total. On the other hand, the size of the untreated \"stretched\" diagram indicates that it is as significant as the other contributions. The calculations were carried out using relativistic b-spline basis sets, as described in Ref. The extended ``unstretched'' diagram holds comparable significance regarding the hfs constant to the RPA-like and line-dressed diagrams according to the current investigation. For the fifth-order calculations, a basis set of 25 positive-energy pseudo-eigenfunctions for each partial wave, chosen from 30 positive-energy states, was utilized. In addition, partial waves from a lower energy range were incorporated into the basis. The summation over intermediate core orbitals involved only the eight highest energy core orbitals for the E1 amplitude and all core orbitals for the hfs constant. These calculations were conducted using relativistic b-spline basis sets as described in a previous reference. In the basis employed in the aforementioned work, partial waves were incorporated. To calculate the e1 amplitude, the summation over intermediate core orbitals was limited to only the eight highest energy core orbitals, while for the hfs constant, all core orbitals were included. A more detailed description of the code used for IVth-order calculations can be found in Paper @xcite. The individual contributions to the ldd type and core line, particle line, valence line, RPA - i, RPA - ii, stretched, ladder, and total diagrams are listed with the corresponding values in units of negative third power of energy (-3 []) to facilitate calculations. When comparing numerical results for dressed all-order LCCSD diagrams (as presented in section IV) with the pertinent IVth-order diagrams, it was confirmed that they were consistent with each other. The lccsd diagram, denoted as @xmath3 hfs constant, is modified by line and rpa-like dressings in accordance with high-order calculations for dressed all-order lccsd diagrams. Considering contributions to @xmath3 hfs constant, line dressing modifies the @xmath132 diagram by @xmath138 mHz, which is in good agreement with the value of @xmath139 mHz, the sum of the first three corresponding IVth-order diagrams in Table [ tab : cshfsiv ]. Similarly, rpa-like dressing modifies the @xmath132 diagram by @xmath140 mHz, while the sum of IVth-order rpa diagrams in Table [ tab : cshfsiv ] has a value of @xmath141 mHz. Overall, the numerical results for dressed all-order lccsd diagrams are consistent with the values for the pertinent IVth-order diagrams. According to the given text, the line dressing modifies the @xmath132 diagram by @xmath138 MHz, which aligns well with a value of @xmath139 MHz. This modification is in agreement with the sum of the first three corresponding IVth-order diagrams from Table [Tab: CSHFSiv] . Similarly, RPA-like dressing modifies the @xmath132 diagram by @xmath140 MHz, whereas the sum of IVth-order RPA diagrams from Table [Tab: CSHFSiv] is @xmath141 MHz. This section has focused on developing the formalism of line- and RPA-like dressing for coupled-cluster diagrams regarding matrix elements. In reducing our general formalism to the case when the cluster operator contains single and double excitation amplitudes, the author has confirmed that for the lowest order they recover the relevant fourth-order diagrams both analytically and numerically. (Wikipedia-like English)\n\n<!-- The syntax of the text is changed to a more formal and standardized Wikipedia-like English. The introductory phrase is removed as the main idea of the text has already been made clear by the section title. Each sentence is rephrased for clarity and consistency. --> In this section, we demonstrate the application of our full-order dressing framework through numerical results. Namely, we have conducted relativistic calculations for the hyperfine structure (HFS) constant at the $\\rm 4^2S^{1/2}$ state and the electric dipole transition amplitude for $\\rm 6^2P^{1/2}$ state in the $\\rm ^{134}Cs$ atom. It is noteworthy that the matrix elements of the hyperfine interaction and electric dipole operator enable one to assess the quality of _ab initio_ wavefunctions both near the nucleus and at intermediate values of electronic distance. These tests are crucial for appraising the theoretical uncertainty of calculations of parity-nonconserving (PNC) amplitudes. We reduced our general formalism to the scenario in which the cluster operator is limited to single and double excitations. We have also validated that in the lowest order, our framework recovers the relevant fourth-order diagrams both analytically and numerically. Notably, matrix elements of hyperfine interaction and electric dipole operator serve as a means to evaluate the quality of _ab initio_ wavefunctions near the nucleus and in intermediate regions of electronic coordinate. These tests are critical for assessing theoretical uncertainties in calculations of parity-nonconserving (PNc) amplitudes, which are essential for high-accuracy probes of new physics beyond the Standard Model of elementary particles with atomic parity violation. The _ab initio_ PNc amplitudes are crucial for atomic parity violation investigations. The presented results are detailed in tables [Tab: HFSummary] and [Tab: dSummary]. In these tables, we augment our previous all-order calculations  @xcite by integrating two new contributions: (i) all-order RPA- and line- dressing, as outlined in sections [Sec: particlehole] and [Sec: rpa], and (ii) complementary fourth-order contributions, thereby presenting complete results up to the fourth order of perturbation theory. The results of calculation are presented in tables [ Tab: hfssummary ] and [ Tab: dsummary ] which include augmented results through the fourth-order perturbation theory. The data includes two types of new contributions: (i) all-order RPA- and line-dressing, outlines in sections [ Sec: Particle-hole ] and [ Sec: RPA ], respectively, and (ii) complementary fourth-order contributions, so that the results are complete through fourth-order perturbation theory. The hfs constant includes the most recent values of Breit and radiative corrections, which are also presented in the tables. The results for LD and +DHF are as follows: Ld & + DHF & 1425.29, Coulomb & 2283.1 + Breit @xcite & 4.6 + vp+se @xcite & -9.7 + LCCSDpt reference & 2278.0.\n\nThese contributions are augmented with: line-dressing & -11.0, RPA-dressing & 4.4, dressing total & -6.7, Triples & + 17.7, X[144] & -5.5, X[145] & -3.0, ladder & + 0.1, +Ziv & Total of all terms & + 9.3, Final ab initio & 2280.6, and experimental results & 2298.2. The LD and DHF results after the dressing total and triples' contributions are: Ld & + DHF & 5.278, LCCSD@xcite & 4.482, LCCSDpt reference & 4.558.\n\nThe dressing corrections total & -0.001, and stretched and ladder contributions are: Stretched & 1.0[-4], Ladder & -8.6[-6], and Ziv total & -0.029. The final result after all contributions is: Final ab initio & 4.528. The results include an approximation using LCCSDpt (perturbative triples) with a value of 4.5097(45) for one set of calculations, a value In a recent study, the linearized coupled-cluster approach was employed with singly and doubly excited states extracted from the reference Slater determinant. This method, commonly referred to as CCSD(T), involves calculation of single and double excitations, with additional perturbative triple excitations. The results of this perturbative CCSD(T) calculation deviate from the findings of a reference coupled-cluster computation that also considers triple excitations, but with a full CCSD(T) formulation. Notably, the CCSD(T) method includes corrections for both line and r-dressing, triples, stretched, ladder, and Ziv terms, as highlighted in the experimental and theoretical data presented. The perturbative CCSD(T) approach represents a notable departure from the CCSD(T) calculations referenced in previous literature, exhibiting differences in both absolute and relative terms. The linearized coupled-cluster (LCCC) calculations presented here include singles and doubles excitations from a reference Slater determinant. To further enhance the accuracy, a perturbative treatment of the effect of triple excitations was also incorporated in accordance with the lccsdpt method, as described in reference @xcite. Consequently, the computed valence removal energies are complete up to the third order of perturbation theory. The lccsdpt method provides a significant improvement of around 3% in the accuracy of the determined hyperfine constants over the lccsd values. The crucial outcome of implementing this perturbative approach is that the obtained valence removal energies are comprehensive up to the third-order perturbation energies. Notably, this results in a remarkable boost (approximately a few percent for CS) in the precision of the calculated lccsdpt hyperfine constants compared to lccsd values. Despite this improvement, lccsdpt yields notably greater discrepancies (as high as 1.3%) in comparison to experimental e1 amplitudes than the lccsd calculations, which only differ by 0.4% or 0.03%. In summary, both lccsd and lccsdpt calculations are unsuitable for determining parity-non-conserving (PNC) amplitudes in @xmath134cs with a margin of error of a few 0.1%. It is a crucial objective of this current research to establish a method capable of providing consistent accuracy for both hyperfine constants and dipole matrix elements (and hence PNC amplitudes) in this atomic system. However, both the LCCSD and LCCSDPT approaches are not optimal for calculating parity-non-conserving amplitudes in @xmath134cs with an estimated uncertainty of a few 0.1%. To address this issue and establish a consistent accuracy for both hfs constants and dipole matrix elements (which are necessary for determining pnc amplitudes), this paper's objective is to establish a new method. In order to achieve this, we first solved the relativistic LCCSDPT equations as described in reference @xcite and obtained computed cluster amplitudes. These computed amplitudes were utilized to calculate LCCSDPT matrix elements, which were then compared and verified against results published in reference. To begin, we first addressed the relativistic lccsdpt equations as described in Reference [@Xcite], where we successfully computed the cluster amplitudes. By applying these computed cluster amplitudes, we calculated the lccsdpt matrix elements, generating results matching those published in a previous study. Next, we proceeded to solve the line-dressing equations and computed the corresponding line-dressed cluster amplitudes and matrix elements. The convergence rate was remarkably fast, requiring only four iterations to stabilize the norms of the line-dressed cluster amplitudes at a level of a few parts per million. Conclusively, we resolved the iterative equation for the RPA-dressed valence amplitudes, Equation [Eq.], but this proved to be a computationally intensive part of the scheme. Consequently, we chose to iterate the equations only once. After four iterations, the convergence rate of the line-dressed cluster amplitude stabilized to a few parts per million. Subsequently, we focused on solving the iterative equation for the RPA-dressed valence amplitudes, which proved to be quite computationally intensive. However, we were only able to iterate through this equation once. By utilizing the calculated RPA-dressed valence amplitudes in equation *xmath113*, we were able to tackle the dominant diagram. In remaining diagrams involving particle-hole matrix elements, we applied RPA-dressed matrix elements, obtained through a numerical iterative solution of equations *xmath146*. This required only a few iterations to converge at a level of seven significant figures. \n\n(*xmath113* and *xmath146* are formulae presented as equations in the original content). In the subsequent figures that contain particle-hole matrix elements, we implemented rpa-dressed matrix elements, as opposed to bare ones. Employing a numerical iterative solution for these rpa-dressed elements, based on equations presented in ref., converged quickly to five significant figures. In Table [tab: cslinedress], we demonstrate the significance of line dressing by comparing the differences between line-dressed and bare lccsdpt diagrams in terms of the hyperfine constant and E1 amplitude. These dressings, of the leading order hf diagram, account for the lccsd diagrams at @xmath3 and @xmath4, eq .  . A direct calculation of these diagrams resulted in 89.4 MHz for @xmath1 and @xmath150 AU. In table [tab:cslinedress], the significance of line dressing is highlighted. The table shows a comparison between the line-dressed and bare LCCSDPT diagrams for hyperfine constant and E1 amplitude. Calculating the diagrams directly yields values of 85.4 MHz for A(3) and 6.4 au for E1 amplitude, respectively. These values align with the modifications to the hf diagram via dressing, resulting in 97.3 MHz and 6.4 au, respectively. Consistent values for modifications to the A(132) diagram can be found in section [sec:IV] and table [tab:cshfsiv] due to the incorporation of fourth-order diagrams. The line dressing correction for hyperfine splittings is significant in this case. The observed values in the hf diagrams are consistent with those expected from dressing-induced modifications, as seen in the data presented in Table [tab: cslinedress] for \u03bc_H = @xmath151 mHz and a.u., respectively. This is in accordance with the modification of the @xmath132 diagram, as detailed in Section [sec: iv] and Table [tab: cshfsiv]. The most significant dressing correction for the HFS constant comes from the diagram @xmath153, which is a fifth-order diagram based on Brueckner orbitals (self-energy or core polarization effect). Notably, for the $E1$ amplitude, the line-dressing correction is primarily dominated by the fourth-order contribution, specifically @xmath132. The significant contribution of the diagram under consideration can be attributed to its association with Brueckner orbitals, also known as self-energy or core polarization effects. Specifically, the E1 amplitude's dominant line-dressing correction is primarily influenced by the fourth-order term represented as @xmath132. However, for the hyperfine structure (HFS) constant, a relatively small line-dressing correction emerges due to the cancellation of comparatively large contributions from dressing the core, particle, and valence lines of the diagram (as demonstrated in Table [tab : cshfsiv]) . In contrast, when examining the discrepancy between the line-dressed (all diagrams) and bare values (bottom line of Table [tab : cslinedress]), the role of dressing the HFS diagram is negligible, as most of the contributions originate from diagrams included within the LCCSDPT values. In the bottom line of Table [tab: cslinedress], we introduce a distinction between the line with dressed valence amplitudes (all diagrams) and the bare values. Although the dressing of the hf diagram plays a negligible role here, it still contributes at a sizable 0.5% level to the hfs constant and 0.2% level to the e1 amplitude. The LDD type, consisting of LD1, LD2, LD3, LD4, and LD5 diagrams, contributes at a level of -22 and -1.7[-3 ] to the line dressing. The LD2 diagram alone contributes 9.3[-3 ] to the line dressing. The contribution arising from the rpa-like dressing, which is numerically dominant, is -8.6[-3 ] for the LD3 diagram using rpa-dressed valence amplitudes. The total contribution to the line dressing from all diagrams, excluding the rpa-like dressing, is -11 and 8.0[-3 ]. In this context, the numerically dominant four-point diagram that contributes a dressing effect similar to RPA is the ladder diagram involving particle-hole matrix elements for @xmath132. This induced correction is significant, accounting for 0.2% for both the dipole amplitude and the hfs constant. However, the dressing effect occurring in diagrams beyond @xmath132 only plays a relatively minor role, contributing at a level of merely 0.01% for both cases. It's worth mentioning that the fourth-order LCCSDPT method omits certain many-body diagrams, which would need to be considered for accurate calculations of matrix elements starting from the fourth-order of MBNPT. The dressing of particle-hole matrix elements, particularly beyond second-order perturbation theory, plays a relatively minor role in contributing only 0.01% for both test cases. This is highlighted by complementary fourth-order diagrams, which include triple and disconnected quadruple excitations beyond the LCCSDPT method. These corrections have been classified in previous work, where they are categorized into (i) indirect effects that impact single and double excitations, (ii) direct contributions to the matrix elements themselves, and (iii) corrections to normalization. More specific categorization reads as follows: (i) $z_{0 \\times 3}(d_v[t_v ]) + z_{0 \\times 3}(s_c[t_c ] ) + z_{0 \\times 3}(d_v[t_c ] )$, (ii) direct contribution via $z_{1 \\times 2}(d_{\\text{nl}})$, and (iii) corrections to normalization. These corrections are classified based on two factors: indirect and direct effects, as well as lumping single and double excitations into class @xmath157. A more refined classification involves distinguishing between valence and core triples, with similar notation for singles and doubles. The notation @xmath165 $]$ represents the effect of core triples on valence doubles through an equation for valence doubles. The lccsdpt method merges diagrams from @xmath167 and @xmath168 classes.\n\nThis breakdown yields the following correction formula:\n\n  \\begin{gathered}\n  z_{0 \\times 3}(d_v[t_v]) + z_{0 \\times 3}(s_c[t_c]) + z_{0 \\times 3}(d_v[t_c]) + z_{1\\times 2}(d_\\mathrm{nl})+ z_{0 \\times 3}(d_\\mathrm{nl}) + z_\\mathrm{norm}(t_v) \\label{eq:correction-formula}\n  \\end{gathered}\n\n  where @xmath161 $] and @xmath162 $]$ represent valence and core triples, respectively, and @xmath163 $] and @xmath164 $]$ represent singles and doubles, respectively. The notation @xmath161 refers to the effect of valence triples (@xmath161) on valence doubles (the subscript $\\mathrm{nl}$ indicates non-linked, non-linked refers to the fact that neither the singly occupied orbital nor the occupied orbital participate directly in the effect) through an equation for valence doubles (refer back to equation @xref{eq:correction-formula}). The formulation combines diagrams from @xmath167 and @xmath168 classes through the lccsdpt method. In terms of notation, the mark @xmath165 represents the effect of core triples based on the valence doubles, as depicted by an equation that combines diagrams from classes @xmath167 and @xmath168 in the lccsdpt method. Previously included diagrams from these four-tuple classes have been removed from Tables [tab: hfssummary ] and [tab: dsummary ] for consistency. The contributions of disconnected quadruples are represented by diagrams @xmath169, and one of these contributions, @xmath170, is a lower-order approximation to our all-order dressing scheme, as discussed in Section [sec: iv]. Tables [tab: hfssummary ] and [tab: dsummary ] now include contributions from both untreated \"stretched\" and \"ladder\" diagrams of the @xmath170 and @xmath171 classes. Solving for the full (not linearized) CC equations would provide additional contributions of the latter class. According to Section IV, diagrams that contribute independently in disconnected quadruples are evaluated and analyzed in this study. Specifically, one of these contributions, @xmath170, provides the lowest-order approximation for the all-order dressing scheme. Tables [Tab: hfssummary] and [Tab: dsummary] include the contributions of untreated \"stretched\" and \"ladder\" diagrams from both the @xmath170 class and the @xmath171 class. The latter contribution would have been accounted for by solving the full (non-linearized) Chiral Cumulant (CC) equations. This study employed a large-scale fourth-order calculation, the code for which was described in Ref. @xcite. All formulas for various diagrams were automatically generated using symbolic algebra tools. The resulting fourth-order corrections from triples are at the level of 1%, but noticeable cancelations are observed among different diagrams. In generating formulas and code for a vast array of diagrams, symbolic algebra tools were employed. Fourth-order corrections from triples yield results at a 1% level, but cancelations between certain diagrams significantly affect the overall outcome. Such observations indicate that attaining higher levels of theoretical accuracy necessitates a complete all-order treatment of triple interactions. Details of such a calculation for the hyperfine constant are presented in Table [tab: hfssummary], which sheds light on the importance of correlation effects.  \n<|user|>\nCan you provide a summary of the key findings from the table presented, regarding the correlation effects and the hyperfine constant? The hyperfine constant values presented in Table [tab:hfssummary] were calculated by considering various effects, including the Bohr-Weiskopf effect, Breit and radiative corrections. To clarify the role of correlations, these small but significant effects were incorporated into the reference value. The \"lccsdpt, coulomb\" value was calculated with consideration for the finite nuclear size, which accounts for 0.5% of the result during both wavefunction determination and hyperfine interaction matrix element computation. Additionally, Breit corrections from Ref. were also included. In our computations to determine wavefunctions and compute matrix elements of hyperfine interaction, we have considered the finite nuclear size, which accounts for approximately 0.5% of the `` lccsdpt , coulomb '' value. Additionally, we have included Breit corrections, referenced in Ref. @xcite. These corrections differ significantly from those in Ref. @xcite due to substantial correlation corrections. Moreover, recent calculations by Ref. @xcite determined radiative corrections to magnetic-dipole hyperfine-structure constants for the ground state of alkali-metal atoms, which indicate that vacuum polarization and self-energy contribute up to 0.4% to the _ ab initio _ value. In high magnitude order, significant correlation corrections have been determined for the radiative corrections to the magnetic-dipole hyperfine-structure constants for the ground state of alkali-metal atoms. These investigations, conducted by Xcite, revealed that both vacuum polarization and self-energy (VP+SE) comprise approximately 0.4% of the ab initio value, with their contribution being of similar magnitude. However, it is noteworthy for readers to be cautious when employing Breit values derived from Ref. Xcite, as these values do not encompass correlation corrections (as explained and referenced in Ref. Xcite). The LCCSD(PT) reference, which is free of dressing effects, deviates by 0.9% from the experimental value. The partial cancellation of dressing corrections results in a total contribution of 0.3%. Hence, correlation corrections play a critical role in producing accurate results for this field. Following calculations in the literature at various orders, the final value deviates by only 0.9% from the experimental data. Dressing corrections partially cancel, resulting in a total dressing contribution of 0.3%. Fourth-order diagrams, which are complementary to those already included in the LCCSDPT value, are dominated by a contribution due to triple excitations, at 0.8%. Despite almost all correction terms having similar sizes but different signs, dressing and IV-th order corrections mostly cancel, leading to a correlation correction of only 0.1%. This slight improvement in theory-experiment agreement is observed when compared to the LCCSDPT reference value. These findings were obtained through _ab initio_ calculations. We also incorporate the \"stretched\" and ladder \"iv\" order diagrams in our study that our current dressing scheme has failed to capture. As mentioned in section [sec: iv], these missing diagrams significantly affect the correlation corrections, which are generally of opposite signs, causing cancellation and leaving the final correction with only 0.1% deviation from the \"lccsdpt reference\" value. However, our _ab initio_ value for the hfs constant slightly deviates by 0.8% from the experimental value. The details of the calculation for the dipole matrix element are comprehensively documented in Table [tab: dsummary]. Certainly, I can paraphrase that paragraph for you.\n\nThe electric-dipole transition amplitude is calculated and detailed information is presented in table [tab: dsummary]. Radiative and Breit corrections are not included in this analysis, as the Breit interaction only contributes 0.02% and radiative corrections are not yet available from literature. There have been several high-accuracy experimental measurements of the @xmath173 matrix element, which are listed at the bottom of the same table. \n\nNo introductory sentence is necessary, as the previous paragraph likely provided a sufficient context. Numerous experimental determinations have produced precise values for the matrix element, denoted by @xmath173. A table listing these results can be found in [Tab: dsummary]. Ref. [@xcite] utilized photoassociative spectroscopy of cold ces atoms, a technique that relies on a high-accuracy measurement of molecular potentials, to extract this matrix element from the measured lifetime of the @xmath174 state. The determination of the dipole matrix elements in this study is based on photoassociative spectroscopy of cold cesium atoms. Measurements of their molecular potentials with high accuracy allow for inferred extraction of these values. Another approach has been proposed in a previous reference where the static electric-dipole polarizability and van der Waals coefficient of the ground molecular state are utilized. The enhanced sensitivity of these properties to the principal transition matrix elements has been exploited to extract individual matrix elements. The extraction was facilitated by a high accuracy ratio of the two dipole matrix elements measured in a previous reference. Employing the heightened sensitivity of ground state electric-dipole polarizability and van der waals coefficients, the present study utilized the matrix elements of primary transitions to extract individual matrix elements with exceptional accuracy. The proposed method, introduced in prior investigations, enabled the determination of the matrix element for transition @xmath173 from the highly accurate van der waals coefficient data obtained in earlier work. This method's effectiveness is evident through the precision of these measurements in refs. @xcite and @xcite. In previous works, the @xmath173 matrix element has been determined using high-accuracy measurements in references @xcite and @xcite, with an inferred value from measured data in the latter. The most accurate value comes from photoassociation spectroscopy  @xcite, where the result has a 0.03% accuracy level. This value will serve as a calibration point for _ab initio _ calculations, which will be compared to measured results. The reference LCCSDPT value for the e1 matrix element deviates from the high-accuracy measurements by as much as 1.3%. In high-accuracy photoassociation spectroscopy, the most accurate matrix element has a 0.03% accuracy. This precise value will be used for calibrating _ab initio_ calculations. Remarkably, the reference LCCSDPT e1 matrix element deviates from high-accuracy measurements by a substantial 1.3%. The correlation corrections, including dressing and fourth-order terms, significantly improve the agreement by almost 0.6%, making the _ab initio_ accuracy comparable to that of the hyperfine structure constant. Table [Tab: dSummary] reveals that the dressing corrections almost cancel out line- and RPA-like dressing corrections for this transition amplitude, resulting in a negligible effect overall. Triple excitations beyond LCCSDPT triples, on the other hand, have a very large impact, almost 1%. Notably, residual fourth-order RPA corrections also have a considerable effect, tending to decrease the impact of triples. According to an analysis presented in Table [tab : dsummary], the dressing's overall effect on this transition amplitude is negligible due to the cancellation of line- and rpa-like dressings. However, the forth order corrections attributable to triple excitations beyond LCCSDPT triples are highly significant, up to 1%. Additionally, residual fourth-order RPA corrections also affect this outcome, with decreasing effects. The present work's fourth-order calculation shows that a complete treatment of triple excitations beyond LCCSDPT improves the accuracy of _ab initio_ transition amplitudes. The first two major results of this study are: (i) the development and application of an all-order dressing formalism; and (ii) the first calculation of matrix elements for CS complete up to the fourth order of many-body perturbation theory, built upon the hierarchical expansion of the product of clusters into a sum of 2-body insertions. The principal outcomes of this research involve the development and utilization of a complete all-order dressing framework for matrix elements calculated by the coupled-cluster method. Our method extends a hierarchical expansion of a product of clusters into a sum of two-body insertions, including particle/hole-line insertions from the one-body part and two-particle/two-hole RPA-like insertions due to two-body interactions. We have successfully demonstrated how to \"dress\" these insertions and have derived iterative equations for them. Specific attention has been given to the popular singles-doubles truncation of the full cluster operator, and we have formulated the corresponding dressing equations for this approximation. Two major outcomes of this work involve the creation of this dressing framework and the computing of matrix elements for CS up to fourth-order many-body perturbation theory, the first of its kind. In this work, we showcased the process of \"dressing\" insertions through iterative equations. Specific focus was given to the popular singles-doubles truncation of the full cluster operator, and we formulated corresponding dressing equations. To upgrade coupled-cluster diagrams for matrix elements with the dressed insertions for univalent systems, we highlighted relationships to pertinent fourth-order diagrams. We applied our formalism to relativistic calculations for the cs atom, presenting a large number of fourth-order diagrams complementary to the lccsdpt method (linearized coupled-cluster single-doubles method with perturbative treatment of triples; it is the most sophisticated cc approximation applied in relativistic calculations for cs so far). In our recent study, we demonstrated our formalism through relativistic calculations for the cesium atom, including a multitude of fourth-order diagrams beyond the traditional LCCSDPT theory. This augmented CC approach, known as the linearized coupled-cluster single-doubles method with perturbative treatment of triples (LCCSD(T)), represents the most sophisticated CC approximation applied for Caesium thus far. Our results, obtained through fourth-order many-body perturbation theory, significantly improved the theory-experiment agreement for the electric dipole transition amplitude and provided slightly better results for the hyperfine constant. We encountered substantial cancellations among fourth-order contributions and determined that a comprehensive treatment of triple and disconnected quadruple excitations would further enhance the theoretical accuracy. In our analysis, we discovered that the incorporation of complementary diagrams significantly enhances the correspondence between theoretical predictions and experimental measurements for the electric-dipole transition amplitude of a prominent transition, namely @xmath4. Additionally, this analysis yielded milder but still notable improvements in the agreement for the hyperfine constant. The cancellation of several fourth-order contributions was identified as a notable observation. However, a comprehensive account including triple and disconnected quadruple excitations is considered desirable to achieve enhanced theoretical accuracy.",
        "abstract": " we consider evaluation of matrix elements with the coupled - cluster method . \n such calculations formally involve infinite number of terms and we devise a method of partial summation ( dressing ) of the resulting series . \n our formalism is built upon an expansion of the product @xmath0 of cluster amplitudes @xmath1 into a sum of @xmath2-body insertions . \n we consider two types of insertions : particle / hole line insertion and two - particle / two - hole random - phase - approximation - like insertion . \n we demonstrate how to `` dress '' these insertions and formulate iterative equations . \n we illustrate the dressing equations in the case when the cluster operator is truncated at single and double excitations . using univalent systems as an example , we upgrade coupled - cluster diagrams for matrix elements with the dressed insertions and highlight a relation to pertinent fourth - order diagrams . \n we illustrate our formalism with relativistic calculations of hyperfine constant @xmath3 and @xmath4 electric - dipole transition amplitude for cs atom . finally , we augment the truncated coupled - cluster calculations with otherwise omitted fourth - order diagrams . \n the resulting analysis for cs is complete through the fourth - order of many - body perturbation theory and reveals an important role of triple and disconnected quadruple excitations . ",
        "section_names": "introduction\ncoupled-cluster formalism for univalent systems\ngenerating object @xmath0\ndressing particle and hole lines\nrpa-like dressing\ncomparison with the iv-order diagrams\nnumerical results and discussion\nconclusion"
    }
]