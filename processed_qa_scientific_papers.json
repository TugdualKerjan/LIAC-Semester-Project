[
    {
        "article": "The interest in studying anchoring phenomena and liquid crystal textures in confined structures, particularly in twisted nematic liquid crystal cells, has significantly grown due to their potential applications in liquid crystal display devices. These cells consist of a nematic liquid crystal sandwiched between parallel, planar surfaces that impose perpendicular easy directions but different anchoring conditions. The external electric or magnetic field tunes the nematic director's orientation. The importance of uniform surface alignment for the device's functionality is emphasized, as non-uniformities caused by surface treatments like rubbing are common.\n\nPrevious research mainly focused on nematics in contact with uniform substrates. However, when inhomogeneities arise naturally from surface treatments, the nematic texture near the surface becomes non-uniform, with a characteristic decay length that is typically much smaller than the thickness of the cell. This non-uniformity can be effectively smeared out, leading to optical properties resembling those of a uniform substrate. The influence of chemically or geometrically patterned substrates on the liquid crystal's phase behavior is an active area of research, as it offers insights into interfacial forces and phase transitions.\n\nRecent advancements have demonstrated the potential of substrates with large periodicities, micrometers or larger, in creating novel electro-optical devices. These devices utilize chemically patterned surfaces to create small regions with tailored nematic director orientations, which can be manipulated electrically. The study of these systems not only contributes to technological development but also provides opportunities to investigate fundamental phenomena like effective elastic interactions and phase transitions in nematics.\n\nContinuum theories, initially developed by Oseen and Zocher, play a crucial role in understanding the static properties of nematic liquid crystals. However, dealing with patterned substrates presents numerical challenges due to the broken symmetry. A different approach is taken in this paper, exploiting the finite decay length of the surface pattern's effect on the nematic liquid crystal. By determining the anchoring energy and surface director orientation, an effective free energy functional is derived that accurately captures the phase behavior of the system, even in cases with arbitrary cell widths and anchoring angles.\n\nIn summary, the study of anchoring in confined nematic liquid crystals, particularly in twisted nematic cells, is driven by their potential applications in display technology. Non-uniformities on the surface arise naturally and are crucial for device functionality. Patterned substrates have emerged as a promising area of research, offering unique opportunities to explore interfacial effects and phase transitions in nematics. Continuum theories are employed, but numerical methods are necessary to A new generation of electro-optical devices relies on nematic liquid crystals with patterned director orientation, achieved through chemically patterned confining surfaces. These devices, such as flat-panel displays with wide viewing angles, utilize sub-pixels with distinct nematic director orientations, which can be controlled electrically. The study of nematic liquid crystals interacting with non-uniform substrates offers insights into elastic forces and phase transitions, but theoretical calculations face challenges due to the broken symmetry caused by the surface patterns.\n\nContinuum theories, although useful, require intricate numerical simulations due to the need for two- or three-dimensional grids to account for the substrate patterns. Determining metastable states and energy barriers, crucial for understanding bistable devices, is particularly difficult in these cases. Instead, the authors propose a novel approach that exploits the finite decay length of the surface pattern's effect on the nematic liquid crystal. They develop an effective free energy function by considering the anchoring energy and average surface director orientation, and compare its results with those obtained from the original free energy functional.\n\nThe Frank theory, a fundamental framework in liquid crystal physics, describes the nematic director and its distortions. The elastic constants, including splay, twist, and bend, govern the free energy cost of these distortions. When surfaces are present, the surface free energy adds to the bulk term, and anchoring conditions at the boundaries determine the director alignment.\n\nThe authors apply this effective free energy approach to a system of a nematic liquid crystal confined between a patterned substrate and a flat one. By minimizing the effective free energy function for a single small cell at a specific mean distance, they can predict the phase behavior, energy barriers, and anchoring angles for arbitrary cell widths and upper surface anchoring angles. This method significantly simplifies the numerical analysis compared to directly minimizing the original free energy functional on a two-dimensional grid.\n\nIn conclusion, the study of nematic liquid crystals in contact with patterned substrates, while theoretically complex, can be effectively addressed using an effective free energy model that captures the essential features of the system's behavior. This work provides a promising avenue for understanding and designing novel electro-optical devices with tailored properties. In this study, the authors present a novel method to investigate metastable states and energy barriers in bistable nematic devices, overcoming the challenges posed by determining these critical features in traditional approaches. By exploiting the finite decay length of the surface pattern's impact on the nematic liquid crystal, they develop an effective free energy function that accurately captures the behavior of the system. This function accounts for the anchoring energy and the average surface director orientation, providing a simplified representation for the nematic cell's free energy.\n\nThe continuum theory for liquid crystals, established by Oseen and Zocher, forms the foundation of their analysis. The Frank theory, which deals with the nematic director and its distortions, is a crucial tool in understanding the static properties. The elastic constants, such as $k_{11}$, $k_{22}$, and $k_{33}$, represent the costs associated with splay, twist, and bend distortions, respectively. The presence of surfaces introduces additional surface free energy, which is essential for the total free energy balance.\n\nThe anchoring at the surfaces, described by Rapini-Papoular's expression, influences the director orientation. The authors consider a system with a patterned lower substrate at a distance from a flat upper substrate, with the latter imposing strong homeotropic anchoring. They develop an effective free energy function based on the average surface director orientation from the patterned substrate, enabling a more manageable numerical analysis compared to the original full free energy functional.\n\nThrough numerical simulations, they find remarkable agreement between the phase diagrams obtained from the effective free energy and those derived from the original free energy functional. This demonstrates the effectiveness of their approach in predicting the phase behavior of the nematic liquid crystal under the given conditions.\n\nThe challenge lies in determining stable and metastable configurations and calculating energy barriers between them, particularly in cases with large cell widths. By mapping the free energy functional onto the effective one, the authors bypass the computational difficulties and can explore the phase behavior and anchoring angles for arbitrary cell widths and anchoring angles at the upper surface.\n\nIn conclusion, the authors' innovative strategy offers a practical method for understanding the complex dynamics of bistable nematic devices by simplifying the analysis of the system's free energy landscape. This work contributes to the ongoing advancements in the field of liquid crystal physics and device engineering. In a nematic liquid crystal, the uniform configuration is considered thermodynamically stable due to the system's ability to relax to its original state after external forces distort it. The stability is described by an elastic free energy functional, which takes into account splay, twist, and bend distortions of the nematic director. The functional depends on the material properties, specifically the elastic constants that change with temperature, and is augmented by surface contributions when interfaces are present.\n\nSurface free energies, often described by the Rapini-Papoular form, are added to the bulk energy to account for anchoring effects at the boundaries. Anchoring conditions, ranging from free (no anchoring) to strong (fixed orientation), dictate the director alignment at the surfaces. For a patterned substrate with a sinusoidal grating and alternating homeotropic and planar anchoring, the effective free energy functional is derived by minimizing the total energy, considering the one-constant approximation for simplicity.\n\nThe phase behavior of the nematic liquid crystal in this system is found through numerical solutions of the Euler-Lagrange equations, which arise from the stationary conditions of the total free energy. Depending on the anchoring angles and the mean separation between the substrates, the system exhibits two stable or metastable phases: the homeotropic (h) phase with a uniform director parallel to the upper surface and the hybrid aligned nematic (han) phase with a varying director orientation. As the anchoring angle decreases, the han phases become stable for certain cell widths, leading to distinct textures with positive and negative average surface angles at the lower patterned substrate. Understanding these phase transitions and the anchoring effects is crucial for designing and controlling the optical properties of nematic liquid crystal devices. In the context of nematic liquid crystals confined between patterned substrates, the study focuses on weak and strong anchoring conditions at the boundaries. When there's no anchoring, the bulk free energy is minimized using classical variational techniques. For strong anchoring, the surface free energy is minimized while keeping the boundary values fixed. In the weak anchoring scenario, the total free energy, including the surface term, must be minimized.\n\nThe surface free energy is typically described by the Rapini-Papoular expression, which depends on the anchoring strength 'w(r)' and the unit vector perpendicular to the surface, $\\hat{\\bf{n}}\\cdot\\nu^2$. The anchoring strength 'w(r)' influences the molecular alignment; negative values favor perpendicular orientation, while positive ones favor degenerate planar arrangements. For a specific system with a nematic liquid crystal between a sinusoidally grating lower surface and a flat upper surface, the anchoring strengths vary periodically, creating distinct anchoring conditions for homeotropic and planar regions.\n\nBy considering the one-constant approximation, the total free energy functional is derived in Eq. (4), incorporating the extension, surface profile, and anchoring constants. The Euler-Lagrange equations are solved numerically to find stable and metastable director fields. However, the presence of the patterned lower surface introduces a challenging numerical problem due to the varying anchoring directions and complex director configurations.\n\nTo overcome this difficulty, an effective free energy method is employed, which maps the original system's free energy to an effective function that depends on the average surface director orientation at the patterned surface. By minimizing this effective function, the phase behavior, energy barriers, and anchoring angles for arbitrary cell widths and anchoring angles can be calculated. This method is shown to be reliable when the inversion condition in Eq. (7) holds.\n\nThe analysis begins with a homogeneous flat-substrate system, where the effective free energy function matches the minimized free energy function. As the system evolves to include the chemically patterned surface, the phase behavior displays the existence of two stable phases: the homeotropic (H) phase and the hybrid aligned nematic (HAN) phase, with distinct director configurations depending on the anchoring angles and cell separation. The comparison between the effective free energy method and direct minimization confirms its accuracy in predicting the phase boundaries. The lower substrate in the system under study exhibits a periodic pattern along the @xmath25 axis, either geometrical or chemically, and is translationally invariant in the @xmath26 direction. The total free energy functional, given by equation \\ref{eq4}, considers the effects of gradients in the director field, surface anchoring, and the extension @xmath28 in the @xmath26 direction. The anchoring at the upper surface is strong, while the lower surface features a surface pattern with a period @xmath24, with alternating homeotropic and planar anchoring stripes.\n\nThe system's behavior is accurately captured by analyzing the effective free energy function, derived from equation \\ref{eq6} and discussed in detail later, which simplifies the numerical analysis significantly. This is particularly useful when dealing with large cell widths @xmath35 and arbitrary anchoring angles @xmath40 at the upper surface. By mapping the original free energy functional onto the effective one, the phase behavior, energy barriers, and effective anchoring angles can be determined for various substrate separations @xmath35 and anchoring angles @xmath40.\n\nFor instance, when the lower surface is purely sinusoidal with alternating grooves and homeotropic anchoring (figure 1(a)), the effective free energy function allows for the prediction of stable and metastable configurations. In the case of a nematic liquid crystal confined between a chemically patterned surface and a strong homeotropic upper substrate (figure 1(a)), the phase diagram shows the coexistence of the homeotropic (h) and hybrid aligned nematic (han) phases, with distinct textures depending on the average surface anchoring angle at the lower surface.\n\nThe reliability of the effective free energy method is verified by comparing its predictions with those obtained directly from minimizing the full free energy functional, as seen in figures 2(a) and (b). These diagrams demonstrate the transition between different nematic phases and highlight the agreement between the two approaches, further validating the usefulness of the effective free energy representation in understanding the complex phase behavior of nematic liquid crystals in structured substrates. The study considers a nematic liquid crystal confined between a structured lower surface with alternating homeotropic and planar anchoring, as depicted in figure [fig1]. The surface grating features a groove depth of x_math34 and a period twice that of the surface structure (x_math24). To accurately predict the phase behavior at arbitrary mean distances (x_math35), the effective free energy function, derived from equations (eq6-eq8), is employed. This simplified analysis bypasses the numerical challenges associated with the complex director field and energy barriers encountered when dealing with large cell widths.\n\nTo achieve this, the authors start by examining a single, small cell with a single anchoring angle at the upper surface (figure [fig1]b) and minimize its free energy. From this minimum, they map the effective surface free energy function (eq41), which incorporates the lower surface's anchoring energy, as a function of the average surface director orientation (x_math42). By inverting equation (eq7) to obtain the director angle (x_math52), they calculate the phase behavior, energy barriers, and effective anchoring angles for varying cell widths and upper surface anchoring angles (x_math35 and x_math40).\n\nFor the case of a chemically patterned sinusoidal surface and a strong homeotropic upper substrate, the phase diagram shows the existence of two stable phases: the homeotropic (h) phase and a hybrid aligned nematic (han) phase. The effective free energy method accurately predicts these phase transitions, as demonstrated by comparing its results with those obtained from directly minimizing the original free energy functional (figure [fig2]b). The first-order phase transition between the h and han phases, despite the monostable planar anchoring, is observed, a phenomenon previously reported for specific conditions using an empirical expression.\n\nIn summary, the effective free energy approach offers a computationally efficient means to model the complex phase behavior of a nematic liquid crystal in contact with structured substrates, providing valuable insights into the director field and energy landscapes in these systems. In the context of studying the behavior of nematic liquid crystals in cells with chemically patterned surfaces, numerical methods are employed to overcome the challenges posed by the complex director field and determining the phase diagram, particularly for large cell widths. The difficulty arises due to the patterned lower surface, which complicates the calculation of the director field. To address this, the authors propose mapping the free energy functional onto an effective surface free energy function, where the average surface director orientation is defined. By minimizing the free energy for a single, small cell with arbitrary anchoring at the upper surface, they derive explicit expressions for the director and effective surface energies.\n\nThe effective free energy method is valid under the condition that an inversion of Eq. (7) is possible, which relates the input surface director orientation to the effective anchoring. This method significantly reduces the computational complexity compared to minimizing the original free energy functional on a two-dimensional grid. However, the applicability of the effective free energy approach relies on the invertibility of Eq. (7).\n\nThe authors further illustrate the effectiveness of their method by comparing their calculated phase diagrams, which show stable and metastable configurations, with those obtained through direct minimization of the original free energy functional. For a system with a sinusoidal patterned surface and a flat strong homeotropic substrate, they find two distinct nematic phases: the homeotropic (h) phase and a hybrid aligned nematic (han) phase. The phase transitions between these phases, despite the effective surface preferring monostable anchoring, are found to be first-order, as revealed by the analysis of the effective free energy function.\n\nThis study underscores the importance of numerical techniques in understanding the intricate behavior of nematic liquid crystals in patterned substrates and highlights the versatility of the effective free energy approach in overcoming the challenges associated with determining phase diagrams and energy barriers. The study focuses on a nematic liquid crystal confined between a chemically patterned sinusoidal surface and a strong homeotropic flat substrate, as depicted in Figure 1(a). The pattern, characterized by alternating stripes with different anchoring strengths, leads to the formation of two stable or metastable director configurations: the homeotropic (h) phase and a hybrid aligned nematic (han) phase. By employing the effective free energy method, the phase diagram is constructed as a function of the anchoring angle at the upper substrate and the mean separation between the substrates (Figure 2(b)). This method shows good agreement with direct minimization of the original free energy functional, validating its reliability.\n\nFor small anchoring angles, the han phases exist for cell widths greater than a critical value, but as the distance between the substrates decreases, the strong homeotropic anchoring at the upper surface stabilizes the h phase, making the han phases unstable. The first-order phase transition between the h and han textures, despite the monostable nature of planar anchoring, is observed due to the effective free energy favoring this configuration. This phenomenon, previously predicted for specific cases with bistable surface energies, can be found in nematic devices with monostable anchoring conditions.\n\nIn cases where the inversion condition for the effective surface free energy (Equation (7)) is not met, as illustrated in Figure 3, the effective free energy function cannot be accurately calculated, and alternative approaches, such as numerical simulations or experimental measurements, become necessary to understand the system's behavior. In the scenario of a nematic liquid crystal confined between a chemically patterned sinusoidal surface and a flat substrate with strong homeotropic anchoring, the effective free energy function derived from the Euler-Lagrange equations (Equations (6) to (8)) accurately matches the minimized free energy function of the original system (Equation (12)). This alignment allows for the description of the system's behavior using the simplified effective free energy, which takes into account the surface profile and anchoring conditions.\n\nFor this specific case, the sinusoidal surface profile is defined by Equation (15), with groove depth @xmath34, period @xmath66, and a periodic step function for the anchoring strength (Equation (18)). The numerical results shown in Figure 2(a) exhibit two distinct nematic director configurations: the homeotropic (H) phase with a uniform director field parallel to the upper surface, and the hybrid aligned nematic (HAN) phase, which exhibits a variation from planar to homeotropic across the cell.\n\nThe phase diagram in Figure 2(b) reveals that for small anchoring angles at the upper substrate, HAN phases are stable for cell widths larger than a critical value. As the distance between the substrates decreases, the stability of HAN phases is compromised due to the dominant strong anchoring. The agreement between the phase boundaries calculated using the effective free energy method and those obtained by direct minimization of the underlying free energy functional further validates its reliability.\n\nThe study also points out that, despite the effective surface free energy favoring monostable planar anchoring, a first-order phase transition between the H and HAN textures occurs. This is in contrast to previous predictions for a monostable lower substrate, where a first-order transition can happen under specific conditions. The nature of the transition depends on the surface free energy near the minimum, allowing for first-order phase transitions even with a monostable surface.\n\nIn cases where the inversion condition for the effective surface free energy (Equation (7)) is not met, alternative methods, like the one employed with the chosen parameters in Figure 3, can still provide insights into the phase behavior for sufficiently large cell widths. This demonstrates the versatility of the effective free energy approach in understanding nematic liquid crystal systems with complex confinement geometries.",
        "abstract": " we study the phase behavior of a nematic liquid crystal confined between a flat substrate with strong anchoring and a patterned substrate whose structure and local anchoring strength we vary . by first evaluating an effective surface free energy function characterizing the patterned substrate we derive an expression for the effective free energy of the confined nematic liquid crystal . \n then we determine phase diagrams involving a homogeneous state in which the nematic director is almost uniform and a hybrid aligned nematic state in which the orientation of the director varies through the cell . \n direct minimization of the free energy functional were performed in order to test the predictions of the effective free energy method . \n we find remarkably good agreement between the phase boundaries calculated from the two approaches . \n in addition the effective energy method allows one to determine the energy barriers between two states in a bistable nematic device . ",
        "section_names": "introduction\neffective free energy function\napplications\nsummary"
    },
    {
        "article": "Galaxy clusters, being the largest concentrations of matter in the cosmic density field, play a crucial role in astrophysics and cosmology. Theoretical studies, grounded in large-scale N-body simulations, accurately predict cluster abundance and clustering patterns across different cosmological models. These predictions are particularly valuable due to the accessibility of clusters as markers of large-scale structure, allowing researchers to impose stringent constraints on both cosmological parameters and the growth of cosmic structures.\n\nThe primary challenge in cluster cosmology lies in establishing a direct connection between the observable baryonic structures and the dark matter halos predicted by theory. This requires a sophisticated understanding of the complex interplay between baryons and dark matter within clusters, which is being advanced through numerical simulations. Observationally, efforts are underway to compile extensive cluster samples using various techniques, given the richness of their multi-wavelength signatures.\n\nClusters were initially detected in the 18th century as distinct groups of bright galaxies. Optical surveys, with their relatively low cost, have historically been the primary source of large cluster catalogs, as they can detect lower mass systems. However, early optical detection was marred by projection effects due to line-of-sight projections. The advent of precision CCD photometry has significantly reduced this issue by enabling the selection of clusters based on spatial, brightness, and color correlations. X-ray observations, with their sensitivity to the square of the density, have also become crucial, offering a higher contrast and immunity to projection effects, although contamination from non-thermal sources can be a challenge at low spatial resolution.\n\nX-ray surveys, over the years, have contributed to the construction of numerous cluster catalogs, with much of our current knowledge of cluster physics derived from these observations. Comparison between optically and X-ray selected catalogs, as well as optical and X-ray properties with weak lensing and Sunyaev-Zel'dovich (SZ) signals, helps refine our understanding of cluster dynamics and composition.\n\nThis paper focuses on measurements of X-ray properties of the MaxBCG catalog, a volume-limited optically-selected cluster sample spanning a redshift range from z=0.1 to 0.3. The MaxBCG catalog, sourced from Sloan Digital Sky Survey (SDSS) data, provides uniform optical photometry and precise photometric redshifts for all clusters, along with X-ray observations from the ROSAT All-Sky Survey (RASS). By analyzing individual cluster X-ray luminosities and the mean relation with optical richness, the study aims to address potential biases and investigate the L_X-richness correlation In the realm of astrophysics, the detection and study of galaxy clusters have significantly advanced over the centuries, starting with their initial identification as peculiar aggregations of bright galaxies in the 18th century. Optical surveys, due to their relatively low cost, have been instrumental in compiling the largest cluster catalogs, particularly due to their ability to detect even low-mass systems. However, early optical detection was hindered by projection effects, where galaxies along the line of sight could be misinterpreted.\n\nThe advent of CCD photometry revolutionized cluster detection by enabling the selection of galaxies based on their spatial, brightness, and color properties, significantly reducing projection biases. These optical surveys also provided accurate photometric redshifts, crucial for characterizing distant clusters. X-ray observations, on the other hand, offered a complementary approach with the detection of thermal emission from the intracluster medium (ICM), which is less affected by projection since it depends on squared density. However, low spatial resolution posed challenges with non-thermal X-ray sources.\n\nX-ray surveys have been instrumental in constructing numerous cluster catalogs, contributing significantly to our understanding of cluster physics. Comparisons between optically and X-ray selected catalogs, as well as the examination of their weak lensing and SZ signals, have deepened our insights into cluster properties. The MaxBCG catalog, described in this paper, is a valuable resource, being the largest optically-selected cluster sample with uniform optical photometry and precise photometric redshifts from the Sloan Digital Sky Survey (SDSS). It spans a redshift range of 0.1 to 0.3 and serves as a basis for analyzing X-ray properties.\n\nUsing the RASS data, the authors measured the mean X-ray luminosity as a function of optical richness and redshift for all MaxBCG clusters. Despite individual detections being limited due to the shallow RASS exposures, these data provide crucial information on the overall X-ray properties. The scatter in the optical richness-X-ray luminosity relation is discussed, along with potential sources of systematic biases in the analysis.\n\nBy combining X-ray measurements with dynamical data from the MaxBCG clusters, the authors derived the luminosity-shear (L-$\\sigma$) relation, an important diagnostic for understanding the mass distribution in clusters. The study underscores the importance of multi-wavelength approaches in refining our understanding of cluster physics and the evolution of the universe.\n\nIn conclusion, the MaxBCG catalog, in conjunction with the RASS data, offers a comprehensive dataset for studying galaxy clusters, allowing researchers to Based on the scientific text provided, the paragraph continuation would be:\n\n\"Utilizing the deep and uniform optical photometry from the Sloan Digital Sky Survey (SDSS) and the RASS All-Sky Survey, the X-ray properties of the MaxBCG catalog, a large optically-selected cluster sample, are thoroughly studied. Despite the RASS's shallow exposures not allowing individual detections for every cluster, the survey provides essential measurements of the mean X-ray luminosity as a function of optical richness and redshift. The low signal-to-noise ratios of individual cluster X-ray emissions support the analysis by confirming the mean luminosity and quantifying the scatter in the optical richness-X-ray luminosity relation. These measurements serve as a valuable resource for understanding cluster physics, as seen in previous studies combining optical (e.g., dynamical and weak lensing) and X-ray data. The authors compare their findings with those from the NIR selected 2MASS catalog by DKM07, which also employed RASS for stacking X-ray properties. Both datasets, MaxBCG and 2MASS, reveal a scaling between X-ray luminosity and optical richness, providing insights into the connection between the baryonic content and the underlying cluster structure.\"\n\nThis paragraph builds upon the previous one, detailing the methodology and the significance of the X-ray measurements on the MaxBCG catalog, contrasting it with the 2MASS catalog, and emphasizing the role of these studies in advancing our understanding of cluster physics. Based on the scientific text provided, the paragraph continues:\n\nDeepening our understanding of galaxy cluster properties, the mean X-ray luminosity (\\(\\mathcal{L}_{\\text{x}}\\)) as a function of optical richness (\\(\\lambda\\)) and redshift (\\(z\\)) has been extensively studied using the MaxBCG catalog. This catalog, extracted from SDSS data, combines detailed observations from both dynamical analyses (B07) and weak lensing measurements (S07, J07). By stacking the X-ray emissions from individual clusters, the MaxBCG catalog offers precise measurements with low signal-to-noise, which not only confirms the mean \\(\\mathcal{L}_{\\text{x}}\\) derived through stacking but also provides insights into the scatter in the \\(\\mathcal{L}_{\\text{x}}-\\lambda\\) relation.\n\nThe \\(\\mathcal{L}_{\\text{x}}-\\lambda\\) relationship for MaxBCG clusters is reported here, complemented by a weak lensing study presented in a separate letter (Xcite). A similar analysis was conducted by DKM07 using the 2MASS catalog, which selects clusters based on near-infrared flux limits and covers a range of masses from \\(M_{\\text{min}}\\) to \\(M_{\\text{max}}\\), with \\(\\lambda\\) quantifying the number of galaxies brighter than a specific magnitude threshold. DKM07 observed a scaling between X-ray luminosity and optical richness in their sample.\n\nIn this work, the authors describe the input data, which includes SDSS imaging, the MaxBCG catalog, and RASS X-ray data. The X-ray analysis focuses on measuring \\(\\mathcal{L}_{\\text{x}}\\) for individual clusters and determining the mean \\(\\mathcal{L}_{\\text{x}}\\) for clusters of similar richness. Section [sec:meanrelation] presents the derived mean relation, scatter, and the median underlying \\(\\mathcal{L}_{\\text{x}}-\\lambda\\) relation. Systematic biases in the results are discussed in [sec:bias], and the combination with dynamical measurements from MaxBCG is used to establish the \\(\\mathcal{L}_{\\text{x}}-M\\) relation in [sec:lsig].\n\nConclusions and future prospects are outlined in [sec:summary], highlighting the importance of these findings for understanding cluster physics and refining cosmological models. Throughout the analysis, a \\(\\Lambda\\text{CD In Section [sec:stacking_analysis], the authors delve into the process of measuring the stacked X-ray properties of the optically-selected 2Mass clusters. They employ a rigorous approach, first calculating individual X-ray luminosities for each cluster and then averaging these values to obtain the mean X-ray luminosity associated with clusters of similar richness. The relationship between X-ray luminosity and optical richness (denoted as L_x - Richness) is explored, revealing a clear correlation with a scatter in this relation. They also determine the median underlying X-ray temperature and hydrostatic masses for the stacked clusters.\n\nTo account for potential biases, Section [sec:bias_analysis] discusses various sources that could influence the results, such as selection effects and limitations in the X-ray data. By combining the X-ray measurements with dynamical data from the MaxBCG catalog (in Section [sec:lsig]), they aim to refine the L_x - Richness relation, providing a more robust estimate of the scaling.\n\nThe study's conclusions, presented in Section [sec:summary], summarize the key findings, emphasizing the importance of using optically selected clusters for X-ray studies and the success in measuring the mean X-ray properties. Future work is suggested to further investigate the cluster population, refine richness estimates, and explore the implications for cosmological models. The reliance on the Sloan Digital Sky Survey (SDSS) and the ROSAT All-Sky Survey (RASS) data sets, along with the MaxBCG catalog, underscores the collaborative nature of the research and the significance of these multi-wavelength datasets in understanding galaxy clusters. Based on the SDSS and RASS surveys, this work utilizes a red sequence selection method to identify galaxy clusters from the SDSS imaging data. The SDSS, a comprehensive project covering 10^({@math 25}) square degrees in the North Galactic Cap and a smaller region in the South, provides deep optical imaging in five bands (\\@math27-\\@math31) with photometric errors typically below 3% at bright magnitudes. The spectroscopic component targets galaxies with specific luminosities and median redshifts for detailed analysis.\n\nThe MaxBCG cluster catalog is constructed from SDSS Data Release 5 (DR5), employing the \"MaxBCG\" algorithm, which identifies clusters by analyzing the spatial clustering of red sequence galaxies and the presence of a bright central galaxy (BCG). The algorithm calculates the likelihood of a galaxy being part of a cluster and selects the redshift with the highest combined likelihood for each galaxy. Richness (\\@math6) is quantified as the number of red sequence galaxies within a certain radius of the BCG.\n\nThe X-ray data for these clusters comes from the ROSAT All-Sky Survey (RASS), providing soft X-ray maps with a broad point spread function (PSF) of around \\@math49. Using these maps, various X-ray catalogs, such as the Bright Source Catalog (BSC) and the Northern ROSAT All-Sky Galaxy Cluster Survey (NORAS), are created. The MaxBCG catalog, with a median redshift of \\@math41, is found to detect a substantial fraction of the NORAS clusters, indicating a high level of overlap and consistency.\n\nFor this study, the clusters are divided into richness bins for stacking analysis, taking into account the same bins used in previous velocity dispersion measurements. By stacking the X-ray data on the BCG positions, the mean X-ray emission from the clusters is measured precisely, despite the limited individual detections due to the short exposure times. The authors employ a method to handle field boundaries and extract overlapping RASS photons for a more robust analysis.\n\nIn summary, this work combines optical and X-ray data to study galaxy clusters, leveraging the MaxBCG catalog and the RASS surveys for cluster selection and X-ray measurements, respectively. The stacking technique is employed to analyze the stacked X-ray emission from these clusters, providing insights into their overall properties and relationships to their optical counterparts. In the process of analyzing galaxy clusters using the \"maxbcg\" algorithm, which is detailed in @xcite and presented in the catalog found in @xcite, the methodology focuses on identifying clusters through their unique characteristics. Rich galaxy clusters exhibit a distinct red sequence of early-type galaxies with a narrow color-magnitude distribution (the E/S0 ridgeline) and a central brightest cluster galaxy (BCG) located near the cluster's core. The algorithm assesses the likelihood of a galaxy being part of an overdensity of these red sequence galaxies with similar color and magnitude, as well as resembling a typical BCG, for each SDSS galaxy at various redshifts.\n\nBy evaluating these probabilities independently, the algorithm determines the redshift that maximizes the combined likelihood for each galaxy. This redshift is then associated with the cluster, providing an estimate of its redshift. Cluster centers are identified based on the highest likelihood locations, and a scaled radius of @xmath37, corresponding to a density 200 times the critical density, is used to define the core. Galaxies within this radius and brighter than 0.4 magnitudes are removed from the potential center list to avoid double-counting.\n\nThe catalog compiled contains properties such as photometric redshifts, richnesses, optical luminosities, and positions, with a total of 13,823 clusters from ~7500 square degrees observed between 0.1 and 0.3 redshifts. The center for each cluster is marked as the BCG, and the richness, @xmath6, ranges from 10 to 188. To study the connection between richness and mass, the catalog has been used in previous works to constrain cosmology.\n\nTo exploit the X-ray data from the ROSAT All-Sky Survey (RASS), the clusters' X-ray emission is stacked using a method that divides them into richness bins. The analysis starts by selecting BCGs as the reference points for stacking and weighs the photons based on the median redshift of the clusters. This approach, despite the challenges posed by the broad PSF of RASS, allows for precise measurement of the mean X-ray emission from the clusters, particularly for those with high richnesses.\n\nIn summary, the \"maxbcg\" algorithm, combined with X-ray data from RASS, provides a comprehensive catalog of galaxy clusters with detailed properties, which are then used to study their properties and relationships, such as richness-mass relations and cosmological implications The subsequent paragraph would continue the description of the data processing and analysis of the MaxBCG catalog, which is a cluster catalog derived from the ROSAT All-Sky Survey (RASS). The text explains that the catalog, containing around 13,823 clusters, is limited to a specific redshift range (0.1 to 0.3) and has a median redshift of @xmath41. It discusses the method used to define the cluster centers as the locations of Brightest Cluster Galaxies (BCGs), the richness estimation (Richness, @xmath6) based on the number of E/S0 galaxies, and the accuracy of the redshift estimates, which have been verified against spectroscopic data.\n\nThe paragraph then delves into the RASS data, which serves as the X-ray input for the analysis. It mentions the ROSAT Bright Source Catalog (BSC) and the Northern ROSAT All-Sky Galaxy Cluster Survey (NORAS), which are used to validate the MaxBCG clusters, with the catalog detecting roughly @xmath58 of NORAS objects. The ROSAT-ESO Flux Limited X-ray Galaxy Cluster Survey (REFLEX) is also mentioned, being nearly 90% complete due to improvements in RASS analysis.\n\nThe text proceeds to describe the stacking technique employed to analyze the X-ray emissions from the clusters, which involves dividing them into richness bins and averaging the X-ray signals from the BCG centers. This method compensates for the lack of individual X-ray detections due to the short exposure times. The paragraph concludes with the challenges faced in dealing with bright foreground sources and the Virgo cluster, which can contaminate the measurements, and the measures taken to address these issues.\n\nOverall, the paragraph provides a detailed account of the data collection, processing, and analysis methods used in the MaxBCG catalog, emphasizing the reliance on X-ray data from ROSAT and the rigorous steps taken to ensure the integrity of the results. The ROSAT All-Sky Survey, conducted primarily in 1990-1991, was a significant X-ray imaging campaign that covered the entire sky using the ROSAT Position Sensitive Proportional Counter (PSPC). The survey aimed to map the sky in soft X-rays (0.1-2.4 keV) with a focus on detecting and studying galaxy clusters. The dataset from this mission has been thoroughly utilized in various X-ray cluster catalogs, such as the Bright Source Catalog (BSC) and the Northern ROSAT All-Sky Galaxy Cluster Survey (NORAS), providing crucial information about cluster properties.\n\nThe RASS exposure times varied significantly, with the typical field near the MaxBCG survey region having an effective exposure time of @xmath48. The survey's broad Point Spread Function (PSF), with a full-width-at-half-maximum of @xmath49, made source detection challenging due to off-axis photons dominating the signal. Despite these limitations, the RASS data has been instrumental in creating catalogs of X-ray-selected clusters.\n\nIn this study, the MaxBCG richness, a proxy for cluster mass, is employed to analyze the relationship with X-ray properties. By combining the MaxBCG catalog with the RASS data, researchers can investigate the mean X-ray emission from different richness bins. To account for the survey's characteristics and minimize biases, a stacking technique is employed. This involves dividing the clusters into nine richness bins and scaling and weighting the X-ray photons based on the median redshift of the clusters.\n\nBefore stacking, certain clusters are excluded to ensure accurate measurements. These exclusions include those with low exposure times (<200 s), which affect ~4% of the total, and those close to extremely bright foreground sources or the Virgo cluster, which removes a negligible ~0.1% of the MaxBCG catalog. The weighted X-ray counts are calculated using the projected physical distances to the Brightest Cluster Galaxy (BCG) and a background annulus to estimate the extragalactic signal.\n\nThe results from this analysis provide valuable insights into the connection between cluster richness and X-ray luminosity, which can be used to refine our understanding of cluster evolution and the underlying cosmological parameters. Further improvements in richness estimates and galaxy populations, as well as deeper X-ray observations, will continue to enhance our knowledge of these enigmatic structures in the universe. Based on the RASS (ROSAT All-Sky Survey) data processed by ROSAT Position Sensitive Proportional Counter (PSPC), the sky was scanned in great circles, with the largest exposure time near the ecliptic poles. The effective exposure time for fields overlapping the MaxBCG survey region, which excludes the northern ecliptic pole, is typically @xmath48. The survey's point spread function (PSF) is broad, with a Full Width at Half Maximum (FWHM) of @xmath49, resulting from its off-axis photon dominance due to the scanning strategy.\n\nReprocessed photon maps and exposure maps of the entire RASS region have been made publicly available, providing the basis for the X-ray analysis in this paper. These maps are used to create catalogs, such as the ROSAT Bright Source Catalog (BSC) with @xmath50 signal-to-noise ratio and a position resolution better than the PSF FWHM, with 68% (90%) of sources within @xmath51 (@xmath52). The Faint Source Catalog (FSC) contains 105,924 sources with a lower signal-to-noise ratio, mostly unresolved.\n\nRASS data has also been employed to construct flux-limited cluster catalogs, including the Brightest Cluster Sample (BCS) with a flux limit and the Northern ROSAT All-Sky Galaxy Cluster Survey (NORAS) which lists 378 confirmed clusters. However, the ROSAT PSF's broadness limits the completeness of the NORAS catalog, being @xmath55 complete only at its stated flux limit, corresponding to a luminosity @xmath57 at a median redshift.\n\nThe _ROSAT-ESO Flux Limited X-ray Galaxy Cluster Survey (REFLEX)_ complements NORAS in the southern sky, with a similar flux limit. It is nearly 90% complete, despite having less overlap with the MaxBCG survey area. The MaxBCG clusters, with a mean exposure time of 400 seconds, lack individual detection capability but benefit from their sheer number, allowing precise measurement of the mean X-ray emission from clusters with richnesses up to @xmath59.\n\nStacking methods are employed to analyze the data, dividing the clusters into richness bins and weighting them according to the median redshift of the clusters. Background estimation is done using a fixed annulus, and weighted count rates are calculated. The stacking procedure is",
        "abstract": " determining the scaling relations between galaxy cluster observables requires large samples of uniformly observed clusters . \n we measure the mean x - ray luminosity  optical richness ( @xmath0@xmath1 ) relation for an approximately volume - limited sample of more than 17,000 optically - selected clusters from the maxbcg catalog spanning the redshift range @xmath2 . by stacking the x - ray emission from many clusters using _ rosat _ all - sky survey data , \n we are able to measure mean x - ray luminosities to @xmath310% ( including systematic errors ) for clusters in nine independent optical richness bins . in addition \n , we are able to crudely measure individual x - ray emission from @xmath4 of the richest clusters . assuming a log - normal form for the scatter in the @xmath5@xmath6 relation , we measure @xmath7 at fixed @xmath6 . \n this scatter is large enough to significantly bias the mean stacked relation . \n the corrected median relation can be parameterized by @xmath8 , where @xmath9 and @xmath10 . \n we find that x - ray selected clusters are significantly brighter than optically - selected clusters at a given optical richness . \n this selection bias explains the apparently x - ray underluminous nature of optically - selected cluster catalogs . ",
        "section_names": "introduction\ninput data\nx-ray analysis\nmean @xmath0@xmath1 relation\nbiases in the @xmath227@xmath1 relation\nthe luminosityvelocity dispersion relation (@xmath227@xmath327)\nsummary"
    },
    {
        "article": "Quantum correlations, the foundation of groundbreaking technologies like quantum information and computation, are explored in optical systems, particularly in those operating in the continuous variable regime, such as optical parametric oscillators (OPOs). These devices, which utilize nonlinear crystals, exhibit interactions between light modes, leading to the generation of squeezing and entanglement in field quadratures. Spatial degrees of freedom, including cavity modes and parts of light beams, have also gained attention for their potential quantum applications.\n\nOptical parametric oscillators, with their multiple light modes, have been successfully implemented in various tasks, including optical switching, quantum imaging, and quantum-enhanced metrology. A significant feature is the ability to manipulate quantum properties of light through spatial inhomogeneities in broad area devices. In photonic crystals (PCs), periodic modulations of the refractive index lead to photonic band gaps, enabling confinement and guidance of light with numerous practical uses. The transverse modulation of refractive index in nonlinear cavities is predicted to suppress modulation instabilities and has been experimentally verified.\n\nModulation in dissipative systems, such as PCOs, can give rise to nonlinear structures, like discrete cavity solitons. The use of PCs has also been proposed for quantum optics, where photonic band gaps can influence spontaneous emission and lead to non-Markovian effects in quantum systems. This work focuses on the effect of a transverse modulation on quantum fluctuations and correlations in a photonic crystal optical parametric oscillator, demonstrating how the presence of gaps can enhance or suppress these quantum properties.\n\nThe model considers a planar cavity with a nonlinear medium and a partially reflecting mirror, supporting a degenerate optical parametric oscillator. Below the instability threshold, the dynamics are described using linear and few-mode approximations, which enable analytical calculations of intensity fluctuations, correlations, squeezing, and entanglement. The modulation is introduced through a spatially inhomogeneous refractive index, and the study reveals how this affects the system's quantum characteristics.\n\nIn summary, the study investigates the impact of transverse modulation in a photonic crystal optical parametric oscillator on quantum fluctuations and correlations, highlighting the potential of structured media in quantum information processing and technology. The study presented in this work focuses on the influence of a transverse modulation on the quantum fluctuations and correlations in a photonic crystal optical parametric oscillator (PCopo), a nonlinear device that exhibits photonic band gaps. The PCopo, with its intracavity photonic crystal, is designed to suppress modulation instabilities and exploit the unique properties of photonic crystals for quantum optics applications. By using linear and few-mode approximations, the authors derive analytical expressions for intensity fluctuations, quantum correlations, such as squeezing and entanglement, and twin beams correlations below the parametric threshold.\n\nBelow threshold, the modulation in the device leads to a modification in the output signal field, which can be tuned by adjusting the spatial inhomogeneity. The authors demonstrate that this modulation not only tunes quantum noise but also enhances the robustness of squeezing and improves entanglement generation. The presence of the photonic crystal breaks the system's translational symmetry, giving rise to coupled mode equations that govern the dynamics of the fluctuations.\n\nThese analytical findings are validated through numerical simulations, which provide insights into the complex interplay between the modulation and the system's quantum behavior. The results highlight the potential of photonic crystals in engineering dissipation and enabling novel quantum optical phenomena in nonlinear cavities, with implications for cavity QED and the study of non-Markovian effects.\n\nIn conclusion, this work contributes to the understanding of the interplay between modulation and quantum fluctuations in photonic crystals, paving the way for further exploration of their potential in quantum technologies and quantum information processing. In this work, we delve deeper into the intriguing effects of photonic crystal optical parametric oscillators (PCOPOs), exploring analytical results beyond the numerical simulations presented in previous studies. These devices, which integrate a photonic crystal (PC) into their cavity design, exhibit unique quantum fluctuations and correlations. Below the parametric threshold, we develop a linear and few-mode approximation to the model, providing analytical expressions for intensity fluctuations, correlations, and quadrature squeezing.\n\nOur analysis reveals that the spatial modulation introduced by the PC can manipulate the instability threshold for signal emission, either raising or lowering it. This modulation significantly impacts the quantum correlations, including squeezing, entanglement, and twin beams correlations between spatial modes. We demonstrate that the PC not only enhances the noise reduction but also broadens the range of angular stability for squeezing.\n\nThe linearized equations of motion for the field fluctuations, derived from the Heisenberg equations, reveal that the far-field fluctuations are no longer independent due to the coupled dynamics induced by the PC. The presence of spatial harmonics introduces additional dynamical coupling among different modes, necessitating further approximations to handle the model analytically.\n\nTo validate our theoretical predictions, we compare our analytical results with numerical simulations of the full model, which employs a Langevin treatment to account for quantum fluctuations. The simulations confirm the agreement between the approximate and full models, particularly in understanding the behavior of the system below the instability threshold.\n\nThese findings have broader implications for nonlinear devices, as the effects observed in PCOPOs can be generalized to other devices, such as Kerr media and second harmonic generation, where spatial modulations play a role. Experimental realizations of these phenomena have already been demonstrated in the literature, showcasing the potential of these systems for quantum information processing and enhanced nonlinear optical applications.\n\nIn conclusion, our study provides a comprehensive understanding of the interplay between the photonic crystal structure and quantum fluctuations in PCOPOs, paving the way for future research on optimizing these devices for enhanced quantum correlations and manipulation. In the final section of the paper, the authors delve into the implications of their theoretical analysis on the quantum correlations generated in the PCopo. They find that the spatial modulation introduced by the intracavity photonic crystal (PC) not only influences the instability threshold but also modifies the quantum features of the emitted fields. The presence of the PC leads to the generation of squeezed, entangled, and twin-beam correlations, which are characteristic of non-classical light.\n\nBy numerically simulating the non-linear Langevin equations, they confirm the validity of their linear and few-mode approximations, demonstrating how the PC's spatial modulation can be harnessed to manipulate the intensities and quantum correlations. For instance, the authors observe that tuning the pump detuning through the PC can raise or lower the threshold for signal emission, thus offering control over the onset of instability.\n\nThe analysis also reveals that the quantum images, which are noisy precursors observed below threshold, exhibit distinct characteristics depending on whether the PC is present. In the case of the PCopo with a bandgap, the signal fluctuations remain relatively unchanged, while for modulation in the pump detuning, the signal patterns become locked and display two dephased modes, altering the nature of quantum correlations significantly.\n\nThese findings contribute to the understanding of the interplay between nonlinear optical devices and photonic crystals, paving the way for potential applications in quantum information processing and optical communication systems that exploit the unique properties of structured light. The transversal inhomogeneity of the medium filling the cavity, exemplified by the introduction of a planar photonic crystal (PC), significantly impacts the intracavity dynamics of the photonic crystal optical parametric oscillator (PCopo). This system's behavior is described by a continuous set of boson spatial modes with frequencies $\\omega_i$ and follows commutation relations. The Hamiltonian, given by Equation \\ref{eq : h}, includes terms for diffraction, interaction with the external pump, and nonlinear interactions. The unique feature of a PCopo is the spatial modulation of the cavity detunings due to the photonic crystal, breaking the translational symmetry.\n\nIn the numerical studies presented in Ref. @xcite, quantum effects below and above the instability threshold were investigated using the quantum fields dynamics in the Heisenberg picture. However, for analytical calculations, a simplified model was developed, considering the dynamics of fluctuations around a macroscopic steady state. This model involves linearizing the equations of motion and approximating the Hamiltonian to be quadratic in the fluctuations. \n\nThe authors identify two regimes: one where the PCopo is below the instability threshold and the pump field is homogeneously detuned, resulting in a vanishing signal operator expectation value. In this regime, the pump field evolves according to Equation \\ref{eq : pump_steady_state}, with a scaling factor $\\eta$ and $\\kappa$. For a non-homogeneous pump detuning, the steady-state solution becomes more complex, involving coupled mode equations for varying wave-numbers.\n\nThe full dynamics of the PCopo is studied numerically by simulating the Langevin equations derived from the master equation. This approach reveals interesting features, such as the appearance of noisy precursors in the signal field below threshold, which exhibit a spatial periodicity corresponding to the critical wave-number. The presence of the photonic crystal modifies these patterns, leading to spatially locked and phase-dephased modulated modes.\n\nBased on these numerical simulations, the authors justify the assumptions made in Equations (5-7), highlighting the relevance of considering only the pump modes at the fundamental and higher harmonics, while neglecting modes below the photonic crystal wavelength. By focusing on the dominant modes, they derive a set of coupled equations in the frequency domain, which govern the spatial quantum effects in the signal field.\n\nThese analytical calculations pave the way for understanding the correlations and behavior of the PCopo in different parameter regimes, providing insights into the interplay between the photonic crystal structure and the In the realm of quantum fluctuations and dynamics, a simplified model is introduced to derive analytical results below the instability threshold in a quantum optical parametric oscillator (PCOPO) operating with a pump field. This approach involves approximating the Heisenberg equations by considering a macroscopic steady state and linearizing the Hamiltonian around it, leading to a set of coupled equations for the quantum fluctuations. These equations, however, are unsuitable for analytical treatment due to their infinite hierarchy.\n\nTo overcome this challenge, the authors focus on the regime where the PCOPO operates below the instability limit, where the signal operator's expectation value vanishes. In this regime, the pump field evolves according to a simplified equation involving scaling factors. For a homogeneous detuning, the steady-state solution is straightforward to find, and for an optical parametric oscillator with a signal-only modulation, the steady state remains uniform.\n\nFor a more complex scenario with a non-homogeneous pump detuning, a Fourier analysis is employed to describe the coupled mode equations for varying wave numbers. The authors neglect higher harmonics, reducing the problem to a manageable four-mode system. By simulating the full PCOPO dynamics using Langevin equations, they observe distinct behaviors depending on the presence of the parametric crystal (PC), which breaks the translational symmetry and introduces modulation.\n\nThe numerical simulations reveal that for a PC with a bandgap, the signal fluctuations exhibit noisy precursors below the threshold, with a preferred spatial periodicity corresponding to the critical wave number. The introduction of the pump modulation leads to the appearance of spatially locked patterns, with two dephased modulated modes dominating the dynamics. These features have significant implications for the quantum correlations in the signal field.\n\nAnalyzing the correlations in the output fields, the authors find that the twin-beam properties, characterized by negativity of the variance and non-classical features, are significantly influenced by the presence of the PC. In the absence of PC or for specific parameter combinations, certain terms vanish, whereas they become prominent when the PC affects the signal. The resulting normalized variance is negative and constant below the instability threshold for the PCOPO, unlike the case without the parametric interaction.\n\nThese findings provide valuable insights into the quantum dynamics of PCOPOs and the role of the parametric crystal in generating non-classical light. Further analytical calculations and discussions on the most relevant spatial modes and their impact on correlations are presented in subsequent sections. In the context of the studied process, when the pump field values are below the instability threshold, the signal operator's expectation value vanishes, regardless of the presence of the parametric coupling (PCOPO). The equation for the pump field's average in this regime simplifies to \\(\\langle \\hat{E}_p \\rangle = \\alpha_0 \\cdot \\text{exp}(-\\gamma_p t) \\cdot \\sqrt{\\frac{\\Omega_p}{\\gamma_p}}\\), where \\(\\alpha_0\\) is a scaling factor, \\(\\Omega_p\\) is the pump frequency, and \\(\\gamma_p\\) is a decay rate. This scaling is used throughout the analysis.\n\nFor a homogeneously detuned PCOPO, the steady-state solution is straightforward. For an optical parametric oscillator (OPO) with a modulation in the signal detuning, the steady state remains uniform. However, for a non-homogeneous pump detuning, identifying the stationary state becomes non-trivial. In the simplified scenario considered, with one transverse dimension and a sinusoidal pump modulation, the steady-state solution in Fourier space involves coupled mode equations for varying frequencies, neglecting higher-order harmonics.\n\nThe fluctuations of the pump and signal fields are defined relative to these steady states. The pump field exhibits three non-zero modes, while the signal field remains zero in the steady state. By approximating the Hamiltonian to be quadratic in the fluctuations, the Heisenberg equations decouple, leading to a separate dynamics for the signal fluctuations.\n\nThe presence of the PCOPO introduces spatial modulation, which dynamically couples different modes due to the signal and pump detunings. This coupling results in specific patterns for the signal fluctuations, such as locked spatially-locked patterns when the pump modulation breaks translational symmetry. These patterns affect the quantum correlations in the signal field, which can be analyzed using inverse matrix calculations.\n\nThe non-classical nature of twin beams, characterized by negative variance, is evident when the PC affects the signal. For modulation only in the signal detuning, the twin beams correlations exhibit a diverging normalized variance, which corresponds to the instability threshold. Conversely, when the pump detuning is modulated, the correlations remain quantum but decrease in strength below threshold.\n\nIn summary, the study of the PCOPO dynamics below threshold involves understanding the interplay between the pump and signal fields, their fluctuations, and the resulting quantum correlations. The modulation of either detuning significantly impacts the nature of the twin beams and their quantum features. In the context of the pump and signal fields described in the scientific text, the pump and signal Heisenberg equations separate, allowing for the derivation of a dynamical equation for the signal fluctuations. This equation resembles the case where the parametric converter (PC) affects only the signal, as shown in figures (b), (c), and (d). The signal fluctuations exhibit a modulation that is similar to the pump's, with unequal amplitudes in general. The presence of the PC leads to spatial coupling among the far fields' fluctuations due to the spatial modulation of both the signal and pump detunings.\n\nThe model is simplified by dropping hats from operators and denoting the fluctuations as \\( \\delta \\alpha \\). The modulation on the signal is compared to the pump's, with the pump having a smaller external intensity than its threshold. The signal dynamics can be studied using nonlinear Langevin equations, which map the full master equation for the PCPO (parametric coupled optical parametric oscillator) to equations governing the Husimi quasi-probability distribution in phase space.\n\nWhen the pump intensity is not too high, the Husimi distribution dynamics can be described by a Fokker-Planck equation, transforming into spatially-dependent pump and signal fields. The equations involve phase-sensitive, white noises, with the spatial dependence of detunings encapsulating the PC's effects. Numerical simulations confirm the presence of noisy precursors in the signal below threshold, forming a spatial pattern with a preferred periodicity corresponding to the critical wave-number.\n\nThe analysis focuses on the most relevant modes for the pump field, neglecting higher-order harmonics below the PC wavelength. This allows for a simplified treatment of the PCopo dynamics, reducing the study to a set of four coupled operator equations in the frequency domain. The output fields' correlations are then derived, highlighting the quantum nature of the twin beams, which is characterized by negativity of the variance and singularities in the P-representation.\n\nModulation of the signal detuning, for instance, shows a transition from quantum twin beams to classical correlations below a certain threshold, whereas modulation of the pump detuning leads to a combination of coherent and depleting effects on the twin beams. Understanding these effects from a microscopic perspective involves the creation and destruction of photons between the critical tilted modes due to the spatial modulation, which influences the twin beams' correlations and their classicality. In the study of the parametric optical parametric oscillator with parametric coupler (PCopo), it is crucial to consider the dynamical coupling among different modes due to the spatial modulation of both the signal and pump detunings through spatial harmonics. The authors note that neglecting higher harmonics amounts to focusing on the fundamental mode, but they emphasize the importance of the full PCopo model to identify and analyze the relevant spatial modes in different regimes. By simulating the system using Langevin equations derived from the master equation, they observe that in the absence of strong pump intensities, the dynamics are governed by a Fokker-Planck equation, which simplifies to nonlinear Langevin equations for spatially dependent pump and signal fields.\n\nIn their numerical simulations, they find that the PCopo exhibits distinct behaviors depending on the presence of a bandgap in the pump cavity. For a signal-only modulation, the system's far fields resemble those of the ordinary OPO. However, introducing a modulation in the pump detuning leads to the generation of multiple even harmonics and a change in the pump field's structure. Surprisingly, the signal's average far field intensity remains unaffected by these changes.\n\nThe signal fluctuations below threshold exhibit unique patterns, with quantum images dominated by the critical wave-number. When the PC breaks the translational symmetry, the signal's noisy pattern becomes spatially locked, with two dephased modulated modes dominating. These variations have significant implications for the system's correlations, as shown by the twin beams' non-classical features, which depend on the presence of the PC.\n\nAnalyzing the correlations, the authors find that the twin beams' negativity and non-classical properties are affected by the PC, with the correlations becoming more classical as the modulation increases. Modulation in the signal detuning leads to a hopping process between critical tilted modes, creating photon pairs and altering the correlations. In contrast, pump detuning modulation primarily triggers secondary processes and has a reduced impact on the twin beams.\n\nIn conclusion, the PCopo system's dynamics and quantum correlations are intricately tied to the spatial modulation of the pump and signal, and understanding these effects is crucial for designing devices that generate and manipulate quantum correlated light. The analytical approximations employed in this study provide valuable insights into the behavior of the system and pave the way for further investigations in this area. The authors present numerical simulations of a parametric optical oscillator (POO) with a periodically modulated cavity (PCopo), focusing on the impact of the PC on the system dynamics. They observe that when the PC has a bandgap, the average far fields of both the pump and signal exhibit similarities to the POO without the PC, but introduce distinct features when the pump detuning is modulated. Below the parametric threshold, the signal fluctuations display quantum images with a preferred spatial periodicity corresponding to the critical wave-number, which diffuses in the transverse direction in the absence of the PC. However, when the PC breaks the translational symmetry, the signal fluctuations become spatially locked, revealing two phase-dephased modulated modes.\n\nThe study employs numerical methods to analyze the correlation dynamics, considering the most relevant pump modes and neglecting higher-order harmonics below the PC wavelength. By analyzing the output fields, the authors find that the twin beams correlations exhibit non-classical features, particularly in the presence of the PC. The correlations are shown to be sensitive to the pump and signal detuning modulations, with the strength and quantum nature depending on the parameters.\n\nIn the case of modulation only in the signal detuning, they observe a divergence in the twin beams correlations, which corresponds to the parametric threshold. For modulation in the pump detuning, the correlations remain quantum but can become more classical as the hopping rate between critical tilted modes increases. These findings suggest that the PCopo offers tunable control over the quantum properties of light by modifying the pump and signal spatial modulation.\n\nOverall, the study highlights the significant influence of the PC on the instability process, parametric threshold, and quantum correlations in the POO, enabling the manipulation of these effects for potential applications in quantum information processing.",
        "abstract": " we show how to control spatial quantum correlations in a multimode degenerate optical parametric oscillator type i below threshold by introducing a spatially inhomogeneous medium , such as a photonic crystal , in the plane perpendicular to light propagation . \n we obtain the analytical expressions for all the correlations in terms of the relevant parameters of the problem and study the number of photons , entanglement , squeezing , and twin beams . considering different regimes and configurations we show the possibility to tune the instability thresholds as well as the quantumness of correlations by breaking the translational invariance of the system through a photonic crystal modulation . ",
        "section_names": "introduction\nfew mode approximation for the pcopo\nconclusions\nlinear dynamics of pcopo with any @xmath120\nsolution of the input-output equation\nsecond order moments in frequency and time domains"
    },
    {
        "article": "Methanol masers, primarily classified into Class I and II, are commonly found in regions of active star formation. Class I masers, particularly at 36 and 44 GHz, are thought to be excited through collisions, while Class II masers, including 6.7 and 12 GHz transitions, are radiationally excited. These two classes can coexist in some sources but often exhibit distinct velocities or spatial separations. Class I masers have been associated with earlier stages of star formation compared to Class II masers like water and OH masers. However, their evolutionary timeline and relationship to these other masers are still subject to debate due to the complex environments often associated with clustered star formation.\n\nSingle-dish telescopes, with limited angular resolution, have traditionally studied Class I methanol masers, making it difficult to determine their exact locations and relationships with other maser transitions and excitation sources. High-resolution observations are crucial to clarify these connections and understand the physical conditions driving maser production in each transition. Recent advancements in radio telescopes, such as the Australia Telescope Compact Array and the Expanded Very Large Array (EVLA), have allowed for the first arcsecond-resolution images of the 36 GHz methanol masers, particularly in the DR21 star-forming complex.\n\nIn this study, the EVLA was used to observe the 36.169 GHz methanol line in the sources DR21(Oh), DR21W, and DR21N on May 26, 2010. The observations, conducted in the most compact configuration, revealed 49 36 GHz maser features, with linewidths ranging from 0.15 to 0.35 km/s. Some masers showed multiple spectral peaks or skewed profiles, suggesting sub-beam-scale structures. A total of 14 maser features had overlapping velocities with 229 GHz masers, revealing potential brightness variations between the two transitions.\n\nFurther analysis of the data is presented, comparing the flux densities of the 36 GHz masers with those from other Class I transitions available in the literature. These initial findings contribute to a better understanding of the evolutionary stages and interactions within the star-forming regions where methanol masers are found. Future high-resolution observations will be needed to establish clearer relationships between different maser transitions and their roles in the complex star-formation processes. In this study, the authors present the first-ever high-resolution images of 36 GHz methanol masers in the Dr21 star-forming complex, obtained using the Expanded Very Large Array (EVLA) at an angular resolution of approximately 12\". These observations were conducted on May 26, 2010, targeting the 36.169 GHz transition of methanol in the sources Dr21(Oh), Dr21W, and Dr21N. The dataset, which spans a velocity range of 33 km/s with a channel spacing of 0.13 km/s, aimed to improve our understanding of the complex environments surrounding clustered star formation and the relationships between masers and their excitation sources.\n\nThe analysis revealed the presence of 49 36 GHz maser features, with 21 in Dr21N, 23 in Dr21(Oh), and 5 in Dr21W. The locations of these masers are compared to previously interferometrically mapped class I methanol transitions, showing consistency in their LSR velocities. The spectra often exhibit multiple peaks or skewed profiles, suggesting the presence of sub-beam-scale structures. This behavior, similar to what has been observed in the 44 GHz transition, indicates that the masers are likely tracing smaller-scale phenomena within the dense cloud cores.\n\nSome 14 maser features display velocity coherence, with 7 showing stronger emission in either the 36 GHz or 229 GHz transitions, suggesting variability or potential brightness differences between the two lines. Additionally, the study finds evidence for a velocity gradient along the redshifted outflow in Dr21(Oh), with LSR velocities increasing from the center towards the edges.\n\nThese high-resolution observations contribute to our knowledge of the physics behind class I methanol masers and their role in the early stages of star formation, as they help pinpoint their exact positions relative to other molecular processes in the vicinity. Further investigations are needed to fully understand the relationships between different maser transitions and their connection to the exciting sources in these complex environments. Continuing from the provided text...\n\nThe Expanded Very Large Array (EVLA) has recently made significant strides in achieving sub-arcsecond resolution imaging, enabling the first detailed analysis of 36 GHz methanol masers in the DR21 star-forming complex. Observations were conducted on May 26, 2010, targeting the 36.169 GHz transition of methanol in the DR21(Oh), DR21W, and DR21N regions. Utilizing 20 telescopes equipped with Ka-band receivers, the EVLA operated in its most compact configuration, resulting in a synthesized beamwidth of approximately 12\". The dataset was correlated using the new WIDAR correlator, with a 4 MHz bandwidth divided into 256 spectral channels providing a velocity coverage of 33 km/s with a channel spacing of 0.13 km/s.\n\nData reduction was carried out using the NRAO's Astronomical Image Processing System (AIPS), and the flux scale was established using 3C48 as a reference. Complex gain calibration was performed using J2048+4310, though EVLA's bandpass calibration suffered due to transition issues. Despite these limitations, the main results, including maser positions, were found to be minimally affected. The total on-source observation time for each source was 3 minutes.\n\nThe 229.758 GHz methanol masers and 226 GHz continuum data were collected for all sources, with the angular resolution of the 229 GHz observations being 12\". The 229 GHz data covered a velocity range from 10 to 11 km/s with a spectral resolution of 0.13 km/s, and the flux densities were corrected for primary beam attenuation. The noise levels near the field center were around 0.15 and 0.3 Jy/beam for DR21(Oh) and DR21W, respectively.\n\nComparison with previously published data from other 44 GHz, 84 GHz, and 95 GHz methanol maser transitions revealed consistency in LSR velocities but indicated the presence of multi-peak structures and non-Gaussian profiles in some spectra, suggesting sub-beam scale structures. A total of 49 36 GHz masers were detected across the three sources, with 21 in DR21N, 23 in DR21 In the study, data were collected and processed, revealing 49 36 GHz maser features across three observed sources: 21 in DR21N, 23 in DR21(Oh), and 5 in DR21W. These masers were imaged with angular resolution of 12\" and their flux densities were corrected for primary beam attenuation. The observations utilized a bandwidth covering a velocity range from @xmath10 to @xmath11 km/s with a spectral resolution of 0.13 km/s. The 229 GHz masers showed a noise floor of around 0.15 Jy/beam for DR21(Oh) and DR21N, and 0.3 Jy/beam for DR21W.\n\nA comparison was made with three other Class I methanol maser transitions at 44 GHz, 84 GHz, and 95 GHz from the literature, and some features displayed multiple spectral peaks or skewed profiles, suggesting sub-beam structure. Weak extended methanol emission was also observed, but the lack of precise bandpass calibration limited confidence in these findings.\n\nOf the 14 overlapping 36 GHz and 229 GHz masers at approximately the same velocity, 7 showed brighter 36 GHz emission and 7 had brighter 229 GHz counterparts, hinting at potential variability in brightness. A similar trend was observed in the 95 GHz and 44 GHz transitions, where closely associated masers with similar velocities had comparable fluxes.\n\nThe 36 GHz masers in the central region of DR21(Oh) exhibited a clustering, with all 14 coinciding with the 44 GHz masers from a previous study. The 84 and 95 GHz masers were mainly within the outflow region, but their lower angular resolution precluded detailed analysis of multi-transition positional coincidences. The velocity gradient was observed, with redshifted features on the western side and blueshifted ones on the eastern side.\n\nClass II masers at 6.7 GHz were detected near the bright continuum source, while a few 229 GHz Class I masers were coincident with the 6.7 GHz masers, albeit offset slightly. Weak 36 GHz maser emission was also identified, possibly due to unresolved spectral blending.\n\nIn summary, the study presents a wealth of information on the distribution, velocities, and potentially variable nature The paragraph provided is discussing the results of observations of methanol masers in the region known as DR21, focusing on their angular resolution, bandwidth, and spectral characteristics. The researchers have detected 49 36 GHz maser features across three sources (DR21n, DR21(Oh), and DR21w), with details on their flux densities and locations relative to other Class I methanol transitions. They observe that the spectra often show multiple peaks or skewed profiles, suggesting sub-beam structures. Additionally, they find 14 overlapping 36 GHz and 229 GHz masers at similar velocities, with some instances showing which line might be brighter based on brightness differences. The study also discusses the presence of a central region in DR21(Oh) with numerous masers forming an elliptical structure, and the alignment of various transitions, including a velocity gradient in the redshifted outflow. Lastly, they mention the detection of weak 36 GHz masers, a unique antisymmetric Stokes V profile in DR21w, and the association of some masers with other sources or regions in the area. The paragraph provides a comprehensive analysis of the observed maser properties and their relationships within the DR21 complex. In the DR21(Oh) region, the maser emission displays a strikingly structured pattern, particularly in the central region, where a multitude of masers across various transitions coexist in an approximately elliptical configuration. Notably, @xcite identifies over 30 44 GHz masers associated with two shock events in a bipolar outflow, with all 14 36 GHz masers found coincident with these 44 GHz sources within the observational uncertainties. The 84 and 95 GHz masers are also distributed within the outflow, albeit with limited angular resolution preventing detailed analysis of their multi-transition positional correlations.\n\nThe 229 GHz masers tend to align more closely with the 44 GHz masers on the western side, particularly in the eastern part of the outflow, showing a distinct velocity gradient with redshifts on the western side and blueshifts on the eastern side. However, there are exceptions, with some masers on the northern and southern edges displaying velocities deviating from this trend. The brightest masers are concentrated on the western side, often colocated with the 44 GHz and 229 GHz counterparts.\n\nClass II 6.7 GHz masers, on the other hand, are projected on top of the bright continuum source near the center, while a few 229 GHz Class I masers appear coincident with the 6.7 GHz ones, offset slightly to the east and south. Weak 36 GHz maser emission is detected, suggesting blended features due to the resolution limitations.\n\nThe southern masers, primarily observed at 36 and 44 GHz, are not always accompanied by 44 GHz masers, and they are closer to other sources like DR21(Oh)W, DR21(Oh)S, and the ammonia emission ridge. The northern and southern masers show velocity shifts relative to the outflow, suggesting they might be connected to different processes.\n\nDR21W stands out with the brightest 36 GHz maser in the sample, reaching 5 Jy, and exhibits an antisymmetric Stokes V profile, indicative of a magnetic field component. This maser's velocity matches closely with the 44 GHz masers detected by @xcite. In DR21N, numerous 36 GHz masers are found on the eastern side, with a mix of redshifted and blueshifted velocities.\n\nOverall, the maser data in DR21( The masers in the DR21(Oh) source exhibit a remarkable concentration in the central region, with numerous masers in various transitions coexisting in an elliptical shape. The 44 GHz observations reveal over 30 associations with shocked regions in a bipolar outflow, with all 14 36 GHz masers coinciding with the 44 GHz counterparts within the observational uncertainties. The 84 and 95 GHz masers, although fewer in detail due to lower angular resolution, are also distributed within the outflow. A velocity gradient is observed, with blueshifts to the east and redshifts to the west. The 229 GHz masers tend to align more closely with the 44 GHz masers, particularly in the eastern part, suggesting a connection between the two transitions.\n\nClass II 6.7 GHz masers are primarily found near the source's origin, while a few 229 GHz Class I masers are offset to the east and south, possibly indicating their association with the 6.7 GHz masers. Weak 36 GHz maser emission is detected, with positions uncertain due to their faint nature. Some Class I masers are detected to the south and north of the outflow, displaying distinct velocity patterns compared to the main outflow.\n\nThe DR21W region stands out with the brightest 36 GHz maser at 25 Jy, which exhibits an antisymmetric Stokes V profile, potentially indicative of a magnetic field component. However, the inferred magnetic field strength is quite high, necessitating a reevaluation of the Zeeman splitting factor for the 36 GHz transition.\n\nIn summary, the maser distribution in DR21 shows complex velocity and spatial relationships across multiple transitions, providing valuable insights into the dynamics and magnetic field conditions in the region. Further study is required to fully understand the underlying mechanisms driving these maser phenomena. The central region of the maser source in DR21(Oh) stands out with its remarkable concentration of masers across various transitions, forming an approximately elliptical structure (depicted in Figure [fig-map-oh]). This exceptional arrangement, which includes more than 30 masers at 44 GHz associated with two shock-induced bipolar outflows, reveals a striking correspondence between the 44 GHz and 36 GHz masers, with 14 out of 36 GHz sources coinciding within the positional uncertainties. The 84 and 95 GHz masers are also situated within the outflow, but their lower angular resolution prevents a detailed analysis of their multi-transition positional relationships.\n\nThe eastern side of the outflow exhibits a blueshift, while the western side shows a redshift, accompanied by a velocity gradient along the redshifted direction. However, some masers on the northern and southern edges deviate from this pattern, challenging the simple velocity gradient model. The 36 GHz masers are predominantly found on the western side, often in close proximity to the brighter 44 GHz and 229 GHz masers, suggesting a strong spatial association. In contrast, 44 GHz masers without corresponding 36 GHz detections are observed in the central and western parts, particularly those with extreme velocities compared to the main outflow.\n\nClass II methanol masers at 6.7 GHz are detected near the source's continuum feature, while several 229 GHz Class I masers are offset to the east and south, often aligning with the 6.7 GHz masers. Weak 36 GHz maser emission is also detected, likely due to unresolved spectral blending. The brightest 36 GHz maser in DR21W, located even farther to the east and south, displays an antisymmetric Stokes V profile, suggesting a possible Zeeman effect. This detection, however, is subject to uncertainty in the Zeeman splitting coefficient, which could affect the inferred magnetic field strength.\n\nThe detection of a bright 36 GHz maser in DR21N, with a distinct \"S-curve\" in its Stokes V spectrum, indicates a magnetic field strength of about 32 mG, although the Zeeman splitting factor used might be overestimated. This high magnetic field, if real, raises questions about the density conditions required for 36 GHz maser excitation, as they are several orders of magnitude higher than typical values for OH masers. Alternative The 36 GHz masers in the region of DR21(Oh) exhibit intricate patterns, with a clear association between the masers at 44 GHz and those at 84 and 95 GHz. The 229 GHz masers, primarily on the western side, show enhanced coincidence with the 44 GHz masers, particularly in the eastern part of the outflow. These transitions display velocity shifts, with a redshift to the west and a blueshift to the east, indicating a velocity gradient along the outflow's direction. However, some masers at the edges deviate from this trend.\n\nThe Class II masers at 6.7 GHz are detected near the continuum source, and a few 229 GHz Class I masers are present, often offset from the 6.7 GHz sources. Weak 36 GHz maser emission is also observed, likely due to unresolved spectral blending. The strongest 36 GHz maser, located even farther east and south, is challenging to pinpoint accurately due to its faintness and the 2-beam size of the observation.\n\nMasers in the southern part of the outflow, particularly at 36 GHz, are often not accompanied by 44 GHz counterparts, suggesting a different association or possibly a different pumping mechanism. The detection of magnetic fields in DR21W through Zeeman splitting, if real, raises concerns about the inferred densities, as they are much higher than typical for Class I maser excitation. An alternative explanation could involve a collisional pumping process involving warm species, but more research is needed to confirm this scenario.\n\nIn conclusion, the multi-transitional maser emission in DR21(Oh) reveals complex velocity and spatial structures, possibly influenced by magnetic fields and different pumping mechanisms, which challenge our understanding of the physical conditions required for Class I methanol maser activity. Further studies are necessary to unravel the underlying physics in these intriguing regions. The 36 GHz masers in the outflow exhibit unique characteristics compared to their counterparts. They are primarily located on the eastern side, confined to the edges, while other transitions are observed throughout the central and western regions. The Eastern 36 GHz masers are distinct, not co-spatial with 44 GHz masers, suggesting different dynamics or excitation mechanisms. In the central and western parts, many 44 GHz masers, particularly those with extreme velocity shifts, lack corresponding 36 GHz detections.\n\nThe 6.7 GHz Class II masers, though present, are restricted to the vicinity of the bright continuum source near the outflow's origin. Some 229 GHz Class I masers, often within a tenth of the beam size, are also detected, spatially coincident with the 6.7 GHz masers but slightly offset. Weak 36 GHz maser emission is observed, likely due to unresolved spectral blending, with positions uncertain due to their faintness and the 2-beam size of the observations.\n\nTo the south of the outflow, 36 GHz masers are present but not always accompanied by 44 GHz masers, revealing a different behavior compared to the western side. These southern masers are closer to sources like DR21(Oh)W, DR21(Oh)s, and the ammonia emission ridge, suggesting they may be unrelated to the primary driving source in DR21(Oh).\n\nThe brightest 36 GHz maser in DR21W, with an antisymmetric Stokes V profile, stands out with a reported magnetic field strength of ~58 MG. However, this value is challenging due to potential overestimation of the Zeeman splitting coefficient, which casts doubt on the direct magnetic field interpretation. The implied density from the magnetic field is much higher than expected for Class I maser excitation, leading to discussions about alternative pumping mechanisms or the possibility of a different magnetic-energy-to-kinetic-energy ratio in these environments.\n\nIn summary, the 36 GHz masers in DR21W exhibit peculiar spatial and velocity patterns, possibly indicating distinct physical processes, with intriguing connections to other transitions and magnetic field properties. Further study, including laboratory measurements of Zeeman splitting coefficients and high-resolution observations, is needed to better understand their origin and behavior.",
        "abstract": " class  i methanol masers are believed to be produced in the shock - excited environment around star - forming regions . \n many authors have argued that the appearance of various subsets of class  i masers may be indicative of specific evolutionary stages of star formation or excitation conditions . until recently , however , no major interferometer was capable of imaging the important 36  ghz transition . \n we report on expanded very large array observations of the 36  ghz methanol masers and submillimeter array observations of the 229  ghz methanol masers in dr21(oh ) , dr21n , and dr21w . \n the distribution of 36  ghz masers in the outflow of dr21(oh ) is similar to that of the other class  i methanol transitions , with numerous multitransition spatial overlaps . \n at the site of the main continuum source in dr21(oh ) , class  i masers at 36 and 229  ghz are found in virtual overlap with class  ii 6.7  ghz masers . to the south of the outflow \n , the 36  ghz masers are scattered over a large region but usually do not appear coincident with 44  ghz masers . \n in dr21w we detect an `` s - curve '' signature in stokes v that implies a large value of the magnetic field strength if interpreted as due to zeeman splitting , suggesting either that class  i masers may exist at higher densities than previously believed or that the direct zeeman interpretation of s - curve stokes v profiles in class  i masers may be incorrect . \n we find a diverse variety of different maser phenomena in these sources , suggestive of differing physical conditions among them . ",
        "section_names": "introduction\nobservations and data analysis\nresults\ndiscussion\nconclusions and future work"
    },
    {
        "article": "Interdisciplinary research has emerged as a critical factor in advancing scientific knowledge, leading to groundbreaking achievements, such as the 2014 Nobel Prize in Chemistry awarded to a team combining physics, physical chemistry, and biology. Despite efforts to define and quantify interdisciplinarity, specific metrics for measuring its impact in scientific research are still lacking. Funding bodies have started to support interdisciplinary projects, recognizing the importance of collaboration across disciplines. The European Research Council, for example, encourages multidisciplinary submissions, and the Marie Curie Fellowships criteria also consider interdisciplinary aspects.\n\nThe challenge in evaluating interdisciplinary research is well-documented, as it poses difficulties for young scholars who may perceive the risks associated with pursuing such careers. This work aims to address this issue by proposing a method to quantify the interdisciplinarity of scientific publications and their creators based on their scientific impact and breadth across disciplines. The approach involves analyzing a complex, interconnected multilayer network of scientific producers and their citations, accounting for the diverse layers representing different scientific disciplines.\n\nThe network, built from citation data and relationships between publications and their manufacturers (scholars, inventors, institutions, companies, or countries), is structured using a bipartite interconnected multilayer adjacency tensor. By considering the interactions within and across disciplines, the method measures versatility, a key aspect of interdisciplinary impact. The ranking is achieved through a diffusion process, where credits flow from papers to their producers, reflecting the influence of each publication on various fields.\n\nThe multilayer pagerank, derived from the eigenvector of the transition tensor representing the probabilities of random walks in the network, is employed to rank both publications and producers. This method ensures that the interconnected nature of the network is taken into account, avoiding information loss when compared to one-mode projections. The effectiveness of the proposed ranking system is demonstrated through case studies on the American Physical Society (APS) and US Patents datasets, showcasing its applicability and potential to enhance the assessment of interdisciplinary research. In this study, the authors propose a novel approach to rank scientific publications and their producers by utilizing a bipartite interconnected multilayer network that considers citations within and across different scientific disciplines. This framework, inspired by Google's PageRank algorithm, aims to capture the cross-disciplinary versatility of scientific outputs. By constructing a network with layers representing disciplines, the model distinguishes between publications that have received equal citation counts but have varying impact due to their origin in highly or low-cited papers.\n\nThe rank@xmath5 adjacency tensor, defined with four components, encodes information about citations between publications and their manufacturers, as well as the connection between authors and their affiliated institutions or countries. The authors normalize citations to ensure each carries equal weight and differentiate interdisciplinary citations. The network structure accounts for the interdisciplinarity of a publication by allowing it to be active on multiple layers if relevant.\n\nTo rank nodes, the authors apply a diffusion process through citation edges, with producers acting as sinks, receiving credit from the papers they produce. The multilayer pagerank, derived from the steady-state solution of a probability diffusion equation, is computed by considering the interconnected structure and all layers simultaneously. This methodology avoids information loss that could occur with one-mode projections and provides a more comprehensive evaluation of the nodes' versatility.\n\nTwo case studies, the American Physical Society (APS) and US Patents datasets, are used to demonstrate the effectiveness of the proposed ranking method. The APS dataset, consisting of physics papers published between 1985 and 2009, is filtered to remove large collaborations and is organized into 10 layers representing distinct subfields of physics. The authors successfully rank the publications and their producers based on their interdisciplinary impact using the multilayer pagerank. This innovative approach provides a more nuanced understanding of the scientific impact beyond traditional citation counts, highlighting the versatility of scientific contributions across different disciplines. In this study, the authors propose a novel approach to rank scientific publications and their producers using a bipartite interconnected multilayer network that incorporates citations across disciplines. By designing a rank tensor that combines information from publication citations and relationships between publications and their manufacturers (scholars, inventors, institutions, countries), the model accounts for the interdisciplinarity of research. The pagerank algorithm, originally developed for ranking web pages, is adapted here to rank nodes based on their versatility, reflecting their impact across different fields.\n\nThe network structure consists of @xmath3 nodes representing publications and @xmath4 layers, each representing a scientific discipline. Citations within and between disciplines are captured by the adjacency tensors. The authors normalize the citations to ensure each carries equal weight, while interdisciplinary citations are represented by additional components. The ranking process involves diffusing scientific credits through the network, with producers acting as sinks, receiving citations from the papers they produce.\n\nThe multilayer pagerank score is computed by solving the steady-state eigenvalue problem, considering the transition probabilities and teleportation rate. This approach ensures a comprehensive evaluation of a node's influence, taking into account its activity across multiple layers. The method is tested on two case studies: the American Physical Society (APS) dataset and the US Patents dataset, demonstrating its effectiveness in capturing the interdisciplinarity of research.\n\nOverall, this innovative framework provides a more accurate and nuanced ranking system for scientific publications and their creators, fostering a better understanding of the interconnectedness and impact of research across various disciplines. The proposed ranking system for scientific publications and their producers utilizes a bipartite interconnected multilayer structure, accounting for citations within and across disciplines. This approach, akin to ranking nodes based on their versatility in an interconnected multilayer network, takes into account the complexity of interdisciplinarity by representing publications and their manufacturers as separate sets of nodes connected through citation and relationship layers. The rank tensor, denoted by @xmath6, is defined considering the adjacency information among layers and nodes.\n\nTo handle interdisciplinarity, the network consists of @xmath21 nodes, representing publications and @xmath23 manufacturers, while @xmath27 layers represent different scientific disciplines. The adjacency tensors @xmath29 are constructed with components @xmath30 and @xmath31 for publication citations and @xmath42 for the relationships between publications and manufacturers. Citations within disciplines are normalized, while interdisciplinary ones are captured by @xmath31.\n\nIn the network, a node is active in a specific layer if the publication it represents belongs to that discipline. The ranking process involves diffusion of scientific credit from papers to papers, with producers, represented by nodes without outgoing edges, acting as sinks. The pagerank versatility is calculated through the steady-state solution of the equation @xmath47, incorporating transition probabilities and teleportation rates.\n\nThe effectiveness of this ranking method is demonstrated through two case studies: the American Physical Society (APS) dataset and the US Patents dataset. The APS dataset, focusing on physics papers, highlights the versatility of researchers, while the US Patents dataset showcases the impact of interdisciplinary works. Comparisons with the Science Author Rank Algorithm (SARA) show that the proposed method outperforms it in identifying versatile researchers who contribute to multiple disciplines.\n\nOverall, this novel ranking system offers a comprehensive approach to evaluating scientific productivity and influence, taking into account the interconnected nature of research across disciplines. In the proposed scientific network, the rank adjacency tensor @xmath8 is composed of four components, @xmath30 and @xmath31, which capture the citation relationships among publications. These components, with @xmath32 as the scaling factor, represent the number of citations between publications within their respective disciplines or subfields. Citations within the same discipline are normalized to ensure each count contributes equally to the overall value. Interdisciplinary citations, involving publications in distinct fields, are encoded by @xmath31.\n\nThe remaining layer, @xmath41, represents interactions between publications and their manufacturers, indicating the involvement of authors or institutions. The tensor @xmath42 captures the connection between publications and their manufacturers, depending on the specified type (scholars, research institutions, or countries). To avoid symmetric relationships, @xmath45 is a null tensor, while @xmath46 is null as the necessary information is already encoded in the other tensors.\n\nThe network's ranking is achieved through a diffusion process of scientific credit, where the pagerank versatility is calculated for both publications and producers. This is done by solving the time-dependent equation @xmath47, considering the transition probabilities represented by @xmath53 and the teleportation rate @xmath52. The pagerank vector @xmath57 is obtained by projecting the multilayer pageranks of each node onto a single vector, ensuring a comprehensive evaluation of the interconnected structure.\n\nTwo case studies, the American Physical Society (APS) and the US Patents datasets, are used to demonstrate the effectiveness of the proposed ranking method. The APS dataset, containing physics papers, highlights the versatility of researchers, while the US Patents dataset demonstrates the impact of interdisciplinary work in technological innovation. By comparing the proposed method with the Science Author Rank (SARA), it is shown that the proposed approach rewards researchers with more significant contributions across multiple disciplines, as evidenced by the higher rankings of prominent figures in complex systems and interdisciplinary fields. In this study, the authors present a novel approach to rank both publications and their producers in a multilayer citation network, accounting for the interdisciplinarity of the content. By designing the network with distinct layers representing different fields, publications are active only in their corresponding layer, reflecting their mono- or interdisciplinary nature. Producers, such as scholars, inventors, and institutions, are placed on a separate layer with directed edges connecting them to their produced publications. The ranking is achieved through a diffusion-based process, where the Pagerank versatility is computed using a time-dependent tensor that considers the probabilities of random walkers transitioning between nodes and layers.\n\nThe proposed ranking method, unlike previous one-mode projections, utilizes the complete bipartite structure to avoid information loss. The multilayer Pagerank is calculated by projecting the values from replicated layers onto a single vector, ensuring a comprehensive assessment of the nodes' influence. The method is tested on two real-world datasets: the American Physical Society (APS) and US Patents, demonstrating its effectiveness in capturing both intrinsic multidisciplinarity (publishing in multiple fields) and effective interdisciplinarity (being cited by diverse scientific areas).\n\nThe results show that the proposed method outperforms existing alternatives, like the Science Author Rank Algorithm (SARA), by rewarding versatile researchers who have conducted interdisciplinary work. The ranking is positively correlated with topical and citation interdisciplinarity, as well as the disciplinary diversity of incoming citations. This framework offers a more nuanced and accurate way to evaluate the citation impact of scientific contributions in an increasingly interconnected research landscape. In this work, the authors propose a novel approach to rank producers in a scientific context by utilizing the complete bipartite structure of the publication-producer network, avoiding information loss. The ranking is achieved through a diffusion process of scientific credit, where producers, represented by nodes without outgoing edges, absorb citations from their produced papers. The multilayer citation network is defined, and the nodes are ranked based on their \"pagerank versatility,\" calculated through the stationary solution of a diffusion equation. This approach differs from previous methods that solely relied on one-mode projections.\n\nThe pagerank versatility tensor is constructed considering multiple layers and teleportation rates, and the eigenvector corresponding to the largest eigenvalue is used to determine the multilayer pagerank. By projecting the pagerank values from different layers onto a single vector, the authors ensure that the interconnected structure is taken into account. Two case studies, the American Physical Society (APS) and US Patents datasets, are employed to demonstrate the effectiveness of the proposed ranking method.\n\nThe APS dataset, consisting of physics publications, shows that the proposed method rewards researchers involved in interdisciplinary works and those with a broader impact across different scientific areas. Comparisons with the Science Author Rank Algorithm (SARA) reveal a high correlation but highlight the advantage of capturing the nuances of versatile researchers. The authors further analyze the correlation between the rank gain and the topical and citation interdisciplinarity of scholars and inventors, demonstrating that the proposed method is not biased by productivity.\n\nThis study contributes a more comprehensive citation impact assessment that considers interdisciplinarity, which could be valuable for funding agencies and academic decision-makers in evaluating the impact of interdisciplinary research and its producers. The authors' approach provides a framework that can be adapted to address specific needs, potentially fostering a faster advancement of excellent science. The eigentensor of the transition tensor @xmath56 plays a significant role in determining the steady-state probability distribution of a random walker in nodes @xmath49 and layers @xmath50. It is important to note that directly calculating the pagerank for each layer independently would not yield the same results, as the proposed formulation takes into account the interconnectedness of the entire structure when solving the eigenvalue problem. The multilayer pagerank vector @xmath57 is derived by aggregating the values from replicated nodes across layers, with @xmath58 being a vector of ones.\n\nExperimental validation of this ranking method is demonstrated using two case studies: the American Physical Society (APS) dataset and the US Patents dataset. The APS dataset consists of papers published in AIP journals between 1985 and 2009, and the US Patents dataset includes patents granted between 1963 and 1999 with citations. By analyzing the metadata and affiliations, the authors create a 10-layer network representing various subfields of physics and a 6-layer network for the patents, demonstrating the effectiveness of the method in capturing interdisciplinarity.\n\nThe comparison with the Science Author Rank Algorithm (SARA) shows a strong Spearman's rank correlation of 0.77 between the rankings, highlighting the similarity in ranking researchers based on citation diffusion. However, the proposed method rewards researchers with more interdisciplinary work, such as Per Bak, Eugene Stanley, and Shlomo Havlin, who exhibit higher rankings due to their ability to publish in multiple fields and receive citations across them. The authors define topical interdisciplinarity as the average number of fields an author's publications belong to and citation interdisciplinarity as the average entropy of their publications' incoming citations. These metrics reveal a positive correlation between rank improvement and interdisciplinarity, suggesting that the proposed method better captures the impact of researchers who contribute to multiple scientific areas.\n\nIn conclusion, the proposed method offers a comprehensive framework for assessing the citation impact of scientific publications and their creators, considering the inherent interdisciplinarity. This approach, based on a bipartite interconnected multilayer network, offers a more nuanced understanding of an author's influence beyond traditional citation counts, making it a valuable tool for funding agencies and academic decision-makers in evaluating interdisciplinary research. In the continuation of the scientific text, the authors delve into the methodology they propose to assess the citation impact of scientific publications and their producers, focusing on the importance of considering interdisciplinarity. They explain that previous citation impact indicators often overlook this aspect, plagued by issues such as self-citations, citation time windows, field normalization, and author credit allocation. By introducing a bipartite interconnected multilayer network, their approach aims to rectify these shortcomings.\n\nThe network, composed of citations and disciplines, is a departure from previous works that ranked scientific publications using a one-mode projection. The authors argue that considering the complete bipartite structure provides a more comprehensive view, capturing the complex relationships between papers and their authors. Using a bilayer version for simplicity, they define four components of the rank adjacency tensor, representing citation relations, authorship, and zeros as needed.\n\nThe motivation behind using this network is to give credit for interdisciplinary works and recognize the impact of researchers across various scientific areas. They demonstrate this by comparing their proposed method, which incorporates interdisciplinarity, with the Science Author Rank Algorithm (SARA) and find a strong positive correlation (0.77 Spearman's rank correlation). This indicates that their method better ranks researchers who have published in multiple fields or have influenced different scientific disciplines.\n\nTo further analyze the relationship between interdisciplinarity and citation impact, the authors compute two measures: topical interdisciplinarity, which is the average number of scientific areas an author's publications span, and citation interdisciplinarity, which is the average entropy of the entropy distribution of an author's publications' incoming citations. They find a positive correlation between these measures and the rank improvement achieved by their proposed method over methods that ignore interdisciplinarity.\n\nBy controlling for productivity, the authors show that the correlation coefficients remain consistent when considering a fixed number of publications. This suggests that their method is not biased by the volume of work produced. The results, presented in the supplementary material, highlight the potential of the proposed method as a tool for funding agencies and academic hiring decisions to evaluate the impact of interdisciplinary research.\n\nIn conclusion, the study introduces a new method for assessing citation impact that takes into account the inherent interdisciplinarity of scientific publications and their creators. The authors' use of a bipartite interconnected multilayer network allows for a more nuanced and accurate evaluation, contributing to a better understanding of the value of interdisciplinary research in advancing knowledge across various fields. In this study, the authors present a new methodology to evaluate the citation impact of scientific publications and their creators, accounting for the crucial aspect of interdisciplinarity. Unlike previous citation indicators, their method takes into account the inherent multidisciplinarity, or the number of fields a publication pertains to, as well as the effective interdisciplinarity, which is measured by the entropy of citations across different layers. The dataset used consists of the APS dataset covering physics and the US Patents dataset from 1963 to 1999, with patents categorized into six layers representing different scientific disciplines. By comparing the rankings obtained using the proposed method with those from the Science Author Rank Algorithm (SARA), they find a strong positive correlation, emphasizing that the proposed method rewards researchers involved in interdisciplinary works.\n\nThe authors demonstrate the effectiveness of their method through a comparison of the interdisciplinary ranking of top physics departments and companies over time, highlighting the rise of the University of Texas at Austin after the establishment of the Center for Nonlinear Dynamics. They further analyze the correlation between the gain in rank and the topical and citation interdisciplinarity of scholars and inventors, showing that these factors play a significant role in the assessment.\n\nThe proposed method not only addresses the limitations of existing citation indicators but also offers a tool for funding agencies and academic hiring decision-makers to gauge the impact of interdisciplinary research. By considering the complete bipartite structure of the citation and discipline network, the method ensures that no information is lost during the evaluation process, providing a more comprehensive and fair ranking of researchers and publications. This innovative approach paves the way for a better understanding and appreciation of the contributions made by interdisciplinary research in various fields.",
        "abstract": " nowadays , scientific challenges usually require approaches that cross traditional boundaries between academic disciplines , driving many researchers towards interdisciplinarity . despite its obvious importance , there is a lack of studies on how to quantify the influence of interdisciplinarity on the research impact , posing uncertainty in a proper evaluation for hiring and funding purposes . here \n we propose a method based on the analysis of bipartite interconnected multilayer networks of citations and disciplines , to assess scholars , institutions and countries interdisciplinary importance . \n using data about physics publications and us patents , we show that our method allows to reward , using a quantitative approach , scholars and institutions that have carried out interdisciplinary work and have had an impact in different scientific areas . \n the proposed method could be used by funding agencies , universities and scientific policy decision makers for hiring and funding purposes , and to complement existing methods to rank universities and countries . ",
        "section_names": "introduction\nmethodology\ndata\nresults\ndiscussion\nacknowledgements\ncomparison with other approaches\nproductivity control"
    },
    {
        "article": "Over the past year, significant advancements have been made in understanding the impact of growth and annealing conditions on the properties of III,MnV diluted magnetic semiconductor ferromagnets. These materials, characterized by III,MnV compounds, exhibit improved ferromagnetic transition temperatures and conductivities. The fundamental behavior of these materials is typically described by a phenomenological model that incorporates the exchange and Coulomb interactions between valence band holes and Mn local moments with spin. In the highly metallic regime, where disorder is minimal, the model's predictions can be accurately explained by considering the role of spin-orbit coupling and the resulting magnetic properties.\n\nThe model, based on the golden rule, successfully predicts the critical temperature, strain-sensitive magnetic anisotropy, anisotropic magneto-resistance coefficients, and the strong anomalous Hall effect. Experimental measurements of infrared magneto-optical properties, such as magnetic circular dichroism (MCD), Faraday rotation, and Kerr effects, offer valuable insights into the itinerant electron quasiparticle states in these materials. By examining these effects, researchers aim to gain a deeper understanding of the physics behind the ferromagnetic behavior of III,MnV materials and potentially identify limitations in the current theoretical formulation.\n\nTheoretical predictions for the infrared magneto-optical response of III,MnV ferromagnets are discussed in this paper, which uses a Kubo formula approach to describe the ac anomalous Hall conductivity. The calculations consider a four-band spherical model, neglecting disorder, to evaluate the Hall conductivity. The influence of disorder, however, is accounted for through finite lifetime scattering rates estimated using Fermi's golden rule, incorporating both screened Coulomb and exchange interactions with Mn ions and compensating defects.\n\nThe study of infrared magneto-optics in III,MnV ferromagnets is deemed crucial due to the relevance of band energy scales in the infrared range, which provides detailed information about the effects of broken time-reversal symmetry on the quasiparticle states. Such experiments could serve as a benchmark for comparing with theoretical predictions and potentially reveal any shortcomings in the current simplified models.\n\nThe paper presents numerical results for the full model Hamiltonian, discussing various magneto-optical effects that can be observed experimentally. The authors' theoretical description starts by coupling the host semiconductor valence band electrons with Mn local moments using a mean-field exchange interaction. The zero-frequency limit of the ac Hall conductivity reduces to an expression previously derived to explain the dc anomalous Hall effect.\n\nIn summary, this work contributes to the In the infrared regime, the magneto-optical properties of heavily p-doped (III,Mn)V ferromagnets, such as (III,Mn)VI, hold significant promise for gaining deeper insights into their unique electronic behavior. These materials, with their band energy scales situated in the infrared, offer a promising platform to investigate the interplay between broken time-reversal symmetry and itinerant electron quasiparticles. Theoretical predictions and experimental measurements in this frequency domain are anticipated, as they can complement the existing visible-range studies that have been instrumental in understanding the P-D and S-D exchange coupling constants.\n\nThe authors' theoretical approach begins by incorporating a Kubo formula description for the AC anomalous Hall conductivity in these (III,Mn)V ferromagnets, accounting for the presence of disorder and the influence of Mn local moments. By neglecting disorder and using a four-band spherical model with isotropic bands, they derive an analytic expression for the anomalous Hall conductivity. This simplified model, while computationally tractable, provides valuable insights into the qualitative features of the conductivity curves.\n\nThe numerical calculations with the full model Hamiltonian, on the other hand, consider the effects of disorder on the quasiparticle states, as revealed by the disorder broadened spectral functions. The scattering rates are calculated using the Fermi's golden rule, incorporating both screened Coulomb and exchange interactions. These calculations are essential for understanding how disorder modifies the magneto-optical responses, such as the optical absorption peak linked to back-scattering localization and inter-valence band transitions.\n\nThe potential applications of studying these ferromagnets are not limited to fundamental physics; understanding their magneto-optical properties could lead to technological advancements in optoelectronics and spintronics, particularly if room temperature ferromagnetism is achieved. The comparison between theoretical predictions and experimental data from infrared magneto-optical studies is eagerly awaited to clarify the intricate physics of these new materials and refine the theoretical formulations employed.\n\nIn summary, the next paragraph could be:\n\nContinuing from the previous discussion, the authors emphasize the importance of infrared magneto-optical investigations in (III,Mn)V ferromagnets, as they promise to provide detailed information about the impact of broken time-reversal symmetry on the itinerant electron states. With their focus on heavily doped materials, these studies would help refine our understanding of the P-D and S-D exchange interactions, as well as address any limitations in the current theoretical models. The combination of theoretical predictions and experimental data in the In the realm of paramagnetic materials, particularly those composed of (II,Mn)VI and (III,Mn)V systems, the study of electronic properties has proven invaluable for understanding the interplay between spin and orbital interactions. Optical measurements in the visible range have provided insights into p-d and s-d exchange coupling constants, highlighting the significance of valence band holes in these materials. Photoemission experiments, despite being surface-sensitive, have probed deeper electronic structures and revealed hybridization dynamics. In the infrared regime, optical conductivity measurements have revealed non-Druddon behavior, with an absorption peak linked to back-scattering localization and inter-valence band transitions, in accordance with theoretical predictions.\n\nThe paper's analysis begins with the Kubo formula for the ac anomalous Hall conductivity (AC AHCS), tailored for discussing the investigated (III,Mn)V ferromagnets. The authors outline a model Hamiltonian and approximations used to calculate the anomalous Hall conductivity, considering both the four-band spherical model for analytical tractability and the full numerical model for a more comprehensive understanding. Disorder, which includes Mn ion disorder, interstitial Mn effects, and antisite disorder, is incorporated through finite lifetimes calculated using the Fermi Golden Rule.\n\nThe numerical results, presented in Sec. 6, demonstrate the impact of disorder on the magneto-optical properties. The disorder-broadened spectral functions and the modified Hall conductivity formula (Eq. [ac_sig_ahe_dis]) account for scattering rates and spectral weights. The spherical model, discussed in Appendix A, serves as a starting point to understand the low-energy anomalies, particularly in the mid-infrared range, where the spectral function exhibits peaks related to light and heavy hole transitions.\n\nIn summary, this research contributes to the understanding of paramagnetic materials by employing advanced techniques to quantify the effects of disorder on their electronic structure, specifically in the context of the anomalous Hall conductivity. The findings not only shed light on the fundamental mechanisms in these systems but also pave the way for future experimental investigations and theoretical refinements. In the isotropic band dispersion limit of the model, analytical calculations can be carried out, albeit with some complexity. These calculations provide insights into the qualitative behavior of the magnetic-optical curves, which are discussed in Sec. 6 alongside numerical results from the full Hamiltonian calculation for the magnetic material. The model begins by coupling host semiconductor valence band electrons with Mn local moments using a semi-phenomenological local exchange interaction. At zero temperature, this interaction leads to an effective exchange field splitting the valence bands, with the strength determined by Mn density and the exchange coupling constant. The paper assumes a collinear magnetization along the growth direction, neglecting non-collinearity due to disorder, which is less likely for the investigated ferromagnetic materials.\n\nThe real part of the ac Hall conductivity, given by Eq. (AC_SIG_AHE_DIS), is derived from linear response theory, considering the effects of disorder on the quasiparticle spectrum. The disorder is modeled through lifetime broadening, which influences the magneto-optical effects, particularly in the form of Faraday and Kerr effects. These phenomena measure the difference in optical absorption between right and left circularly polarized light, known as magnetic circular dichroism (MCD). The numerical calculations presented in Sec. 6, using both a 4-band spherical model and a more realistic 6-band model with band warping, highlight the importance of including these factors to obtain accurate predictions that can be compared with experimental data.\n\nIn summary, this work presents a theoretical framework for understanding the magneto-optical properties of (III,Mn)V ferromagnets, focusing on the role of disorder, spin-orbit coupling, and band structure in the anomalous Hall conductivity. By combining analytical calculations and numerical simulations, the authors aim to provide a comprehensive understanding of the complex behavior of these materials in the presence of external magnetic fields and optical excitation. In the context of studying the real part of the ac Hall conductivity in non-interacting electrons without disorder, the equation provided [Equation \\ref{ac_sig_ah}] serves as the theoretical foundation. This linear response theory, derived from Kubo's formula, connects the conductivity to the electronic band structure and occupation numbers. The formula involves integrals over momentum space and differences in band energies, with the conductivity reducing to the DC value at zero frequency.\n\nThe introduction of disorder, however, significantly impacts the Hall conductivity through broadening of quasiparticle spectral functions. Equation \\ref{dis_sigma} presents the modified expression incorporating disorder, where the spectral function is broadened by scattering rates and the lifetime of quasiparticles. The disorder-induced scattering is captured using the golden rule, leading to additional terms involving the imaginary parts of the velocity operators and the scattering rates.\n\nThe authors emphasize the relevance of disorder in understanding the anomalous Hall effects in itinerant electron ferromagnets, particularly for materials like GaAs. They calculate the influence of disorder on the valence band quasiparticles using a combination of Fermi's Golden Rule and the virtual crystal approximation. The Kohn-Luttinger model is employed to describe the host band Hamiltonian with finite lifetime scattering rates.\n\nA specific example, the 4-band spherical model, is discussed, where the strong spin-orbit coupling leads to a simplified Hamiltonian with clear implications for the spectral function and, consequently, the anomalous Hall conductivity. Calculations for different hole and Mn concentrations show the mid-infrared feature and highlight the importance of considering higher bands for more accurate predictions.\n\nThe numerical results, presented in figures, demonstrate the significance of including band warping in the model, as shown in Figure \\ref{sigah_x6_p4}, where the anomalous Hall conductivity with and without warping exhibits significant differences. The faraday and Kerr effects, which are key magneto-optical phenomena, are also discussed, highlighting their potential practical applications in measuring the properties of these materials.\n\nIn summary, the study on the ac Hall conductivity in disordered (iii, Mn)V ferromagnets, as described by Equations \\ref{ac_sig_ah} and \\ref{dis_sigma}, reveals the intricate interplay between electronic band structure, disorder, and the resulting magneto-optical effects. The consideration of disorder broadening and the influence of different models, such as the 4-band spherical model, plays a crucial role in accurately predicting the material's behavior. In the absence of disorder, the influence of positional randomness and interstitial Mn ions on the electronic properties is significant. These factors, characterized by their charges, contribute to the broadening of the valence band quasiparticles and affect their lifetimes. Using the Fermi's Golden Rule, the disorder-induced broadening of the spectral functions is incorporated into the Hall conductivity formula, Eq. (\\ref{dis_sigma}). By approximating the expression and considering the first-order effects, Eq. (\\ref{ac_sig_ahe_dis}), the authors calculate the anomalous Hall conductivity (AHE) in GaAs.\n\nIn the virtual crystal approximation, the interaction is replaced by its spatial average, leading to a simplified Hamiltonian for the holes. The Kohn-Luttinger model is employed to describe the host band, with the four-band spherical model being derived by setting spin-orbit coupling to infinity and considering a specific value for the parameter. This model simplifies the calculation of the AHE, as demonstrated through Eq. (\\ref{h_4b_sph}).\n\nThe spectral function, crucial for understanding the mid-infrared regime, exhibits distinct contributions from transitions near the light and heavy-hole bands. The anomalous Hall conductivity, shown in Fig. [fig4b], displays features that arise from these transitions and the disorder introduced by Mn ions. The calculated results are found to be sensitive to the inclusion of the warping of bands at higher concentrations, as shown in Fig. [sigah_x6_p4].\n\nThe impact of disorder on magneto-optical effects, such as Faraday and Kerr rotations, is discussed. The Faraday rotation, being a direct measure of MCD, exhibits a strong dependence on the hole concentration, as seen in Fig. [faraday_x6]. The obtained magneto-optical signals are comparable to those observed in materials used for magnetic recording devices.\n\nOverall, the study provides a detailed theoretical framework for understanding the infrared regime of the AHE in disordered GaAs systems, highlighting the importance of disorder-induced broadening and the role of various bands in predicting measurable magneto-optical effects. The results offer valuable insights for experimentalists and researchers working in the field. The anomalous Hall effect in a disordered GaAs system is analyzed using a 4-band spherical model with a strong spin-orbit coupling. By taking the spin-orbit coupling to infinity and considering an isotropic model, the Hamiltonian (Equation \\ref{h_4b_sph}) is derived, revealing that the spin quantization axis aligns with the Bloch state wavevector. The calculation of the anomalous Hall conductivity (\\( \\sigma_{xy} \\)) involves evaluating the interband scattering rates and the spectral function, which exhibits distinct contributions from transitions between light and heavy holes with opposite polarization.\n\nIn the low-energy regime (Equation \\ref{aomlow}), the spectral function has peaks near the light-hole and heavy-hole Fermi wavevectors, while in the high-energy regime (Equation \\ref{aomhigh}), the contribution shifts to higher frequencies. The numerical results, as shown in Figure \\ref{fig4b} and \\ref{sigah_x6_p4}, demonstrate significant peaks in the conductivity that are sensitive to both the hole concentration and the presence of Mn impurities. These peaks lead to pronounced magneto-optical effects, such as Faraday rotation and Kerr effects, which are discussed in detail with their dependencies on the free carrier concentration.\n\nThe 4-band model's limitations, mainly due to not accounting for conduction-band transitions, suggest the need for future studies to incorporate interband contributions in more realistic models. The model's predictions, particularly the Faraday rotation, are found to be one order of magnitude larger than those in paramagnetic (II,Mn)VI materials and exhibit a non-trivial dependence on the hole concentration. The observed features in the transverse conductivity provide valuable insights for experimental measurements and can serve as a benchmark for understanding the interplay between ferromagnetism and disorder in III-Mn-V semiconductor systems. In this section, we delve into the analytical calculation of the anomalous AC Hall conductivity (AC_Hall) in a disorder-free 4-band model, known as the spherical model, for GaAs. This model assumes strong spin-orbit coupling, where the spin quantization axis is aligned with the momentum vector. Starting from the Hamiltonian given in Eq. \\eqref{h_4b_sph}, we employ degenerate perturbation theory to evaluate the first-order correction to the scattering rate due to the anti-ferromagnetic coupling between the localized moments and holes.\n\nThe AC_Hall conductivity, as expressed in Eq. \\eqref{ac_sig_ah}, is calculated by considering transitions between heavy and light holes with opposite polarization. The spectral function, represented by \\( A(\\omega) \\), exhibits distinct contributions from three energy intervals, as described in Eqs. \\eqref{aomlow} and \\eqref{aomhigh}, depending on the energy regimes. These expressions involve the Fermi wave-vectors of light and heavy holes (\\( \\hbar \\vec{k}_F^{L,H} \\)), as well as parameters such as \\( u \\) and \\( h \\).\n\nFor low energies, \\( A(\\omega) \\) has a dominant peak near the light-hole bands and another peak near the heavy-hole bands. The high-frequency part of the spectral function contributes to the rigid shifts in the low-frequency range, suggesting the need to include higher bands for more accurate calculations. The influence of disorder, lifetime broadening, and band warping, although not explicitly included in the spherical model, is crucial for quantitative numerical results at higher concentrations.\n\nTo illustrate the impact of these effects, Fig. [fig4b] compares the AC_Hall conductivity calculated with and without accounting for band warping, showing the importance of considering the full model in experimental comparisons. The figure also highlights the significant Faraday and Kerr effects, which are directly related to the anomalous Hall conductivity and can serve as diagnostic tools for verifying the model predictions.\n\nThe results presented in this section provide a fundamental understanding of the infrared regime behavior of the AC Hall effect in GaAs-based ferromagnets, such as (Ga,Mn)As, and pave the way for future magneto-optic experiments. The reliance on weak-scattering approximations makes the predictions more reliable in more metallic systems, emphasizing the potential for studying these materials in the infrared domain. Confirmation of these predictions through experimental measurements would significantly strengthen the validity of the theoretical approach and In the context of the 4-band model, the anomalous optical conductivity, represented by the spectral function @xmath68, exhibits distinct features in the mid-infrared regime. This behavior arises from the interplay between transitions involving light and heavy holes with opposite spin polarization, which are first-order corrections due to strong spin-orbit coupling. The calculation, detailed in Appendix A, yields the expression for @xmath65, with spectral functions differing for three energy intervals. The low-energy part, given by Eq. [aomlow], contributes mainly from the light-hole bands, while the high-energy part in Eq. [aomhigh] is associated with the heavy-hole bands.\n\nThe presence of these peaks and valleys in the spectral function is evident in the figure [fig4b], which shows the anomalous AC Hall conductivity for various itinerant hole and Mn concentrations. These features are crucial for understanding magneto-optical effects such as magnetic circular dichroism (MCD), Faraday rotation, and Kerr effect. The Faraday rotation, in particular, displays a significant dependence on the free carrier concentration, with large signals potentially observable in metallic samples.\n\nThe 4-band model, despite its limitations in capturing interband transitions, provides a simplified understanding of the physics. The strong spin-orbit coupling is a key factor in this phenomenon, as it aligns the spin quantization axis with the momentum vector. However, higher bands, including the conduction bands, might be required for more accurate calculations at higher concentrations. The model's predictions, particularly the magneto-optic effects, are sensitive to the lifetime broadening, finite spin-orbit coupling, and band warping, which become more pronounced at higher doping levels.\n\nExperimental verification of the model's predictions would further validate the understanding of ferromagnetism in these materials. The authors emphasize the need for future theoretical work to address the crossover between intraband and interband contributions, as they are not fully separable in heavily doped semiconductors. The predictions in this study serve as a basis for motivating infrared magneto-optic experiments in (Ga,Mn)As and other (III,Mn)V ferromagnets, with the hope that the approximations made in weak-scattering scenarios hold true in more metallic samples. The paragraph provided is a detailed explanation of the calculations and results from a scientific study involving the infrared regime of the anomalous Hall effect in a particular material, such as Ga,MnAs. The authors discuss the use of a 4-band model, which takes into account the interplay between itinerant holes and Mn ions, and how these interactions lead to features in the mid-infrared spectral function. They show that transitions between heavy and light holes with opposite spin polarization are responsible for the observed conductivity changes.\n\nThe model includes the effects of disorder, lifetime broadening, and spin-orbit coupling, which are crucial for a quantitative understanding of the numerical results. The authors emphasize the importance of considering the warping of bands, as demonstrated by comparing calculations with and without warping for different Mn concentrations. They present numerical calculations for the anomalous Hall conductivity, Faraday rotation, and Kerr effect, revealing significant magnitudes that can be compared to experimental observations.\n\nThe peaks and valleys in the conductivity are attributed to the transitions between heavy and light holes, and their dependence on carrier concentration. The study's findings suggest that the magneto-optical effects, particularly the Faraday rotation, are highly sensitive to the free carrier concentration and are much larger than those in paramagnetic materials. The authors also mention the limitations of the 6-band model used, which does not account for interband transitions, and call for future theoretical work to address this aspect.\n\nThe paper concludes by acknowledging the support from various funding sources and encourages access to the data and calculations through a database. The authors' goal is to motivate further experimental efforts in the infrared regime to verify the predictions and understand the ferromagnetic properties of materials like Ga,MnAs.",
        "abstract": " we present a theoretical study of the infrared magneto - optical properties of ferromagnetic ( iii , mn)v semiconductors . our analysis combines the kinetic exchange model for ( iii , mn)v ferromagnetism with kubo linear response theory and born approximation estimates for the effect of disorder on the valence band quasiparticles . \n we predict a prominent feature in the ac - hall conductivity at a frequency that varies over the range from 200 to 400 mev , depending on mn and carrier densities , and is associated with transitions between heavy - hole and light - hole bands . in its zero frequency limit , our hall conductivity reduces to the @xmath0-space berry s phase value predicted by a recent theory of the anomalous hall effect that is able to account quantitatively for experiment . \n we compute theoretical estimates for magnetic circular dichroism , faraday rotation , and kerr effect parameters as a function of mn concentration and free carrier density . \n the mid - infrared response feature is present in each of these magneto - optical effects . ",
        "section_names": "introduction\ntheoretical approach\nmodel hamiltonian\n4-band spherical model\nnumerical results\nconclusions\nderivation of @xmath3 in the 4-band spherical model"
    },
    {
        "article": "In numerical simulations of stellar systems, the gravitational force is modified to address the issue of Newton's law's singularity at short distances, often implemented through a softening length, @xmath1. The choice and implementation of this modification significantly impact the system's dynamics, as softened gravity affects regions where it becomes singular. Understanding these dynamical effects is crucial for interpreting experimental results and designing experiments in such systems, as highlighted by various studies (Hernquist & Barnes, 1990; Hernquist & Ostriker, 1992; Kandrup et al., 1992; Pfenniger, 1993, etc.).\n\nIn a previous work (Paper I), the authors investigated the stability of 2D models with Plummer softening, a common choice for galactic discs, finding that the effect of softening becomes pronounced for wavelengths below a typical value, @xmath2. They derived a criterion for physical consistency and a stability condition for the Toomre parameter. This study laid the groundwork for further exploration.\n\nIn this paper, the authors extend their analysis to encompass more general isotropic forms of softening, as different softened-gravity models are increasingly used in simulations (e.g., Hernquist & Katz, 1989; Pfenniger & Friedli, 1993). They also revisit the classical relaxation problem, revising the commonly held belief about the optimal softening length and demonstrating an intermediate choice that optimizes the \"dynamical resolution\" of the model. Additionally, they analyze the equilibrium problem for axisymmetric states with epicyclic motions and explore the effects of softening in 3D models, considering both isotropic and anisotropic cases.\n\nThese extensions serve as a framework to compare and contrast the dynamical effects of softened gravity in 2D and 3D systems, and in different geometries, such as discs versus the Jeans problem. By doing so, the authors aim to understand the unique differences in dynamics introduced by the softening in these scenarios.\n\nThe methodology, described in Section 2, involves analyzing the Poisson equation for the potential perturbations in 2D discs with isotropic softened gravity using a Bessel-Hankel representation. The reduction factor @xmath16, which quantifies the weakening of the potential perturbation and the gravitational contribution to the dispersion relation, is derived both analytically and numerically. This factor plays a central role in understanding the stability properties.\n\nIn summary, In the realm of softened gravity simulations in astrophysical systems, the stability criterion for the Toomre parameter, a key aspect of stability analysis, is revisited and extended in the current study. The authors generalize the analysis from isotropic to arbitrary forms of softening, acknowledging the growing popularity of alternative softened gravity models, such as those proposed by Hernquist & Katz (1989) and Pfenniger & Friedli (1993). This extension allows for a more comprehensive comparison between different softened gravity implementations and their dynamical consequences.\n\nThe study further delves into the impact of softened gravity on the classical relaxation problem, which is intricately linked to stability. They revise the argument regarding the optimal value of the Toomre parameter (@xmath1), revealing that neither small nor large values are ideal. Instead, they identify an intermediate regime that optimizes the 'dynamical resolution' of the model, making it more faithful to Newtonian dynamics in 3D disc systems, particularly near stability thresholds.\n\nIn addition, the authors investigate the effects of softened gravity on 2D equilibrium states with epicyclic motions, providing quantitative deviations from Newtonian behaviors at varying distances from the center. They also examine 3D models with isotropic softening, considering both discs and the Jeans problem, highlighting the strong modifications introduced by a homogeneous geometry and the absence of rotation.\n\nThe investigation of anisotropic softening, crucial for resolving high-resolution issues in disc galaxies, is also addressed. This extension complements the understanding of the dynamical interplay between softened gravity and system anisotropies. The authors outline a systematic method to explore these effects in 3D @x0-body simulations.\n\nThe paper concludes by summarizing the main results in a broader perspective, motivating future applications. The authors emphasize the importance of their findings in accurately modeling complex stellar systems with softened gravity, as they contribute to a more accurate representation of the dynamics in these systems, overcoming limitations imposed by traditional softened potentials. The study revisits the role of velocity dispersion in the relaxation time, challenging the classical preference for large softening lengths (@math1) in gravitational models. It finds that an intermediate value optimizes the model's \"dynamical resolution,\" particularly in simulations close to the stability threshold. The authors delve into the optimal characteristics and evaluate them for softened gravity, revealing how softening affects the dynamics of 2D and 3D discs with Newtonian gravity. They also discuss the impact of softening on noise reduction and the modification it introduces compared to 2D and 3D systems.\n\nIn 2D models with isotropic softening, the authors analyze the Poisson equation using a different spectral representation to derive a reduction factor (@math16), which shows how the potential perturbation is weakened by softening. This factor, combined with the reduced active surface density, provides insights into the stability of discs. The method involves calculating this factor analytically or numerically, with numerical methods proving more efficient due to their ability to handle oscillatory integrands.\n\nThe stability analysis reveals that softening mimics thickness on large scales, affecting the scale height and potentially causing artificial stabilization or 'blueshift' in certain regimes. The study also addresses the classical relaxation problem, where a large softening length might initially seem advantageous. However, it demonstrates that the optimal value lies in between small and large softening, leading to a longer relaxation time that better matches the desired stability level.\n\nIn summary, this work presents a refined understanding of the effects of softening in gravitational simulations, emphasizing the importance of an intermediate softening length for optimizing the model's dynamic resolution and addressing the misconceptions associated with choosing large or small values of the softening parameter. The findings contribute to a more accurate representation of realistic stellar systems in simulations. In the realm of numerical simulations and theoretical investigations, simplified models often lack the precision to capture the intricacies of complex systems. Our objective is to discern the distinct dynamical impacts of softening in three-dimensional (3D) environments compared to two-dimensional (2D) ones, particularly focusing on isolated stellar components and the differences between disc dynamics and the Jeans problem. A homogeneous geometry and the absence of rotation significantly alter the dynamics in 3D, contrasting with the more isotropic behavior in 2D.\n\nTo further delve into the 3D dynamics, we extend the study to consider anisotropic softening, which becomes crucial in simulations that require high spatial resolution along specific axes, such as those found in disc galaxies. The Pfenniger & Friedli (1993) proposal offers a suitable framework for exploring these variations. This anisotropic softening approach, although not without its own challenges, highlights the need to conserve angular momentum, as suggested by Hernquist (private communication).\n\nOur simulation methodology, outlined in Sect. 2, combines these elements to analyze the effects of softening in multi-body stellar systems. We demonstrate this in Sect. 3, presenting two applications of the method, complementing the earlier discussion. In 2D discs with softened gravity, the impact of surface density perturbations is quantified through a reduction factor, $\\mathcal{H}_0[\\varphi_n(r)](k)$, derived from the Bessel-Hankel representation, which weakens the potential perturbation. This reduction factor is essential for understanding the stability of these systems.\n\nThe stability analysis, discussed in Sect. 4, plays a crucial role in characterizing the system's response to perturbations. By examining the marginal stability curve, we determine the effective parameter $\\mathcal{Q}$, which governs the stability level. We find that softening behaves like a scale height, mimicking thickness and influencing the scale of density waves. The stability threshold and the critical radial wavelengths are also affected, highlighting the importance of choosing appropriate softening lengths for accurate simulations.\n\nLastly, Sect. 4 discusses the implications of our findings for the classical relaxation problem, which suggests that the relaxation time is inversely proportional to the softening length. However, our stability analysis reveals that the optimal choice of softening is not simply determined by the relaxation time, but rather by the effective stability parameter. This counterintuitive result underscores the need for a refined understanding of the interplay between softening and stability in numerical simulations.\n\n In Section 3, the two specific applications of the developed theory are presented, further illustrating its applicability. The paper's conclusions, in Section 4, delve into the broader implications of the findings, discussing the general perspective and motivating future research. The theoretical framework is built upon 2D discs with softened gravity, where a surface-density perturbation leads to a potential perturbation. By analyzing the Poisson equation using the WKBJ approach with a radial-dependent representation, a reduction factor is introduced, representing the weakening of the potential perturbation due to the softening. This factor is crucial for understanding the stability of 2D discs, and its numerical evaluation serves as the starting point for the method.\n\nThe evaluation of the reduction factor can be done either analytically through integrals from Gradshteyn and Ryzhik's comprehensive table or numerically using fast Hankel transforms. This factor, denoted by \\(\\mathcal{H}_0[\\varphi_n(r)](k)\\), reduces the self-gravity contribution to the dispersion relation and affects the stability properties.\n\nThe effective scale height, a key concept in understanding the softened gravity, is derived and plays a significant role in mimicking thickness on large scales. The stability analysis is performed using techniques of asymptotic expansions, revealing how the stability level is affected by the softening parameter. The marginal stability curve, describing the stability properties, is discussed, with the effective parameter \\(Q_{\\text{eff}}\\) and the critical wavelengths being critical indicators.\n\nLastly, the implications of the stability analysis for the classical relaxation problem are addressed. The corrected Rybicki-White relaxation time, accounting for the dependence on softening, is derived and shown to be optimized for intermediate values of the softening length. The optimal characteristics, including the relaxation level and the choice of parameters, are identified, ensuring approximate physical consistency.\n\nIn summary, the paper presents a comprehensive study of 2D discs with softened gravity, focusing on the effects of the reduction factor and the implications for stability and relaxation. The methodological advancements and the connection to real-world scenarios make this work a valuable contribution to the field of astrophysics. In the context of analyzing the stability of 2D discs under isotropic softening, the reduction factor plays a crucial role. This factor, represented by @math16(k) divided by the Newtonian point-mass potential @calh0[\\varphi_{\\rm n}(r)](k), weakens the potential perturbation caused by surface-density perturbations. Consequently, the self-gravity contribution to the dispersion relation is reduced by the same factor, ensuring a comprehensive understanding of the softening's impact on disc stability.\n\nTo evaluate @math18, both analytical and numerical methods can be employed. Analytically, it can be derived from equation (4), involving integrals from Gradsteyn & Ryzhik's comprehensive table. Alternatively, numerical computation using the point-mass force, as in the Nag library, is more efficient due to the improved decay of oscillatory integrands with distance.\n\nThe effective scale height, @math26, derived from @math18, is crucial for analyzing stability at small scales. By expanding equations (5) or (6) using techniques of asymptotic expansions, one finds that @math31, which is positive for practical softened gravity models. This scale height-like effect mimics the thickness of the disc and affects density waves similarly to a genuine scale height.\n\nCharacteristics, such as the conversion factor @math49 and the safety threshold @math50, are also extracted from the analysis. The stability properties are described by the marginal stability curve, where the effective parameter @math46 governs the stability level, with @math47 marking the threshold.\n\nThe corrected Rybicki-White relaxation time @math63, accounting for general isotropic softened gravity, includes a correction factor @math73 that modifies the original expression. The optimal characteristics, including the optimal relaxation level @math80 and the optimal choice of @math81, are identified to balance relaxation and stability.\n\nThe reduction factor in the equilibrium problem, represented by the transformed surface density in the context of the Hankel transform, captures the combined effects of softening on both particle interactions and self-consistent fluctuations. The relative quadratic deviations, although not explicitly mentioned, can be inferred from the overall behavior of the system as softening suppresses small-scale fluctuations. In the realm of astrophysical simulations, understanding the behavior of softened gravity, represented by the term @xmath25, is crucial for capturing the dynamics accurately. Integrating by parts and isolating the Newtonian regime at large distances, as demonstrated in equations (5) and (6), allows for improvements in the integrand's decay rate, which in turn affects the effective scale height @xmath33. This scale height serves a significant dynamical role, mimicking thickness in discs and influencing the behavior of density waves.\n\nExpanding @xmath29 from equation (5) and @xmath30 from equation (6), we arrive at @xmath31, a quantity that is positive under practical softened gravity conditions. The scale height @xmath33 plays a similar role as the scale height in Newtonian discs, affecting the stability of density perturbations. \n\nThe stability analysis, detailed in the text, involves extracting information about the stability properties through the marginal stability curve @xmath45. The effective parameter @xmath46 measures the stability level, with the threshold @xmath47 corresponding to the maximum of the curve. The critical radial wavelength @xmath48 marks the sensitivity of the stability to changes in the model.\n\nThe stability analysis has implications for the classical relaxation problem, where the relaxation time @xmath63, originally derived under simplified assumptions, needs to be corrected due to the dependence on the softened potential. The optimal characteristics for relaxation and stability, represented by @xmath80 and @xmath81, are found to minimize the relaxation level @xmath77. The suggested definition of these optimal values, @xmath92, highlights their significance.\n\nFurthermore, the reduced effect of small-scale fluctuations, quantified by the reduction factor, is an important aspect that softened gravity helps suppress in 2D models. The equilibrium problem for axisymmetric states can be solved using Hankel transforms, and the transformed surface density acts as a reduction factor for the softened case.\n\nLastly, the deviations of angular speeds @xmath98 and @xmath99 from their Newtonian counterparts, @xmath104, highlight the changes in the system's stability and relaxation properties near the center, influenced by the softened mass distribution and the disc scale length. These deviations have consequences for the presence and locations of inner Lindblad resonances and affect the overall dynamics of the system. The stability properties of a disc are crucial for understanding its dynamics and relaxation processes. In the context provided, the marginal stability curve, represented by the dispersion relation for marginally stable perturbations, plays a central role. The effective parameter @xmath46, with its threshold @xmath47, determines the stability level, and the critical radial wavelength @xmath48, marked by the maximum of the marginal curve, is sensitive to changes in softening.\n\nSoftening, a modification in the gravitational force law, can mimic thickness on large scales and affect the behavior of density waves and bending waves, as shown by @xmath33. The conversion factor @xmath34, which converts characteristic parameters, is discussed in detail. The safety threshold @xmath50 ensures approximate physical consistency, and the\u8f6f\u5316\u7a0b\u5ea6 (@xmath51) influences whether it acts as a thickness enhancer or causes artificial stabilization.\n\nAt the critical value @xmath56, velocity dispersion loses its direct control over stability, preventing the simulation of spiral structures that require precise tuning. This has significant dynamical implications. The critical radial wavelength @xmath60 is used to measure the sensitivity of the stability properties to variations in softening.\n\nThe stability analysis is applied to the classical relaxation problem, where the Rybicki-White relaxation time @xmath63, which depends on the softening length @xmath1, velocity dispersion @xmath64, and particle count, is considered. Initially, a high softening length seemed beneficial due to its association with a constant stability level. However, a closer examination reveals that the actual stability threshold @xmath66 depends on @xmath1, leading to a shorter relaxation time for larger values. An intermediate choice of @xmath1 is necessary for optimal relaxation and stability.\n\nThe optimal characteristics for 2D discs with softened gravity are identified, including the optimal relaxation level @xmath80 and the choice of @xmath81. These values minimize the relaxation time while maintaining a realistic vertical-to-radial velocity dispersion ratio. The correction factor @xmath73 for the Rybicki-White relaxation time is also derived.\n\nThe reduced effect of small-scale fluctuations due to softening, quantified by the reduction factor, is mentioned, although a thorough treatment of collective interactions is beyond the scope of the current discussion. The formulae for angular speeds @xmath98 and @xmath99, which determine stability and relaxation, exhibit deviations from The paragraph discussing the stability analysis and its implications for the classical relaxation problem in discs with softened gravity continues. The relaxation time, denoted by @xmath63, is found to be proportional to the softening length @xmath1, velocity dispersion @xmath64, and the number of particles @xmath0, and inversely proportional to the cube of @xmath64. Initially, large @xmath1 values were considered favorable due to their extended relaxation times; however, the actual stability threshold @xmath66 depends on @xmath1 and is measured by the effective parameter @xmath67. This leads to the realization that choosing large @xmath1 is counterproductive as it results in a lower stability level and shorter relaxation times.\n\nThe optimal relaxation level @xmath80 and choice @xmath81 are identified, based on numerical calculations, to minimize the relaxation time while maintaining stability. The correction factor @xmath73, derived from generalizing the relaxation time to an arbitrary isotropic potential, plays a crucial role in understanding the effects of softened gravity. The study also reveals that the temperature anisotropy corresponding to the optimal value of the effective thickness parameter is realistic, ensuring a balanced vertical-to-radial velocity dispersion ratio.\n\nThe optimal characteristics are characterized by specific values for @xmath44 and related quantities, such as @xmath88 and @xmath89, which provide a convenient range for simulations. The reduction factor, which accounts for collective interactions and self-consistent fluctuations, is briefly mentioned as an important aspect that requires further investigation but is indirectly captured through the dispersion relation.\n\nFinally, the paragraph mentions the use of Fourier transforms to solve the 3D Jeans problem with isotropic softened gravity, highlighting the challenges and the need to carefully select the softening length @xmath1 to balance the effects of softening with the vertical structure and stability properties. The stability and relaxation properties of computer particles in a two-dimensional disc system are intricately linked to the Safronov-Toomre parameter, represented by @xmath65, and the radial velocity dispersion @xmath64. Initially, it was proposed that large @xmath1 values were advantageous due to their association with longer relaxation times for a given @xmath65 and @xmath0. However, a deeper analysis reveals that the actual stability threshold @xmath66 depends on both @xmath1 and an effective parameter @xmath67, making large values of @xmath1 unsuitable as they lead to shorter relaxation times.\n\nTo find the most physically consistent configuration, an intermediate value of @xmath1 is sought that optimizes the relaxation and stability properties. The optimal characteristics are identified through numerical simulations and theoretical calculations, considering factors such as the relaxed level @xmath77 and the correction factor @xmath73. The relaxation level is optimized when the system's response to fluctuations is balanced, ensuring stability while minimizing noise.\n\nA crucial aspect of the relaxation problem involves the effects of collective interactions and self-consistent fluctuations, which are not fully accounted for in the current treatment. However, the dispersion relation in @xmath26 provides valuable insights into the suppression of small-scale fluctuations by softening, which can influence the noise in two-dimensional models.\n\nThe equilibrium problem for axisymmetric discs with softened gravity can be solved using the Hankel transform, which simplifies the Poisson equation and reveals the reduction factor for the transformed surface density. This factor, @xmath26, is essential in understanding the combined effects of isotropic softening on the dynamics of the system.\n\nThe deviation of angular speeds from their Newtonian behavior, @xmath98 and @xmath99, are important indicators of stability and relaxation. The relative quadratic deviations from the Newtonian forms are sensitive to the mass distribution and the disc scale length. Near the center, these deviations can alter the number and location of inner Lindblad resonances and affect the system's response to modifications.\n\nWhile the dynamics of three-dimensional discs with isotropic softened gravity are challenging due to the combined influence of softening and vertical random motion, the authors suggest choosing @xmath1 values smaller than the characteristic scale height to minimize the impact on the vertical structure and stability. They also compare the Jeans problem in three dimensions with the dynamics of discs, emphasizing the use of Fourier transforms for numerical computations.\n\nIn summary, the study focuses on",
        "abstract": " two questions that naturally arise in @xmath0-body simulations of stellar systems are :    1 \n .   how can we compare experiments that employ different types of softened gravity ? \n 2 .   given a particular type of softened gravity , which choices of the softening length optimize the faithfulness of the experiments to the newtonian dynamics ?    \n we devise a method for exploring the dynamical effects of softening , which provides detailed answers in the case of 2-d simulations of disc galaxies and also solves important aspects of the 3-d problem . in the present paper we focus on two applications that reveal the dynamical differences between the most representative types of softened gravity , including certain anisotropic alternatives . \n our method is potentially important not only for testing but also for developing new ideas about softening . \n indeed , it opens a _ direct _ route to the discovery of optimal types of softened gravity for given dynamical requirements , and thus to the accomplishment of a physically consistent modelling . ",
        "section_names": "introduction\nmethod\napplications\nconclusions\nuseful analytical formulae for sect.3"
    },
    {
        "article": "In Loop Quantum Gravity (LQG), spinfoams provide a covariant framework for describing the dynamics through transition amplitudes between states within the kinematical Hilbert space. This amplitude is expressed in terms of spins and intertwiners associated with the links and nodes of a spin-network graph. The recent literature has primarily considered spinfoams represented by a spatial configuration encoded by a complex, with faces labeled by spins and edges by intertwiners.\n\nSpinfoams are generally defined using these variables: a space-time configuration is represented as a complex, where faces are labeled by spins and edges by intertwiners. Different representations of LQG's Hilbert space can be lifted to the covariant level in spinfoams, such as the \"holonomy representation\" which resembles a Feynman path integral and involves integrating over Ashtekar connections smeared along links. Another representation, the \"holomorphic representation,\" arises from the Segal-Bargmann transform for connections, as introduced by Ashtekar et al. These distinct representations share the same physical content but offer novel insights and computational tools.\n\nThe holomorphic representation is particularly significant due to its connection to coherent spin-network states, which are peaked on both classical intrinsic and extrinsic discrete geometries. These states, with their peakedness, play a crucial role in understanding the semiclassical behavior of spinfoams. Recently, the large-scale asymptotics of the Lorentzian spinfoam vertex have been derived using the spin and normals representation introduced by Livine and Speziale. However, this representation's maximal spread on the conjugate momentum causes it to miss one of the classical solutions in the semiclassical expansion.\n\nThis paper presents a detailed study of the holonomy and holomorphic representations of spinfoams, emphasizing their relation to other commonly used representations, such as the \"spin and intertwiner\" and \"spin and normals.\" It derives the expressions for the Euclidean and Lorentzian vertex amplitudes in both representations and demonstrates how the peakedness of coherent spin-network states allows for selecting a unique classical solution in the analysis of the Lorentzian vertex's large-scale behavior. In Loop Quantum Gravity, the Hilbert space for a graph embedded in a 3D hypersurface is described by a specific function space, and spinfoams act as maps connecting in and out states through a sum over geometries. Boundary amplitudes, which involve a local product of vertex amplitudes and integrated bulk variables, are also discussed. In the final paragraph, the discussion shifts towards the implications and advantages of using the holomorphic representation in spinfoams. The authors emphasize that coherent spin-network states, which are central to the holomorphic representation, exhibit a remarkable peakedness on both the classical intrinsic and extrinsic geometry of space. This property allows them to distinguish between the two classical solutions that arise in the large-scale asymptotics of the Lorentzian vertex, an issue that plagues the commonly used \"spin and normals\" representation.\n\nThe authors point out that the holomorphic representation diagonalizes the area operator, resulting in a more compact phase space representation that does not suffer from the artifact of having a maximal spread on the conjugate momentum. They argue that this feature, combined with the peaked nature of coherent spin-networks, enables a cleaner identification of a single classical solution in the semiclassical expansion.\n\nFurthermore, the paper presents a detailed comparison between the holonomy and holomorphic representations of spinfoams, highlighting the benefits of the latter for studying the asymptotics. The authors demonstrate how the application of the derived formulae in the holomorphic representation allows for selecting the correct classical solution in the analysis of the vertex, a crucial step in understanding the geometric aspects of loop quantum gravity.\n\nFinally, the paragraph sets the stage for future research, suggesting that the exploration of coherent spin-networks and their role in spinfoams is an exciting area with untapped potential. The authors invite further investigation into the use of this representation in Loop Quantum Gravity and its potential impact on the understanding and computational efficiency of the theory. side, ensuring a consistent discrete geometry. This setup corresponds to the choice of twisted geometries in the covariant spinfoam framework, where Freidel-Speziale variables are used. In the holomorphic representation, the complex variables associated with the spin and normals can be understood as quantized versions of classical fluxes and holonomies, capturing the essence of the underlying discrete geometry. The peakedness property of coherent spin-network states allows for selecting a unique classical solution in the study of large-scale asymptotics of Lorentzian spinfoam vertices in Loop Quantum Gravity. This connection between the quantum amplitudes and the classical configurations highlights the potential for a deeper understanding of spacetime geometry in the context of these quantum models. In the context of spinfoams, which serve as a quantum representation for geometries, the last paragraph discusses the mathematical formulation and properties of these structures. A spinfoam is a map that transitions between \"in\" and \"out\" geometries, represented by spacetime manifolds, through a sum over 4-dimensional geometries described by a spin-foam model. The amplitude for this transition is given by an integral over the spacetime metric (denoted as $g^{\\text{(4)}}$) with a complex exponential factor.\n\nThe formalism allows for a generalization to \"boundary amplitudes,\" where the Hilbert space $\\mathcal{H}$ associated with a graph on the boundary of a 4-dimensional ball is considered. The boundary amplitude of a state is determined by the spinfoam amplitude in the holonomy representation, $\\mathcal{A}(\\text{out}) = \\int_{g^{\\text{(4)}}_{\\text{in}}}^{g^{\\text{(4)}}_{\\text{out}}} \\text{e}^{iS[g^{\\text{(4)}}]} d g^{\\text{(4)}}$, where $S$ is the action functional. This amplitude is local in spacetime, meaning it decomposes into a product of vertex amplitudes, each integrated over the bulk variables.\n\nA key feature is the presence of \"face amplitudes,\" which are delta functions involving products of holonomies associated with the faces of the 4-complex. These are necessary to ensure the composition law of the overall amplitude. The construction involves associating a boundary graph to each vertex, and the bulk variables are holonomies for the links intersecting the 3-sphere at that vertex.\n\nThe Ponzano-Regge model for 3D gravity, for example, employs a complete graph with 4-valent nodes, while the EPRL-FK vertex, initially defined on a complete graph with 4-valent nodes, has been generalized by KMisielowski-Kisielowski-Lewandowski. Spin-vertices in the literature are often represented in the spin and intertwiner formalism, which is related to the holonomy representation via the Peter-Weyl transform.\n\nA new representation called the \"spin and normals\" representation has emerged, where coherent spin-networks with labels from a certain space play a crucial role. These coherent spin-networks provide a holomorphic representation for loop quantum gravity, allowing for a Segal-Barg In the context of the provided scientific text, the next paragraph would be:\n\nThe construction of the spinfoam vertices, denoted as @xmath31, involves associating a boundary graph to each vertex of the @xmath0-complex, which includes a vertex label @xmath31 and link labels @xmath14. The bulk variables @xmath32 are holonomies associated with the links intersecting the faces at the vertex, both for internal and external faces. This process is visually represented in Figure [figura]. The vertex amplitude for the Ponzano-Regge model in 3D gravity is given by @xmath35 as an example. The Ponzano-Regge vertex is initially defined on a complete graph with 4-valent nodes. The EPRL-FK vertex, proposed by Engle-Pereira-Rovelli-Livine and Freidel-Krasnov, starts with a complete graph with a different number of nodes (@xmath36). More recently, a generalized version by Kaminski-Kisielowski-Lewandowski allows for arbitrary boundary graphs.\n\nThe transition from the holonomy representation to the \"spin and intertwiner\" representation, which is commonly used in the literature, involves the Peter-Weyl transform. Here, a spin-network state is represented by @xmath38 with spin matrices @xmath39 and intertwining tensors @xmath41. The Ponzano-Regge vertex in this representation is represented by a Racah symbol.\n\nIn the study of spinfoam vertices, an alternative representation called \"spin and two unit-normals per link\" has emerged, which simplifies the semiclassical analysis. This representation utilizes Livine-Speziale coherent intertwiners, labeled by unit-vectors satisfying closure conditions. Coherent spin-networks, with labels @xmath62 in @xmath63, form an overcomplete basis for the boundary Hilbert space and provide a holomorphic representation through a Segal-Bargmann transform.\n\nThe introduction of coherent spin-networks and their holomorphic representation offers a clear classical interpretation for the @xmath63 variables, making it easier to connect with discrete geometries. The authors discuss two equivalent descriptions for the discrete geometry associated with these labels: the first, based on Ashtekar variables, and the second, in terms of twisted geometries. These representations are crucial for understanding the semiclassical behavior of spinfoams and their relation to loop quantum gravity In the context of Loop Quantum Gravity, a recent development by Kaminski-Kisielowski-Lewandowski introduces a generalized EPRL-FK vertex that can accommodate arbitrary boundary graphs. This vertex, which was initially defined in the holonomy representation, is commonly presented in the \"spin and intertwiner\" representation, which is connected to the holonomy via the Peter-Weyl transform. The \"spin and normals\" representation, introduced for the purpose of a more convenient semiclassical analysis, uses spin networks with coherent intertwiners labeled by unit-vectors satisfying closure conditions. These coherent intertwiners are related to Bloch coherent states, allowing for a more classical interpretation of the system.\n\nA spin network state, represented by matrices @xmath38 and intertwining tensors @xmath39, in the \"spin and intertwiner\" representation is characterized by a specific amplitude given by a Racah symbol. In the study of spinfoam vertices, a third representation involving spin labels and two unit normals per link has proven to be valuable, particularly in understanding their semiclassical behavior.\n\nThe new spinfoam vertex in the \"spin and normals\" representation is expressed as an amplitude involving the amplitudes of spin network basis elements. Coherent spin networks, with labels associated to each link, form an overcomplete basis in the boundary Hilbert space and provide a holomorphic representation through a Segal-Bargmann transform. This representation offers a clearer classical geometric interpretation of the spin and normal variables.\n\nThe 4-dimensional Lorentzian vertex amplitude, relevant in Loop Quantum Gravity, can be derived and expressed in both the holonomy and holomorphic representations. The holonomy representation involves a contraction with Clebsch-Gordan coefficients and a distribution over \"face\" terms, while the holomorphic representation makes use of the Segal-Bargmann transform to rewrite the vertex as a distribution on a complexified space.\n\nOverall, this section presents a deeper exploration of various representations of spinfoam vertices and their connections to classical geometry in the context of Loop Quantum Gravity, emphasizing the utility of coherent spin networks and the holomorphic representation in understanding the quantum-to-classical transition in the theory. Coherent spin-network states, introduced in the context of loop quantum gravity, serve as an overcomplete basis for the boundary Hilbert space, labeled by elements of a space @xmath63 associated with links in a graph. These states are defined by specific parameters @xmath65 and @xmath66, which are related to the analytic continuation of the heat kernel on the @xmath1 manifold. They provide a Segal-Bargmann transform, allowing for a holomorphic representation of spinfoam amplitudes.\n\nThe key advantage of this representation is that the @xmath63 labels can be interpreted in classical terms, capturing the phase space dynamics of the theory. Coherent spin-networks exhibit peaked properties and geometric significance in the context of loop quantum gravity, but their application in spinfoams was not fully explored until recently. The holomorphic representation allows for a more intuitive understanding of the discrete geometries underlying these amplitudes.\n\nA spinfoam vertex can be expressed in the holomorphic formalism using a formula (eq: wpsi), and specific examples like the Ponzano-Regge vertex amplitude (eq: pr h) can be rewritten in this representation. This representation simplifies the evaluation of vertex amplitudes, particularly when combined with the Segal-Bargmann transform.\n\nThe connection between the two descriptions of the discrete geometry associated with @xmath63 variables, mentioned in the text, highlights the correspondence between classical fluxes and holonomies (description (a)) and the more geometric quantities like areas, angles, and normals (description (b)). These descriptions are related through a suitable parametrization that reduces the coherent spin-networks to superpositions of Livine-Speziale spin-networks for large areas.\n\nIn summary, coherent spin-networks and their holomorphic representation offer a powerful tool for studying spinfoam amplitudes in loop quantum gravity, enabling a more transparent connection to classical geometry and facilitating the exploration of the theory's semiclassical behavior. In this section, the paper presents a novel representation of spinfoams, which relies on coherent spin-networks. Coherent spin-networks serve as an overcomplete basis for the boundary Hilbert space, labeled by elements from a set associated with each link in the graph. These states are characterized by complex numbers and parameters that encode the heat-kernel on a specific manifold, enabling a Segal-Bargmann transform for loop quantum gravity. The scalar product of a coherent spin-network with a given state provides a holomorphic function, well-suited for analyzing the theory's phase space.\n\nThe authors emphasize the significance of these labels, not only in representing the phase space but also in their geometric interpretation in terms of classical holonomies and fluxes. This representation has been inspired by the work of Hall and offers a clearer understanding of the discrete geometry associated with the spinfoam vertices. Using the holomorphic representation, the Ponzano-Regge vertex amplitude can be expressed more concisely, revealing a direct connection between the spin-network labels and classical configurations.\n\nTwo equivalent descriptions of the discrete geometry are discussed: one based on Ashtekar variables, suitable for the canonical formulation of loop quantum gravity, and the other in terms of Freidel-Speziale variables for twisted geometries, commonly used in covariant approaches. The authors demonstrate that coherent spin-networks with specific labels reduce to Livine-Speziale spin-networks for large areas, a connection that is crucial for understanding the semiclassical behavior of spinfoams.\n\nIn the context of spinfoam vertices, the Lorentzian vertex amplitude is defined and its holonomy and holomorphic representations are derived. A specific choice, known as EPRL-FK, is introduced using principal series representations of the Lorentz group. The vertex amplitude is gauge-invariant and involves a two-step process: mapping the boundary state into a coherent spin-network state and imposing Lorentz invariance at the nodes. The resulting expressions exhibit a clear connection to the Segal-Bargmann transform and provide insights into the semiclassical behavior of the spinfoam amplitudes.\n\nThe authors conclude by highlighting the utility of the holomorphic representation in studying the semiclassical limit, particularly through a comparison with a simple quantum mechanical system. They plan to further explore the interplay between the different representations in the context of the 4D Lorentzian vertex amplitude and its implications for loop quantum gravity. In the realm of loop quantum gravity, the application of coherent spin-networks and their associated holomorphic representation has been relatively uncharted until recently. This work explores the potential benefits of utilizing the holomorphic formalism within spinfoam theory. A key feature is the ability to represent spinfoam vertices, as seen in equation (eq: wpsi), in terms of these labels, which offer a transparent connection to classical discrete geometries.\n\nThe Ponzano-Regge vertex amplitude, for instance, when expressed in the holomorphic representation, takes the form given by equation (eq: pr h). The utility of this representation lies in the straightforward interpretation of the complex labels, which are directly linked to classical fluxes and holonomies. Two equivalent descriptions of the discrete geometry associated with these labels are discussed: (a) the one from the perspective of canonical LQG, and (b) the covariant spinfoam framework's Freidel-Speziale variables for twisted geometries.\n\nIn the first description (a), the flux of the electric field through a face is represented by an algebra element, while the holonomy of the connection is a group element. These together form a point in a truncated phase space associated with the graph. Coherent spin-networks, with labels as in equation (eq: complexification), are tailored to these configurations, being peaked on the classical configuration.\n\nIn the second description (b), the geometry is described in terms of areas, angles, and normals, reflecting the twisted nature of the spacetime. Here, each cell is equipped with a Euclidean geometry, and shared faces must have equal areas. The extrinsic angle and the normal vectors encode the rotation information. \n\nThe conversion between the flux/holonomy description (a) and the area/angle/normals description (b) is facilitated by recognizing the underlying structures in the polar decomposition. In the context of spinfoams, this allows for a more explicit understanding of their behavior in the semi-classical regime.\n\nThe discussion of the Lorentzian vertex amplitude further emphasizes the relevance of the holomorphic representation. For a specific choice of the map (EPRL-FK), the vertex amplitude is derived, and its holonomy and holomorphic forms are presented. These representations play a crucial role in analyzing the semiclassical properties of spinfoams, as illustrated through a quantum mechanical example involving a particle on a line.\n\nIn conclusion, the exploration of coherent spin-networks and their holomorphic representation within spinfoams not only deepens our understanding of the In the context of the discussion on spin-foam models in quantum gravity, the paragraph discusses the relationship between the geometric variables associated with a cellular decomposition of a boundary 3-manifold and the corresponding elements in the phase space of general relativity. For each link in the graph dual to the cellular decomposition, there are three key variables: an area, an extrinsic angle, and two unit normals. These data, together with the angular information, are used to construct a Lorentz group element represented by a complexified version of the group element, denoted as @xmath62. The choice of parametrization in terms of the extrinsic angle and normals is crucial for understanding the interpretation of these variables as classical fluxes and holonomies.\n\nCoherent spin-networks, which label the graph, are particularly relevant in this context, as they become peaked on classical configurations when the areas are large. The connection to twisted geometries, discussed in the reference, is significant for understanding the semi-classical behavior of these spin-foams. The passage also mentions the relation between the flux-holonomy description and the one involving areas, angles, and normals, which can be derived through a polar decomposition.\n\nThe vertex amplitude in the holonomy representation, which is derived from the Euclidean spinfoam vertex, involves integrals over the representation spaces and is defined using maps that preserve the diagonal action of the Lorentz group. The EPRL-FK vertex, a specific example, is introduced through a particular choice of embedding into the Lorentz group's representations. The vertex amplitude in this case is expressed as a sum over classical solutions, highlighting the importance of the holomorphic representation for studying the semiclassical limit due to its ability to isolate the contribution from a single classical trajectory.\n\nIn conclusion, the paragraph presents the mathematical framework for encoding geometric data in a spin-foam model, emphasizing the role of coherent spin-networks and the connection between different representations in understanding the dynamics of quantum gravity. The holomorphic representation, being particularly suited for analyzing semiclassical behavior, plays a central role in this context.",
        "abstract": " we study a holomorphic representation for spinfoams . the representation is obtained via the ashtekar - lewandowski - marolf - mouro - thiemann coherent state transform . \n we derive the expression of the 4d spinfoam vertex for euclidean and for lorentzian gravity in the holomorphic representation . \n the advantage of this representation rests on the fact that the variables used have a clear interpretation in terms of a classical intrinsic and extrinsic geometry of space . \n we show how the peakedness on the extrinsic geometry selects a single exponential of the regge action in the semiclassical large - scale asymptotics of the spinfoam vertex . ",
        "section_names": "introduction\ni. spin foams in various representations\nii. the holomorphic representation\niii. 4d euclidean vertex amplitude in the holomorphic representation\niv. 4d lorentzian vertex amplitude in the holomorphic representation\nv. semiclassical analysis: role of the extrinsic curvature in the large spin asymptotics\nvi. conclusions and perspectives\nacknowledgments"
    },
    {
        "article": "Driven diffusive systems, particularly asymmetric simple exclusion processes (ASEPs), have garnered significant interest from physicists due to their intricate and diverse behavior. These systems, serving as a simplified model, have been extensively studied in chemistry, physics, and even biology, where they find applications in various processes such as gel electrophoresis, protein synthesis, mRNA translation, molecular motor movement, and microtubule dynamics. A totally asymmetric simple exclusion process (TASEP) is considered the minimal model for ASEP, representing particles moving unidirectionally.\n\nIn recent years, TASEP has been used to model intracellular transport and traffic problems, with notable contributions from researchers like MacDonald et al., who simulated ribosome dynamics, and Kruse and Sekimoto, who introduced a two-headed TASEP for molecular motor traffic. Lipowsky et al. investigated the density and current profiles of motors on different microtubule tracks, while Parmeggiani et al. extended the single-lane TASEP to incorporate Langmuir kinetics (particle attachment and detachment), known as the PFF model, revealing unexpected stationary regimes for finite systems.\n\nNishinari et al. proposed a model combining TASEP, Langmuir kinetics, and Brownian ratchet mechanism for simulating kinesin motor movement. This model captures ATP hydrolysis and the ratchet mechanism, reproducing experimental observations in the low-density limit. The interest in multi-lane ASEP systems has grown, as experimental evidence suggests that molecular motors can move between parallel microtubules without constraints.\n\nPronina and Kolomeisky's work introduced a two-lane model with symmetric lane-changing rules, demonstrating the impact of lane-changing rates on the system's steady-state properties. Their findings revealed a more complex phase diagram with seven phases compared to three in symmetric systems. Other studies have investigated synchronization of kinks and the effects of lane-changing rates on particle currents and densities.\n\nThis paper aims to investigate the collective effect of particle attachment and detachment in both lanes of a two-lane system with symmetric inter-lane coupling, focusing on finite-size effects. The model, based on TASEP, Langmuir kinetics, and lane-changing, aims to provide insights into molecular motor traffic and related biological processes, as well as potential applications in vehicular traffic. The study explores the influence of parameters such as lane-changing rates, attachment and detachment rates, and system size through Monte Carlo simulations and mean-field approximation, providing a comprehensive analysis of the system's dynamics. The Parmeggiani et al. model, referred to as the PFF model, highlights the presence of unexpected stationary regimes in large yet finite particle systems, characterized by phase coexistence in both low and high density zones separated by domain walls. This behavior was later addressed by Nishinari et al. through a model integrating a TASEP (Totally Asymmetric Simple Exclusion Process), Langmuir kinetics, and Brownian ratchet mechanism, which mimics the movement of single-headed kinesin motor KIF1A. A key difference in their model is the introduction of three distinct states (strong binding, weak binding, and no binding) compared to the conventional two-state systems.\n\nThe PFF and Nishinari et al.'s models have successfully captured the ATP hydrolysis effects and the ratchet mechanism driving individual motors, reproducing experimental observations in the low-density regime. However, most previous works on molecular motor traffic modeling focused on single-lane systems with binary attachment/detachment dynamics. The impact of multi-lane, asymmetric systems, which allow particles to switch lanes, has been gaining interest due to experimental evidence of kinesins moving between parallel microtubule protofilaments.\n\nRecent studies have investigated the influence of lane-changing rates on the traffic properties of multi-lane systems. Pronina and Kolomeisky's work showed that increased lane-changing can lead to decreased particle currents and increased densities. Jiang et al. further incorporated Langmuir kinetics into a two-lane system, revealing synchronization of shocks under specific conditions. These models, however, either lack simultaneous particle attachment and detachment on both lanes or simplify the process.\n\nIn this paper, we aim to explore the collective effect of simultaneous attachment and detachment on both lanes in a two-lane system with symmetric inter-lane coupling. By varying lane-changing rates, attachment and detachment probabilities, and system sizes, we seek to understand how these parameters influence density and current profiles. Our Monte Carlo simulations (MCS) are conducted using a two-lane lattice with updating rules that incorporate the mentioned processes. Mean-field approximation (MFA) is employed to validate the simulation results, providing insights into the dynamics of interacting particles in a biological and non-biological context.\n\nThe investigation of these parameters promises to enhance our understanding of molecular motor traffic and potentially shed light on particle transport in various applications, such as vehicular traffic. The findings from this study can contribute to the development of more accurate models that better reflect the complex behaviors observed in real-life scenarios. In the realm of biological systems, the study of motor protein dynamics, particularly kinesins moving along microtubules, has led to the development of two-dimensional models, such as the two-lane traffic simulation proposed by Pronina and Kolomeisky@xcite. This model, devoid of Langmuir kinetics, explores the effects of symmetric lane-changing rules on the behavior of particles. The computational results reveal that variations in lane-changing rates significantly impact the steady-state properties, with particle currents decreasing and densities increasing as coupling increases.\n\nBuilding upon this framework, Pronina and Kolomeisky extended their model to account for asymmetric coupling, which generates a much more intricate phase diagram with seven distinct phases compared to the three in the symmetric case. This asymmetric interaction between lanes introduces complex dynamics that challenge the synchronization of kinks, or domain walls, as investigated by Mitsudo and Hayakawa@xcite. The presence of different injection and ejection rates at lane boundaries further complicates the dynamics.\n\nRecent advancements by Jiang et al.@xcite introduced Langmuir kinetics into one lane, revealing a critical threshold for synchronized shock formation in two-lane systems. They observed a boundary layer as a finite-size effect. However, this study's assumption of particle attachment and detachment occurring exclusively on one lane limits its realism in modeling biological systems.\n\nThe current work aims to delve deeper into the collective effects of simultaneous particle attachment and detachment on both lanes with symmetric inter-lane coupling. By investigating varying lane-changing rates, attachment/detachment probabilities, and system sizes, the authors aim to understand the role of these parameters on density and current profiles. Through Monte Carlo simulations, they will explore how these factors influence the dynamics of a two-lane system, which could provide valuable insights into the complex traffic patterns observed in molecular motor systems and other biological contexts.\n\nThe model described involves a two-lane lattice with particles moving from left to right, undergoing lane selection, injection, detachment, movement, lane-changing, attachment, and ejection. The updating rules, which incorporate symmetric lane-changing, are stochastic and mimic the asynchronous nature of molecular motor traffic. The results will be compared using mean-field approximation to confirm the validity of the Monte Carlo simulations and offer a comprehensive understanding of the interplay between lane-changing, attachment/detachment, and system size in shaping the system's behavior. In this study, the authors delve into the collective behavior of particles in a two-lane system with symmetric inter-lane coupling, where attachment and detachment events occur on both lanes. By extending the previous work that employed Langmuir kinetics in a single lane, the researchers aim to better understand the synchronization of shocks and finite-size effects in real multi-lane molecular motor traffic. The model incorporates the dynamics of a Totally Asymmetric Simple Exclusion Process (TASEP) and Langmuir kinetics, allowing for a more realistic description of particle traffic.\n\nThe parameters investigated include varying lane-changing rates, different combinations of attachment and detachment rates, and the system size itself. The team uses Monte Carlo simulations to analyze how these factors influence the density and current profiles. They find that the domain wall, representing the boundary between the two lanes, exhibits a unique \"jumping effect,\" where it initially moves left and then shifts right as the lane-changing rate increases. This phenomenon is sensitive to attachment and detachment rates, with a critical value determining the direction of the domain wall's movement.\n\nThe authors also demonstrate that the jumping effect is a finite-size effect, disappearing as the system size grows larger. This is important because in biological contexts, such as molecular motor traffic in cells, the system size is often not exceedingly large. By studying finite-size effects, the results can be applied more accurately to real-world scenarios.\n\nThe comparison between the two-lane system with equal lanes (Figures 2(a) and 2(b)) and the separated TASEPs with Langmuir kinetics (Figure 4(a) and 4(b)) highlights the impact of increased attachment and detachment rates on the slope of the domain walls. This insight can contribute to understanding the complex dynamics of particle traffic in various biological and non-biological contexts. Future work will likely explore the effects of asymmetric lane-changing rates to further refine the model. The next paragraph would continue discussing the findings from the Monte Carlo simulations, specifically focusing on the influence of lane-changing rates (@xmath3) on the density and current profiles in the two-lane system. The authors note that an intriguing phenomenon, referred to as the \"jumping effect,\" emerges where the domain wall initially moves left before switching to the right with increasing @xmath3. This behavior is observed for various system sizes, with a critical value, @xmath55, determining the direction of the domain wall movement. They also observe that the slope of the domain walls increases proportionally with higher attachment and detachment rates (@xmath7 and @xmath8), indicating a shift in particle distribution between lanes. The impact of @xmath3 on the system properties, such as the average density and current profiles, is discussed, revealing that beyond a certain maximum density, the rate does not significantly affect these characteristics. The study's results emphasize the importance of considering finite-size effects, as the jumping effect vanishes for extremely large systems. Lastly, it is mentioned that the maximum current, @xmath69, is observed at a specific lane-changing rate, @xmath66, and that there exist distinct current trends depending on the ratio of @xmath3 to @xmath39. Overall, these simulation results contribute to understanding the complex dynamics of interacting particles in a two-lane TASEP with varying lane-changing rates and attachment/detachment rates. The Monte Carlo simulations and mean-field theory findings in the study confirm the validity of the proposed model. The system is described using a two-lane lattice with @xmath11 sites, where @xmath4 represents the length of each lane. Particles move from left to right, as depicted in Figure 1. The left boundary is at @xmath12=1 and the right at @xmath12=@xmath4, while sites @xmath12=2 to @xmath13 form the bulk. Various processes occur in the system, including lane selection, particle injection, detachment, movement, lane-changing, attachment, and ejection.\n\nThe updating rules for the model are meticulously defined, with equivalent updating rules for both lanes. The state of each site is represented by an occupation variable @xmath18, with @xmath19 denoting the lane. The simulation results, as presented, involve analyzing the average current @xmath38, which is calculated using the formula @xmath38 = @xmath39(1 - @xmath40). Parameters such as @xmath41, @xmath42, @xmath43, @xmath44, @xmath45, and @xmath7 relative to @xmath8 are used, with a symmetric lane-changing probability.\n\nThe authors observe a peculiar phenomenon in their simulations: as @xmath3 increases, the domain wall initially moves left before shifting right. This \"jumping effect,\" characterized by the domain wall's alternating movement, is a unique feature that was not anticipated. The jumping effect is sensitive to parameters like @xmath7 and @xmath8, with their proportional increase reducing its intensity. In larger systems, the jumping effect fades away, suggesting its dependence on finite-size effects. \n\nThe influence of the lane-changing rate @xmath3 on the system properties, such as average density and current profiles, is also studied. It is found that the current initially decreases with increasing @xmath3 until reaching a maximum at @xmath66, after which it levels off at @xmath67 @xmath39(1 - @xmath68). The maximum current, @xmath69, is 0.25 under these conditions. The effect of varying @xmath46, which relates to @xmath7 and @xmath8, reveals an increase in average density accompanied by a leftward shift of the In the context of the scientific text provided, the next paragraph would discuss the results and analysis of Monte Carlo simulations conducted to study the dynamics of interacting particles in a two-lane system with Langmuir kinetics. The simulations reveal an intriguing phenomenon known as the \"jumping effect,\" where the domain wall (a signature of the particles' movement) initially moves left before switching to the right as the parameter @xmath3 (representing the lane-changing rate) increases. This behavior is observed for different values of @xmath3 and is influenced by the attachment and detachment rates. The authors also mention that the jumping effect weakens as the ratio of @xmath7 to @xmath8 increases proportionally, suggesting that the system's properties are affected by the relative rates.\n\nThe simulations show that the maximum current is around 0.25 when the densities of adjacent sites are equal to 0.5, and the current profiles exhibit non-monotonic changes with varying @xmath3. An increase in @xmath46 (related to the ratio @xmath73/@xmath8) leads to an increase in average density and a leftward shift of the shock, indicating that particles are more likely to remain in the bulk. The mean-field theory developed helps explain these observations by considering the interplay between particle movement, lane changes, and attachment/detachment processes.\n\nThe study further supports the idea that the jumping effect is a finite-size effect, disappearing as the system size increases. The density and current profiles exhibit characteristic changes with system size, suggesting that understanding these finite-size effects is crucial for accurately modeling real-world systems, such as those involving molecular motors like kinesin proteins. Overall, the simulations contribute valuable insights into the complex dynamics of multi-lane particle systems with kinetic constraints. In the context of the provided scientific text discussing the dynamics of interacting particles in a two-lane system with lane-changing rules, the next paragraph could be:\n\nContinuing from the previous description, the authors delve into the analysis of the Monte Carlo simulations, which reveal intricate behaviors. The simulation results, presented in Figure 2, show how the average density and current vary with an important parameter, @xmath3, representing the lane-changing rate. As @xmath3 increases, a peculiar phenomenon called the 'jumping effect' emerges, where the domain wall initially moves left before shifting to the right. This unexpected behavior is attributed to particles jumping between lanes. The authors also find a critical value, @xmath55, that distinguishes the direction of the domain wall's motion. The influence of attachment and detachment rates, represented by @xmath7 and @xmath8, on this jumping effect is discussed, with simulations demonstrating that it weakens as their ratio increases.\n\nMoreover, the study considers the effect of different system sizes on the jumping effect, observing that it disappears in the limit of very large systems. The authors emphasize the importance of examining finite-size effects in real-world applications, such as those involving molecular motors like kinesin proteins, where a system size of around 1,000 is considered sufficient for accurate modeling.\n\nThe mean-field theory, introduced later in the text, offers a mathematical framework to understand the evolution of particle densities. By approximating correlation functions, the authors derive a continuum mean-field equation that simplifies in the limit of large @xmath4, providing insights into the stationary-state behavior of the system. This theoretical analysis supports the observation that, at equilibrium, the density profiles exhibit a transition region with a decreasing width due to the interplay between particle movement, lane-changing, and attachment/detachment processes.\n\nIn summary, the paragraph continues the exploration of the simulation results and the theoretical understanding of the two-lane system, highlighting the role of key parameters and the significance of considering finite-size effects in real-world applications. In the simulations, a unique phenomenon known as the \"jumping effect\" is observed, where the domain wall initially moves left before transitioning to the right with an increase in a parameter denoted by '@xmath3'. This behavior, characterized by particles jumping between lanes, is particularly evident in Fig. 2(a) and (b), and Fig. 3(a) and (b). The occurrence of this effect is sensitive to the ratio of attachment and detachment rates, '@xmath7' and '@xmath8', as shown in the comparison between Fig. 2 and Fig. 4. The authors also find a critical value, '@xmath55', that separates the direction of the domain wall movement.\n\nThe jumping effect is demonstrated in a larger system, @xmath56, where for small '@xmath3', the domain wall exhibits alternating movement. The influence of the system size is discussed, suggesting that for real-world applications involving molecular motors, a system of around 1,000 sites can provide a reasonable representation. As the system size increases, the jumping effect weakens, supporting the idea that it is a finite-size effect.\n\nThe dependence of the average density and current profiles on '@xmath3' is investigated. The current initially decreases with increasing '@xmath3', reaching a maximum value of @xmath65 when '@xmath66' is close to @xmath39. The position of maximum current shifts left as '@xmath3' increases. The effect of '@xmath46', which relates to the ratio of attachment and detachment rates, is also studied. An increase in '@xmath46' leads to an increase in average density, with the shock moving left and the transition region narrowing.\n\nA mean-field theory is developed to describe the dynamics, providing equations that match the Monte Carlo simulations. In the limit of large '@xmath4', the mean-field approximation accurately captures the stationary-state behavior. The comparison between the mean-field results and the Monte Carlo simulations, as shown in Fig. 8, confirms the validity of the theoretical predictions.\n\nOverall, these findings provide insights into the complex behavior of two-lane totally asymmetric exclusion processes with Langmuir kinetics, highlighting the importance of considering finite-size effects and the interplay between particle movement and lane-changing dynamics. The domain wall movement in the two-lane Totally Asymmetric Simple Exclusion Process (TASEP) with Langmuir kinetics exhibits a peculiar \"jumping effect,\" characterized by an initial leftward motion followed by a subsequent rightward shift due to particles switching lanes. This phenomenon is observed when parameters like @xmath3 and @xmath54 change, with a critical value, @xmath55, distinguishing the direction of the domain wall. The jumping effect is influenced by both attachment and detachment rates, as demonstrated in the comparison between Fig. 2 and Fig. 4. It is also noted that for larger systems, like @xmath56, the effect weakens at smaller @xmath3.\n\nIn these simulations, the authors observe that the domain wall's leftward movement ceases when @xmath57, and rightward motion starts at @xmath58. The critical value @xmath3, governing the jumping effect, varies with system size, shifting from 0.2 to 0.3 as the system expands from 1,000 to 10,000 sites. The presence of a finite-size effect is crucial, as the effect vanishes for very large systems, akin to the scenario in cellular transport, where kinesin proteins move within cells.\n\nThe study reveals that the slope of domain walls increases with proportional increases in @xmath7 and @xmath8, indicating a preference for particles to attach to the lanes. However, when @xmath3 increases significantly, the density decreases, and the shocks shift rightward. The lane-changing rate @xmath3 does not significantly affect system properties after reaching maximum density, as seen in the current profiles.\n\nMean-field theory, developed in the text, provides a theoretical framework to understand these behaviors. It successfully predicts the density and current profiles, supporting the Monte Carlo simulations' results. The agreement between the two methods validates the mean-field approximation for the studied two-lane TASEP with Langmuir kinetics under specific conditions.\n\nOverall, this research contributes to the understanding of complex dynamics in multi-lane systems with particle movement, highlighting the importance of considering finite-size effects in modeling real-world systems.",
        "abstract": " in this paper , we study a two - lane totally asymmetric simple exclusion process ( tasep ) coupled with random attachment and detachment of particles ( langmuir kinetics ) in both lanes under open boundary conditions . \n our model can describe the directed motion of molecular motors , attachment and detachment of motors , and free inter - lane transition of motors between filaments . in this paper \n , we focus on some finite - size effects of the system because normally the sizes of most real systems are finite and small ( e.g. , size @xmath0 ) . \n a special finite - size effect of the two - lane system has been observed , which is that the density wall moves left first and then move towards the right with the increase of the lane - changing rate . \n we called it the jumping effect . \n we find that increasing attachment and detachment rates will weaken the jumping effect . \n we also confirmed that when the size of the two - lane system is large enough , the jumping effect disappears , and the two - lane system has a similar density profile to a single - lane tasep coupled with langmuir kinetics . \n increasing lane - changing rates has little effect on density and current after the density reaches maximum . also , lane - changing rate has no effect on density profiles of a two - lane tasep coupled with langmuir kinetics at a large attachment / detachment rate and/or a large system size . \n mean - field approximation is presented and it agrees with our monte carlo simulations . ",
        "section_names": "introduction\nmodel\nmonte carlo simulations\nmean-field approximation\nconclusion\nacknowledgements"
    },
    {
        "article": "The Coupled Cluster (CC) method, as implemented in the XCite package, is a powerful and widely employed technique for tackling quantum many-body problems. It relies on an exponential ansatz for the wavefunction, which is constructed using a cluster operator that involves amplitudes representing multiple particle-hole excitations from a reference Slater determinant. This ansatz, though containing an infinite number of terms due to the exponentiation, results in a finite number of equations for the cluster amplitudes when solving the eigenvalue problem. However, when these wavefunctions are used in calculations of matrix elements, the expansion leads to an infinite series again.\n\nIn this context, the main focus of this work is to address the challenge of summing up an infinite number of terms in the matrix element expansion, particularly for transitions between univalent atoms, such as alkali-metal species. The Linearized Coupled Cluster (LCC) method, which neglects non-linear terms, has been applied in previous studies but falls short of capturing a significant portion of the Random Phase Approximation (RPA) diagrams. To rectify this, researchers have employed different strategies, like direct RPA dressing or manual removal of diagrams, which can lead to double-counting.\n\nOur contribution presents an alternative infinite-series summation scheme for the RPA chain that avoids double counting and streamlines the process. This scheme, in addition to addressing the RPA-like dressing of CC diagrams, also considers a subset that dresses particle and hole lines within the CC diagrams. The corrections introduced by this dressing scheme arise at the fourth order of Many-Body Perturbation Theory (MBPT). The paper presents a detailed comparison with relevant fourth-order diagrams and demonstrates its effectiveness through numerical illustrations for the hyperfine-structure constants and dipole matrix elements of the cesium (Cs) atom.\n\nThe paper's organization begins with a comprehensive discussion of the CC formalism in Section \\ref{sec:ccformalism}, followed by the dressing of particle and hole lines in Section \\ref{sec:particlehole} and the RPA-like dressing in Section \\ref{sec:rpa}. It serves as an extension of the fourth-order calculation in Ref. \\cite{reference}, and Section \\ref{sec:iv} provides a comparison with fourth-order diagrams. Numerical results are presented in Section \\ref{sec:numerics}, and conclusions are drawn in Section \\ref{sec:conclusion}. Throughout the paper, atomic units are used, and Brueckner-Goldstone diagrams follow a specific convention. In this work, we address the limitations of the linearized coupled-cluster (LCC) method, which neglects non-linear terms in the expansion for univalent atoms, leading to a missed fraction of the random-phase approximation (RPA) diagrams. To rectify this issue, we propose an alternative infinite-series scheme for the RPA chain that avoids double-counting without the need for manual diagram removal. Our approach not only dresses the coupled-cluster diagrams for matrix elements but also considers a different subset of diagrams affecting particle and hole lines, contributing to a fourth-order many-body perturbation theory (MBPT) dressing.\n\nWe present a detailed comparison with the relevant fourth-order diagrams from direct MBPT calculations in the context of the cesium (Cs) atom, where we extend the fourth-order LCC results to completeness. By incorporating certain diagrams from fourth-order MBPT, our calculations provide a comprehensive treatment through the fourth order. The paper is structured to provide a comprehensive overview of the coupled-cluster formalism, its approximations, and the matrix element calculation technique. We delve into the dressing of particle and hole lines, as well as the RPA-like dressing, in sections dedicated to these aspects.\n\nThe numerical illustrations in Section VI showcase the effectiveness of our approach, and the conclusions are drawn in Section VII, emphasizing the importance of our all-order extension for accurate predictions in univalent systems. The use of atomic units and standard conventions for drawing Brueckner-Goldstone diagrams is maintained throughout the study. This work significantly improves upon previous LCCSD calculations by providing a more accurate and systematic treatment of RPA and MBPT corrections in Cs, setting a new benchmark for future studies in this field. In this section, the authors delve into the all-order extension of fourth-order Many-Body Perturbation Theory (MBPT) calculations, specifically for the Cs atom, by introducing a new dressing scheme for the Coupled Cluster (CC) diagrams. This scheme addresses the issue of double counting and eliminates the need for manual removal of \"extra\" diagrams. They consider both RPA-like dressing for matrix elements and another subset of diagrams that dress particle and hole lines in the CC diagrams.\n\nThe leading corrections from this dressing scheme appear at the fourth order of MBPT, and the paper presents a detailed comparison with relevant fourth-order diagrams. The authors emphasize the importance of completeness, as they incorporate certain classes of diagrams from direct fourth-order MBPT calculations while maintaining the full fourth-order result. The paper's organization is as follows: a comprehensive discussion of the CC formalism is presented in Section \\ref{sec:ccformalism}, followed by the dressing of particle and hole lines in Section \\ref{sec:particlehole}, and RPA-like dressing in Section \\ref{sec:rpa}. In Section \\ref{sec:iv}, a comparison with fourth-order diagrams is illustrated numerically, and the devised summation schemes are showcased in Section \\ref{sec:numerics}. Finally, Section \\ref{sec:conclusion} summarizes the findings and concludes the study.\n\nThe discussion revolves around the use of atomic units, adhering to the conventions established in Ref.~\\cite{xcite}. The focus is on atomic systems with one valence electron outside a closed shell core, and the formalism is applied to compute matrix elements for such systems. The authors outline the CC formalism, discuss approximations like the CCSD and LCCSD, and emphasize the role of disconnected diagrams in cancellations when computing matrix elements.\n\nBy dressing the CC diagrams with the proposed scheme, the authors aim to enhance the accuracy of their calculations, particularly for the fourth-order corrections. The dressed CC diagrams are then used to compute hyperfine-structure constants and dipole matrix elements for the Cs atom, demonstrating the effectiveness of their approach. In Section IV, the paper presents a numerical illustration of the designed summation schemes, showcasing the performance and accuracy of the proposed methods. This section supports the theoretical discussions by comparing the fourth-order diagrams (IV-Order Diagrams) against the results obtained using the coupled-cluster method. The comparison serves as a benchmark for evaluating the efficiency and reliability of the proposed approach.\n\nThe coupled-cluster formalism, as discussed earlier, is tailored for atomic systems with one valence electron beyond a closed-shell core. The authors review various approximations and outline the steps to compute matrix elements, which are crucial for solving the atomic many-body problem. The total Hamiltonian is divided into a core part and a residual interaction, where the frozen-core Hartree-Fock Hamiltonian is typically chosen as the starting point.\n\nThe matrix elements required for calculations are derived from the eigenvalue equation, and disconnected diagrams are shown to cancel out in the final expression, Eq. (12). The core and valence contributions are separated, with the valence part being the primary focus due to its relevance to non-scalar operators.\n\nRepresentative lccsd (linearized coupled-cluster singles and doubles) diagrams are depicted in Figure 3, highlighting the key RPA (random-phase approximation) and BO (Brueckner-orbital) corrections. These diagrams serve as a starting point for the more advanced dressing process, which involves nonlinear CC (coupled-cluster) terms to account for higher-order correlations.\n\nThe dressing procedure, outlined in Section III, is applied systematically to both particle and hole lines in the diagrams. All-order insertions are derived algebraically, starting from a \"seed\" diagram and iterating through a series of contractions. This leads to the construction of dressed particle and hole lines, represented by Eq. (76) and (77), which are then used to update the lccsd diagrams.\n\nNumerical results for the dressed diagrams, as shown in Figure 4, demonstrate the effectiveness of this dressing scheme in enhancing the accuracy of the calculations. These computations not only improve upon the lccsd results but also address the limitations of the earlier work by Ref. [xcite].\n\nIn Section V, the conclusions are drawn, summarizing the main findings and the significance of the developed dressing methodology for atomic systems. The section also discusses potential future extensions and applications of the technique to other many-body problems. In the context of the coupled cluster method applied to univalent systems, the discussion continues with the mathematical formalism involving the Wick theorem. The central expression, Equation \\ref{eq : zconn}, is decomposed into core and valence contributions, with the core part vanishing for non-scalar operators due to the closed-shell nature of the core. The focus shifts to the valence part, represented by diagrams like those in Figure \\ref{fig : zsdrepresentative}, which are derived from the linearized coupled-cluster singles and doubles (LCCSD) approximation.\n\nThe objective of this work is to extend the LCCSD results by considering higher-order corrections due to nonlinear cluster terms. This is achieved by dressing the \"skeleton\" diagrams with these non-linear contributions. The dressing process involves expanding the product in terms of normal forms using the Wick theorem, as expressed in Equation \\ref{eq : zconn}, and subsequently dressing particle and hole lines with all-order insertions.\n\nA key aspect of this dressing procedure is illustrated through the explicit expressions for one-body term insertions, represented as rectangles in Figure \\ref{fig : dressinggeneral}. These insertions are generated by iteratively solving implicit equations, as seen in Equation \\ref{eq : dressinggeneral}, where the dressed core cluster amplitudes are computed.\n\nBy introducing dressed matrix elements and valence cluster amplitudes, the LCCSD diagrams can be upgraded, as shown in Figure \\ref{fig : zsdrepresentativedressed}, where the line-dressed matrix elements and double-lined valence cluster amplitudes are the dressed versions. The numerical computations for these dressed diagrams are discussed in Section \\ref{sec : numerics}.\n\nIn summary, this paper aims to provide a systematic method for dressing coupled-cluster diagrams for matrix elements, going beyond the linearized LCCSD approximation and incorporating the effects of nonlinear cluster terms. The dressing scheme, as outlined, offers a rigorous and efficient means to compute corrections to properties of univalent systems, ultimately refining the accuracy of the theoretical predictions. In the context of quantum chemistry, the authors delve into the application of the Coupled Cluster method (CC) for calculating properties of univalent systems. They have derived an expression for the exact matrix element, represented by Equation \\ref{eq:zconn}, which separates core and valence contributions. The key component, the normalization factor, is formed from connected diagrams, where disconnected ones cancel out. The focus shifts to the valence part as the core contribution vanishes for non-scalar operators.\n\nThe authors utilize the LCCSD wavefunction parametrization to obtain 21 diagrams for the valence sector and 5 contributions for the core part. They aim to extend these linearized LCCSD results by considering nonlinear CC terms, which will dress the \"skeleton\" diagrams provided by LCCSD. Representative diagrams are showcased in Figure \\ref{fig:zsdrepresentative}.\n\nThe coupled cluster method involves manipulating expressions using the Wick's theorem to simplify and expand products of operators. By applying the theorem, the authors demonstrate how to dress particle and hole lines in diagrams, as illustrated in Figure \\ref{fig:dressinggeneral}. This dressing process is carried out systematically, starting from a \"seed\" diagram and iterating through contractions to generate all-order insertions.\n\nThe one-body term in the expansion is particularly important, as it contributes to the dressing of particle and hole lines. The dressed particle and hole line insertions are defined algebraically, where each term represents a specific connection pattern. For example, the dressed particle-line insertion is given by Equation \\ref{eq:dressed_line_insertion}.\n\nThe paper further discusses the dressing of RPA-like diagrams, represented by the rpa-dressed object, Equation \\ref{eq:rpa_dressed_object}, which is obtained through a similar iterative process. The resulting equations for dressed matrix elements resemble those of traditional RPA but with modified couplings and a different source of interaction.\n\nIn summary, the work presented explores the extension of the LCCSD approximation in quantum chemistry by incorporating nonlinear CC terms to dress diagrams, focusing on the one-body and RPA-like contributions. The method allows for a more accurate representation of matrix elements and offers a deeper understanding of the system's electronic structure. In the realm of computational chemistry, the Local Cluster Configuration Interaction with Single and Double excitations (LCCSD) approximation offers a powerful framework for refining Hartree-Fock (HF) calculations by accounting for non-linear correlation effects. As depicted in Figure [fig:zsdrepresentative], LCCSD diagrams serve as basic building blocks, which are subsequently \"dressed\" with higher-order corrections from the inclusion of Coulombic interactions. Representative diagrams include those from Random Phase Approximation (RPA) and Brueckner Orbitals (BO), contributing dominantly to the total energy.\n\nThe dressing process involves algebraic manipulations using the Wick's theorem to simplify complex expressions involving creation and annihilation operators. The LCCSD contributions are represented by equations [eq:51-53], where hermitian conjugation and valence index swapping are accounted for. Heavy horizontal lines symbolize cluster amplitudes, while RPA diagrams involve valence double excitations and BO diagrams involve valence single ones.\n\nAt the heart of the method lies the concept of one-body terms, which not only dress particle and hole lines but also generate the essential RPA-like corrections to the diagrams. These corrections are crucial for accurate predictions of molecular properties. By applying the Wick's theorem, products are expanded into normal forms, with zero-body terms and one-body terms affecting particle and hole dressing, respectively.\n\nThe dressing of particle and hole lines is visually represented as rectangles with \"stumps\" in Figure [fig:dressinggeneral], where the particle or hole lines are indicated by attachment points. One-body insertions are derived systematically, leading to the definition of dressed particle-line insertions, which are essential for upgrading LCCSD diagrams.\n\nThe dressing process can be generalized to all particle-hole lines, even those within the original \"bare\" object. Truncating the cluster operator at single and double excitations simplifies the expressions for hole-line insertions, denoted by Equation [eq:76], and particle-line insertions, represented by Equation [eq:77]. Anti-symmetric quantities are introduced to maintain symmetry.\n\nIn numerical computations, the dressing of selected diagrams is demonstrated by replacing bare matrix elements with dressed ones. This not only encompasses the Hartree-Fock diagrams but also eliminates the need for additional LCCSD diagrams from previous work. The results, as shown in Figure [fig:zsdrepresentativedressed], showcase the improved accuracy of the LCCSD approximation after dressing.\n\nIn conclusion, the LCCSD approximation, through its In the context of a theoretical study involving normal forms and particle-hole dressing in quantum many-body systems, the paragraph focuses on the one-body term of a specific product, represented by \"processed_article[-1]\", and its connection to the dressing of particle and hole lines. The one-body term is described as a topological entity with free hole or particle lines entering and leaving more complex structures. It is shown that by considering certain contractions and permutations within the product, one can dress these lines systematically.\n\nThe dressing process is illustrated with diagrams, where rectangles with \"stumps\" indicate attachment points for the particles or holes. The dressing procedure is not limited to individual lines; it can be generalized to simultaneously dress all particle-hole lines of a diagram. The dressing of particle lines is represented by \"dressed_particle-line_insertion\", while the hole-line dressing is similar, with the presence of \"anti-symmetric quantities\".\n\nThe algebraic representation involves dressing core cluster amplitudes and solving an iterative equation to obtain these dressed amplitudes. These dressed amplitudes are then used to upgrade LCCSD (Local Cluster Configuration Interaction Singles and Doubles) diagrams by replacing bare matrix elements with their dressed counterparts.\n\nNumerical computations are presented, demonstrating the application of the dressing scheme to specific diagrams. The RPA (Random Phase Approximation)-like dressing, which involves a two-particle-two-hole part of the product, is also discussed, leading to an implicit equation for the dressed particle-hole insertion. This scheme recovers the RPA diagrams but modifies the role of the residual Coulomb interaction.\n\nIn the context of a truncated cluster operator at single and double excitations, the bare RPA-like insertion is derived, and its dressing is incorporated into LCCSD calculations. The importance of dressing both vertices is emphasized, as the proposed dressing scheme not only upgrades the RPA diagram but also correctly captures the full chain of diagrams.\n\nOverall, this paragraph provides a detailed explanation and demonstration of the dressing formalism applied to one-body terms in a quantum many-body system, highlighting its implications for the improvement of LCCSD calculations and the recovery of RPA results. In the context of quantum field theory, the discussion revolves around the concept of particle and hole lines, represented as \"seed\" diagrams derived from specific contractions. These seeds are further dressed by considering a subset of terms from a given set of equations, Eq. (@xmath59), where contractions are performed within specific operator combinations. The number of ways to choose and contract pairs of @xmath2 operators is given by the binomial coefficient @xmath67. After selecting the pairs, the resulting objects are contracted into a chain, giving rise to a dressed particle-line insertion, denoted as @xmath71.\n\nThe dressing process is generalizable to include simultaneous dressing of all particle-hole lines in a diagram, including inner lines. By truncating the cluster operator to single and double excitations, the hole-line insertion becomes @xmath76 and the particle-line insertion is @xmath77, involving antisymmetric quantities @xmath78. The dressing scheme is illustrated in diagrams and algebraically represented by Eq. (@xmath80), where dressed core cluster amplitudes are computed iteratively.\n\nThe focus shifts to the dressing of coupled-cluster diagrams for matrix elements, considering their topological structure. For the two-body term, an RPA-like dressing is introduced, represented by the RPA-dressed object @xmath98, which ensures that the final result has the same free particle-hole ends as the original bare object. Solving an implicit equation for the RPA-dressed particle-hole insertion, the dressed matrix elements can be obtained, resembling traditional RPA formulas but without coupling specific indices @xmath109 and @xmath110.\n\nIn the case of the truncated cluster operator, the RPA-like dressing of valence contributions to the wavefunction, such as the valence double amplitude @xmath112, involves attaching the RPA object to appropriate vertices. This is demonstrated by Eq. (@xmath118) and generalizes to multiple equivalent vertexes in more complex situations.\n\nThe rpa-like dressing in the context of the coupled-cluster singles and doubles (CCSD) approximation leads to specific expressions for the dressed matrix elements and the rpa-dressed valence doubles. While the CCSD method captures most of the RPA diagrams, the full sequence is recovered when including the non-linear terms in the coupled-cluster equations. However, using the linearized version of the equations might miss part of the RPA diagrams. This highlights the importance of incorporating the ccs The dressing process, as described in the provided text, is a systematic method for refining diagrams in quantum many-body theories, particularly in the context of the coupled-cluster singles and doubles (CCSD) approximation. It generalizes the dressing of particle-hole lines to encompass all lines within a diagram, including inner ones, by introducing dressed core cluster amplitudes and dressed valence cluster amplitudes. This dressing scheme is iterative, where the amplitudes are computed with coefficients obtained from the previous step.\n\nFor diagrams truncated at single and double excitations, the hole-line insertion is represented by @xmath76 and the particle-line insertion by @xmath77, both involving antisymmetric quantities. The dressing scheme is demonstrated by upgrading LCCSD diagrams with these dressed matrix elements and amplitudes, represented visually in Figure [fig:zsdrepresentativedressed].\n\nThe algebraic representation involves solving for the dressed amplitudes in an iterative manner, where the bare matrix elements are replaced with their dressed counterparts. The dressing of the Hartree-Fock diagram includes all LCCSD diagrams, as shown by equation @xmath93. The RPA-like dressing, specific to two-particle-two-hole parts of the object @xmath94, introduces an RPA-dressed object @xmath98 with a contraction condition ensuring the correct free particle-hole ends.\n\nNumerical computations for the four dressed diagrams confirm the effectiveness of this dressing scheme. The RPA-like dressing, applied to the two-body term of the coupled-cluster operator, leads to dressed particle-hole insertions that resemble traditional RPA formulas, but without coupling between different sectors. The residual Coulomb interaction from traditional RPA is replaced by matrix elements of the @xmath111 object, which is second-order in the Coulomb interaction.\n\nIn the CCSD approximation, the bare RPA-like insertion @xmath121 is derived, and by substituting the corresponding ccsd insertion, expressions for dressed matrix elements and RPA-dressed valence doubles are obtained. The rpa-dressing of specific diagrams, such as @xmath125, is shown to be crucial for recovering the entire RPA sequence in a consistent manner.\n\nHowever, the linearized version of coupled-cluster equations, as employed in some studies, might miss certain RPA diagrams. The authors emphasize the importance of including the non-linear ccsd terms in the coupled-cluster equations for a complete treatment.\n\nOverall, the dressing process presented enhances the accuracy of the CCSD method by",
        "abstract": " we consider evaluation of matrix elements with the coupled - cluster method . \n such calculations formally involve infinite number of terms and we devise a method of partial summation ( dressing ) of the resulting series . \n our formalism is built upon an expansion of the product @xmath0 of cluster amplitudes @xmath1 into a sum of @xmath2-body insertions . \n we consider two types of insertions : particle / hole line insertion and two - particle / two - hole random - phase - approximation - like insertion . \n we demonstrate how to `` dress '' these insertions and formulate iterative equations . \n we illustrate the dressing equations in the case when the cluster operator is truncated at single and double excitations . using univalent systems as an example , we upgrade coupled - cluster diagrams for matrix elements with the dressed insertions and highlight a relation to pertinent fourth - order diagrams . \n we illustrate our formalism with relativistic calculations of hyperfine constant @xmath3 and @xmath4 electric - dipole transition amplitude for cs atom . finally , we augment the truncated coupled - cluster calculations with otherwise omitted fourth - order diagrams . \n the resulting analysis for cs is complete through the fourth - order of many - body perturbation theory and reveals an important role of triple and disconnected quadruple excitations . ",
        "section_names": "introduction\ncoupled-cluster formalism for univalent systems\ngenerating object @xmath0\ndressing particle and hole lines\nrpa-like dressing\ncomparison with the iv-order diagrams\nnumerical results and discussion\nconclusion"
    }
]