Many intelligent robots have come and gone, failing to become a commercial success. We’ve lost Aibo, Romo, Jibo, Baxter—even Alexa is reducing staff. Perhaps they failed to reach their potential because you can’t have a meaningful conversation with them. We are now at an inflection point: AI has recently made substantial progress, speech recognition now actually works, and we have neural networks in the form of large language models (LLMs) such as ChatGPT and GPT-4 that produce astounding natural language. The problem is that you can’t just have robots make API calls to a generic LLM in the cloud because those models aren’t sufficiently localized for what your robot needs to know. Robots live in the physical world, and so they must take in context and be hyperlocal. This means that they need to be able to learn quickly. Rapid learning is also required for using LLMs for advising in specialized domains, such as science and auto repair.
Figure 1: Our intelligent-robot comrades

To use robots in specialized domains, we will need to train LLMs ourselves or refine existing ones so that they can learn more quickly. Hand-in-hand with quick learning is a long-term memory. If your robot doesn’t remember what you talked about last month or what it did to fix your 1979 Honda Civic, it’s going to be of limited use. And we need our robots to tell the truth and say when they don’t know—to actually be useful, robots need to be trustworthy.
Robots need strong mental models

Robots need strong mental models so that they can learn quickly, form long-term memories, and understand truth. A model enables learning because it can encode new input based on what the robot already knows. Models enable memory because they condense information so the learner doesn’t have to store everything that happened, and models enable truth because they provide a prior to minimize spurious correlations. Without truth, robots will make mistakes that no human would make, not even a child.

It’s surprising and wonderful to see that LLMs do seem to be learning implicit mental models of the world [28, 29]. LLMs are only trying to predict the next token, but at some point the most efficient way to do that becomes building a model of the world to understand what is actually going on [30]. We need to train LLMs to maximize this model-building capability with the smallest amount of training data and in a way that aligns with our goals.
Robots need to think forward in novel situations

In addition, we need our robots to think and analyze in novel situations, LLMs are masters at recognizing patterns and blending them, but they don’t reason forward well to reach new conclusions from first principles. Real life consists of sequences of events that have never previously happened, and our robots need to adapt and improvise, which sometimes requires thinking multiple steps into the future. We need to give our robots cognitive tools so they can help us create new theories and explanations so we can move humanity forward, such as by helping us find cures for rare diseases.

In short, robots and domain-specific AI need two things: strong mental models and tools for forward thinking.
Strengthening Mental Models using Curriculum Learning to Acquire a Cognitive Foundation

Robots need strong mental models to learn quickly and adapt to novel situations. Human mental models consist of layers that form our cognitive foundation [1-7]. To give robots strong mental models, we can approximate our cognitive foundation by training them using curriculum learning. Curriculum learning consists of teaching a robot by starting with simple, general, and concrete inputs and gradually moving to complicated, specific, and abstract inputs.

Our human cognitive foundation is depicted in Figure 1. It emerged bit-by-bit through evolution, complexifying simultaneously as our sensory and motor capabilities expanded. This gradual building encouraged reuse and formed the basis for learning ever-more sophisticated behaviors. In this section, we look at the levels of the human cognitive foundation and discuss how curriculum learning can be done at each level to make robots more understandable and trustworthy. Using curriculum learning, we can control what they value and how they represent information, which will better align them with our goals and how we humans understand the world.
Figure 2: Internet content sits on a cognitive foundation
The origin of life

The origin of life itself sits at the base of the cognitive foundation [8]. At life’s inception, self-generating chemical reactions [9] found themselves within lipid enclosures [8], and those reactions that could stay around longer and reproduce became more common. The process needed to “stay around” is called metabolism. These metabolism processes were randomly mutating, and when by chance the first sensor element connected to the first effector (motor) element, purpose came into being [10]. Some purposes happened to allow their attached metabolisms to stay around even longer, and purpose is how the movement of life is different from nonlife, such as rocks. Rocks move due to gravity, but life moves due to purpose. Purpose and life arose together and manifest in a striving to maintain metabolism that we see all around us.

The purpose of life is to maintain metabolism, and the purpose of LLMs is to predict the next token (a token is a generalization of a word to any discrete thing). Building a cognitive foundation entails teaching the model that some tokens are more important than others. In a branch of machine learning geared towards actions reinforcement learning, this notion of importance is often specified as reward. The robot will learn to take actions and to focus its attention to make better predictions of important events, while ignoring others. Training LLMs this way will enable our robots to have goals. Goals are the end states of purposes, and the first goal in life on Earth was single-celled organisms moving toward resources [8]. At the bottom of the cognitive foundation is where we determine the goals for our robots.

At this level of the origin of life, curriculum learning entails specifying that some tokens are more important to predict than others. What is important to predict will depend on the type of robot or specialized AI you want to build.
The development of mind

On top of life’s origin is the development of mind. Some lines of cells were able to better maintain their metabolism when they banded together into groups, eventually becoming complex animals with specialized components that helped them to better survive by making sophisticated decisions. The developmental psychologist Elizabeth Spelke describes the ontology used by the human mind as consisting of six systems of core knowledge [11-12]. She and her collaborators identified this knowledge by taking newborn babies and seeing what they know right at birth. They found that this knowledge consists of

    Places: including distance and direction
    Objects: including continuity and contact
    Social partners: including shareable experiences
    Agents: including cause, cost, and value
    Forms: including shapes and length
    Number: including the natural numbers.

They were able to determine what babies know at birth by using the fact that babies look longer at things that surprise them. If they look at something impossible longer, such as an object disappearing, the researchers know that the baby knows it is impossible.

Alongside this world ontology is a set of fundamental patterns that seem to enable many of our cognitive abilities. Perceptual patterns include those such as force and inside-outside. We understand the world in terms of these patterns [3,4,7]. These patterns likely evolved by being useful for one decision and were then reused by evolution for many decisions, even later becoming abstract through metaphor [13]. We can force an object up a hill and we can force an adversary to back down. Simultaneously, action patterns were built on previous simpler ones. Humans are born with motor programs that are refined through experience [14], which can often be understood as control laws [15]. Because our abilities evolved gradually through evolution, these patterns are reused in humans. By starting simple and adding complexity, we can maximize pattern reuse in robots.

At this level of the development of mind, curriculum learning entails training data that represents basic objects, relationships, and interactions. For example, objects can be attached to other objects and move with them, and objects can be supported by other objects so they don’t fall. Agent objects can push other objects and chase other agent objects. This level of curriculum learning begins with simple, concrete situations that are then followed by abstract ideas that generalize what has been learned through those concrete examples.
Pre-literate learning

Once babies are born, they learn. Pre-literate learning rests upon the development of mind. Children learn through exploration and through shared attention with caregivers [7,16,17]. At this level of pre-literate learning, curriculum learning entails properties and interactions of specific kinds of objects, especially the kinds of objects that are of interest to your domain.
Internet content

Finally, the content of the internet sits on top of this cognitive foundation. Every piece of content created implicitly assumes that the consumer has this cognitive foundation. When we consume this content, its tokens take their meaning from their mapping to this cognitive foundation [1]. Large language models have less to map to, and this is why they have such a hard time with truth and knowing when they don’t know or aren’t sure. Without this mapping, any sequence of real-world events is as possible and likely as any other, as long as the tokens line up. By training with curriculum learning, our LLMs will have this mapping.

Curriculum learning is also important because the less guided our robots are as they acquire a cognitive foundation the more alien they will be. As we have seen, our own cognitive foundation arose by following one path through evolution. All that evolution does is maintain those matabolisms that reproduce, so there is no reason to believe that our sensors allow us to perceive the Truth—we only know that what we perceive is mostly internally consistent and allows us to survive on Earth [25,26]. To illustrate the point, there’s a kind of bird called the Common Cuckoo (Cuculus canorus) that lays its eggs in the nest of another kind of bird, often a small passerine, and the new parents raise it even though the imposter bird is six times as heavy as the real young’s parents [27].  We laugh when animals and insects don’t see the truth, but we assume that we see it ourselves. Since there are likely many cognitive foundations that an AI could acquire, there is no guarantee it will learn the same cognitive foundation that we have. These learning methods will require guidance from humans so we can communicate with and trust them. It starts with how we build their cognitive foundation.
Training Mental Models Situated in the World

Curriculum learning encourages reuse and therefore a strong mental model, and in this section we turn our attention to the need to learn situated in the world. When large language models (LLMs) train only on text they are only learning from part of the information. Language is a unique form of training data because most of the information is left unsaid. When learning to play chess or Go, a neural network sees the whole state. Even in Stratego or poker where the state is hidden, machine learning algorithms know the state space and can create distributions over beliefs of the true state. But in language, a computer only sees the words (tokens), and what they refer to is hidden. This is why ChatGPT has a limited sense of truth. It is difficult to understand the meaning of what is said by only learning from text like LLMs do, even if they are trained through reinforcement learning on the text they generate. Language is a representation medium for the world—it isn't the world itself. When we talk, we only say what can't be inferred because we assume the listener has a basic understanding of the dynamics of the world (e.g., if I push a table the things on it will also move).

By providing the learning a wider window into what is happening, we give it more information to triangulate on a truth. Multimodal learning is a step in the right direction. The DeepMind Flamingo surprised many with its good conversations about pictures from training on image and text data from the web, and the yet unreleased GPT-4 vision model was trained on both images and text. Toward even deeper world immersion, Google has recently been training robots from video demonstrations using its Robotics Transformer 1 (RT-1) system. The key innovation is the tokenization of the robot actions and the events in the video. This allows it to use the next-token-prediction machinery behind large language models, with the goal of predicting the next most likely action based on what it has learned from the demonstrations.

An even deeper immersion beyond learning from videos is learning directly in simulations of the environment. DeepMind has made impressive progress on building household simulations and having robots learn about the world in those. Training immersed in the world leads to stronger mental models because the learner can often directly perceive causes of events instead of having to guess. A stronger mental model allows the agent to ground what it perceives by mapping those perceptions to the model. It’s still possible, of course, to function without a strong mental model in many situations. In fact, we act with limited understanding all of the time. We know that bananas are good for us, but most of us don’t know their molecular structure. And when we were kids and we bought birthday presents for our parents, we didn’t have a grounded understanding of what they would want, we were just guessing by looking at patterns, just like a language model.

Large language models (LLMs) trained only on text are going to do best in domains where deep grounding isn’t needed or in domains where everything they need to know is in the internet content. It doesn’t take deep understanding to take some bullet points and make it fluffy or a particular style. Likewise, many programming tasks are fairly generic. But without a strong mental model there are problems with truth, because there are going to be sequences of tokens that have a high probability that don’t match the particulars of the context, and without a strong mental model proving causal guidance, the LLM has no way of identifying those cases of random coincidence.

Following a curriculum and training situated in the world are two ways to learn a strong mental model. But a strong mental model isn’t the whole story. We need robots that also have robust reasoning skills, robots that can reason from first principles. We can achieve this by giving LLMs cognitive tools.
Expanding LLM Capabilities with Tools

Developing tools enabled our ancestors to surpass the limitations of their bodies to better hunt and protect their families, and tools will similarly allow LLMs to extend their capabilities. LLMs are master interpolators, but to act in novel situations they need tools that can make exact calculations, and to create new knowledge they need to predict the world forward. These capabilities can help them move beyond understanding to invention.

The most basic tool for an LLM is an API call to do a well-defined calculation, such as WolframAlpha. Another tool is an api call for actions in the physical world. Microsoft has built functions that tie into action patterns of a robot [18], allowing UAVs to be controlled by a LLM. These functions have descriptive names so that the language model can infer how to use them. Since LLMs can generate code as easily as words, another tool is enabling an LLM to build programs that can do its thinking for it. For example, ViperGPT generates Python code to answer questions about images [35]. ChatGPT is building a plugin ecosystem to enable it to use tools in a straightforward way, such as by calling WolframAlpha.

One step further is LLMs writing configuration files that can be fed into programs that do thinking for them. Consider GOFAI (good old-fashioned artificial intelligence) planning algorithms such as STRIPS. A situation can be encoded into a planning file in PDDL and a plan can be automatically generated by a planner. The problem with GOFAI methods has always been that they are brittle. You proudly build a planning representation for one situation, but you later find that it doesn’t cover unexpected variations of that situation. LLMs overcome this brittleness by simply building a new representation when the need arises. Dynamically rewriting the representation overcomes the brittleness of GOFAI but maintains the benefits of exactness and forward thinking. Similarly in logic, you spend a lot of time building formulas, and they work great until something unexpected happens. LLMs can simply rewrite the formulas as the situation changes. We can think of thinking as two steps, representing the current situation and projecting possibilities forward. We dive deeper into this idea in the next section.
Tools for deliberate thinking

We can dive deeper into how tools can help robots understand and invent. The way we humans use our mental models and cognitive foundation to understand language (and sensory input more generally) is depicted in Figure 3. According to this model, when we read or listen to someone, we create a mental scene of what they are talking about, and from this mental scene we can generate a set of possibilities [19,20]